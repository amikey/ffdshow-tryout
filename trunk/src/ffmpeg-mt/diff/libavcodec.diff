--- avcodec.h	Sun Jan 25 13:18:58 2009
+++ avcodec.h	Sun Jan 25 13:07:20 2009
@@ -525,22 +525,26 @@
  */
 #define CODEC_CAP_DELAY           0x0020
 /**
  * Codec can be fed a final frame with a smaller size.
  * This can be used to prevent truncation of the last audio samples.
  */
 #define CODEC_CAP_SMALL_LAST_FRAME 0x0040
 /**
  * Codec can export data for HW decoding (VDPAU).
  */
 #define CODEC_CAP_HWACCEL_VDPAU    0x0080
+/**
+ * Codec supports frame-based multithreading.
+ */
+#define CODEC_CAP_FRAME_THREADS    0x0100
 
 //The following defines may change, don't expect compatibility if you use them.
 #define MB_TYPE_INTRA4x4   0x0001
 #define MB_TYPE_INTRA16x16 0x0002 //FIXME H.264-specific
 #define MB_TYPE_INTRA_PCM  0x0004 //FIXME H.264-specific
 #define MB_TYPE_16x16      0x0008
 #define MB_TYPE_16x8       0x0010
 #define MB_TYPE_8x16       0x0020
 #define MB_TYPE_8x8        0x0040
 #define MB_TYPE_INTERLACED 0x0080
 #define MB_TYPE_DIRECT2    0x0100 //FIXME
@@ -795,24 +799,37 @@
      * - encoding: Set by user.\
      * - decoding: Set by libavcodec.\
      */\
     int8_t *ref_index[2];\
 \
     /**\
      * reordered opaque 64bit number (generally a PTS) from AVCodecContext.reordered_opaque\
      * output in AVFrame.reordered_opaque\
      * - encoding: unused\
      * - decoding: Read by user.\
      */\
-    int64_t reordered_opaque;\
-
+     int64_t reordered_opaque;\
+\
+    /**\
+     * the AVCodecContext which ff_get_buffer was last called on\
+     * - encoding: Set by libavcodec.\
+     * - decoding: Set by libavcodec.\
+     */\
+    struct AVCodecContext *owner;\
+\
+    /**\
+     * used by multithreading to store frame-specific info\
+     * - encoding: Set by libavcodec.\
+     * - decoding: Set by libavcodec.\
+     */\
+    void *thread_opaque;
 
 #define FF_QSCALE_TYPE_MPEG1 0
 #define FF_QSCALE_TYPE_MPEG2 1
 #define FF_QSCALE_TYPE_H264  2
 
 #define FF_BUFFER_TYPE_INTERNAL 1
 #define FF_BUFFER_TYPE_USER     2 ///< direct rendering buffers (image is (de)allocated by user)
 #define FF_BUFFER_TYPE_SHARED   4 ///< Buffer from somewhere else; don't deallocate image (data/base), all other tables are not shared.
 #define FF_BUFFER_TYPE_COPY     8 ///< Just a (modified) copy of some other buffer, don't deallocate anything.
 
 
@@ -950,23 +967,23 @@
      * Frame rate emulation. If not zero, the lower layer (i.e. format handler)
      * has to read frames at native frame rate.
      * - encoding: Set by user.
      * - decoding: unused
      */
     int rate_emu;
 
     /**
      * If non NULL, 'draw_horiz_band' is called by the libavcodec
      * decoder to draw a horizontal band. It improves cache usage. Not
      * all codecs can do that. You must check the codec capabilities
-     * beforehand.
+     * beforehand. May be called by different threads at the same time.
      * - encoding: unused
      * - decoding: Set by user.
      * @param height the height of the slice
      * @param y the y position of the slice
      * @param type 1->top field, 2->bottom field, 3->frame
      * @param offset offset into the AVFrame.data from which the slice should be read
      */
     void (*draw_horiz_band)(struct AVCodecContext *s,
                             const AVFrame *src, int offset[4],
                             int y, int type, int height);
 
@@ -978,23 +995,23 @@
      * audio sample format
      * - encoding: Set by user.
      * - decoding: Set by libavcodec.
      */
     enum SampleFormat sample_fmt;  ///< sample format, currently unused
 
     /* The following data should not be initialized. */
     /**
      * Samples per packet, initialized when calling 'init'.
      */
     int frame_size;
-    int frame_number;   ///< audio or video frame number
+    int frame_number;   ///< Number of audio or video frames returned so far
     int real_pict_num;  ///< Returns the real picture number of previous encoded frame.
 
     /**
      * Number of frames the decoded output will be delayed relative to
      * the encoded input.
      * - encoding: Set by libavcodec.
      * - decoding: unused
      */
     int delay;
 
     /* - encoding parameters */
@@ -1188,31 +1205,35 @@
     int error_recognition;
 #define FF_ER_CAREFUL         1
 #define FF_ER_COMPLIANT       2
 #define FF_ER_AGGRESSIVE      3
 #define FF_ER_VERY_AGGRESSIVE 4
 
     /**
      * Called at the beginning of each frame to get a buffer for it.
      * If pic.reference is set then the frame will be read later by libavcodec.
      * avcodec_align_dimensions() should be used to find the required width and
      * height, as they normally need to be rounded up to the next multiple of 16.
+     * May be called by different threads if frame threading is enabled, but not
+     * by more than one at the same time.
      * - encoding: unused
      * - decoding: Set by libavcodec., user can override.
      */
     int (*get_buffer)(struct AVCodecContext *c, AVFrame *pic);
 
     /**
      * Called to release buffers which were allocated with get_buffer.
      * A released buffer can be reused in get_buffer().
-     * pic.data[*] must be set to NULL.
+     * pic.data[*] must be set to NULL. May be called by different threads
+     * if frame threading is enabled, but not more than one at the same time.
+     *
      * - encoding: unused
      * - decoding: Set by libavcodec., user can override.
      */
     void (*release_buffer)(struct AVCodecContext *c, AVFrame *pic);
 
     /**
      * If 1 the stream has a 1 frame delay during decoding.
      * - encoding: Set by libavcodec.
      * - decoding: Set by libavcodec.
      */
     int has_b_frames;
@@ -2298,22 +2319,48 @@
      * - encoding: Set by user.
      * - decoding: unused.
      */
     float rc_max_available_vbv_use;
 
     /**
      * Ratecontrol attempt to use, at least, <value> times the amount needed to prevent a vbv overflow.
      * - encoding: Set by user.
      * - decoding: unused.
      */
     float rc_min_vbv_overflow_use;
+
+    /**
+     * Whether this is a copy of the context which had init() called on it.
+     * This is used by multithreading - shared tables and picture pointers
+     * should be freed from the original context only.
+     * - encoding: Set by libavcodec.
+     * - decoding: Set by libavcodec.
+     */
+    int is_copy;
+
+    /**
+     * Which multithreading methods to use, for codecs that support more than one.
+     * - encoding: Set by user, otherwise the default is used.
+     * - decoding: Set by user, otherwise the default is used.
+     */
+    int thread_type;
+#define FF_THREAD_FRAME   1 //< Decode more than one frame at once
+#define FF_THREAD_SLICE   2 //< Decode more than one part of a single frame at once
+#define FF_THREAD_DEFAULT 3 //< Use both if possible.
+
+    /**
+     * Which multithreading methods are actually active at the moment.
+     * - encoding: Set by libavcodec.
+     * - decoding: Set by libavcodec.
+     */
+    int active_thread_type;
 } AVCodecContext;
 
 /**
  * AVCodec.
  */
 typedef struct AVCodec {
     /**
      * Name of the codec implementation.
      * The name is globally unique among encoders and among decoders (but an
      * encoder and a decoder can share the same name).
      * This is the primary way to find a codec from the user perspective.
@@ -2340,22 +2387,42 @@
     void (*flush)(AVCodecContext *);
     const AVRational *supported_framerates; ///< array of supported framerates, or NULL if any, array is terminated by {0,0}
     const enum PixelFormat *pix_fmts;       ///< array of supported pixel formats, or NULL if unknown, array is terminated by -1
     /**
      * Descriptive name for the codec, meant to be more human readable than \p name.
      * You \e should use the NULL_IF_CONFIG_SMALL() macro to define it.
      */
     const char *long_name;
     const int *supported_samplerates;       ///< array of supported audio samplerates, or NULL if unknown, array is terminated by 0
     const enum SampleFormat *sample_fmts;   ///< array of supported sample formats, or NULL if unknown, array is terminated by -1
     const int64_t *channel_layouts;         ///< array of support channel layouts, or NULL if unknown. array is terminated by 0
+
+    /**
+     * @defgroup framethreading Frame threading support functions.
+     * @{
+     */
+    /**
+     * If the codec allocates writable tables in init(), define init_copy() to re-allocate
+     * them in the copied contexts. Before calling it, priv_data will be set to a copy of
+     * the original.
+     */
+    int (*init_copy)(AVCodecContext *);
+    /**
+     * Copy all necessary context variables from the last thread before starting the next one.
+     * If the codec doesn't define this, the next thread will start automatically; otherwise,
+     * the codec must call ff_report_frame_setup_done(). Do not assume anything about the
+     * contents of priv data except that it has been copied from the original some time after
+     * codec init. Will not be called if frame threading is disabled.
+     */
+    int (*update_context)(AVCodecContext *, AVCodecContext *from);
+    /** @} */
 } AVCodec;
 
 /**
  * four components are given, that's all.
  * the last component is alpha
  */
 typedef struct AVPicture {
     uint8_t *data[4];
     int linesize[4];       ///< number of bytes per line
 } AVPicture;
 
--- beosthread.c	Sun Jan 25 13:18:58 2009
+++ beosthread.c	Sun Jan 25 13:07:20 2009
@@ -113,23 +113,29 @@
 
         c[i].func= NULL;
         if(ret) ret[i]= c[i].ret;
     }
     return 0;
 }
 
 int avcodec_thread_init(AVCodecContext *s, int thread_count){
     int i;
     ThreadContext *c;
 
+    if(!(s->thread_type & FF_THREAD_SLICE)){
+        av_log(s, AV_LOG_WARNING, "The requested thread algorithm is not supported with this thread library.\n");
+        return 0;
+    }
+
     s->thread_count= thread_count;
+    s->active_thread_type= FF_THREAD_SLICE;
 
     assert(!s->thread_opaque);
     c= av_mallocz(sizeof(ThreadContext)*thread_count);
     s->thread_opaque= c;
 
     for(i=0; i<thread_count; i++){
 //printf("init semaphors %d\n", i); fflush(stdout);
         c[i].avctx= s;
 
         if((c[i].work_sem = create_sem(0, "ff work sem")) < B_OK)
             goto fail;
--- dsputil.c	Sun Jan 25 13:18:59 2009
+++ dsputil.c	Sun Jan 25 13:07:20 2009
@@ -430,46 +430,51 @@
 int w53_32_c(void *v, uint8_t * pix1, uint8_t * pix2, int line_size, int h){
     return w_c(v, pix1, pix2, line_size, 32, h, 1);
 }
 
 int w97_32_c(void *v, uint8_t * pix1, uint8_t * pix2, int line_size, int h){
     return w_c(v, pix1, pix2, line_size, 32, h, 0);
 }
 #endif
 
 /* draw the edges of width 'w' of an image of size width, height */
 //FIXME check that this is ok for mpeg4 interlaced
-static void draw_edges_c(uint8_t *buf, int wrap, int width, int height, int w)
+static void draw_edges_c(uint8_t *buf, int wrap, int width, int height, int w, int sides)
 {
     uint8_t *ptr, *last_line;
     int i;
 
     last_line = buf + (height - 1) * wrap;
     for(i=0;i<w;i++) {
         /* top and bottom */
-        memcpy(buf - (i + 1) * wrap, buf, width);
-        memcpy(last_line + (i + 1) * wrap, last_line, width);
+        if (sides&EDGE_TOP)    memcpy(buf - (i + 1) * wrap, buf, width);
+        if (sides&EDGE_BOTTOM) memcpy(last_line + (i + 1) * wrap, last_line, width);
     }
     /* left and right */
     ptr = buf;
     for(i=0;i<height;i++) {
         memset(ptr - w, ptr[0], w);
         memset(ptr + width, ptr[width-1], w);
         ptr += wrap;
     }
     /* corners */
     for(i=0;i<w;i++) {
-        memset(buf - (i + 1) * wrap - w, buf[0], w); /* top left */
-        memset(buf - (i + 1) * wrap + width, buf[width-1], w); /* top right */
-        memset(last_line + (i + 1) * wrap - w, last_line[0], w); /* top left */
-        memset(last_line + (i + 1) * wrap + width, last_line[width-1], w); /* top right */
+        if (sides&EDGE_TOP) {
+            memset(buf - (i + 1) * wrap - w, buf[0], w); /* top left */
+            memset(buf - (i + 1) * wrap + width, buf[width-1], w); /* top right */
+        }
+
+        if (sides&EDGE_BOTTOM) {
+            memset(last_line + (i + 1) * wrap - w, last_line[0], w); /* top left */
+            memset(last_line + (i + 1) * wrap + width, last_line[width-1], w); /* top right */
+        }
     }
 }
 
 /**
  * Copies a rectangular area of samples to a temporary buffer and replicates the boarder samples.
  * @param buf destination buffer
  * @param src source buffer
  * @param linesize number of bytes between 2 vertically adjacent samples in both the source and destination buffers
  * @param block_w width of block
  * @param block_h height of block
  * @param src_x x coordinate of the top left sample of the block in the source buffer
--- dsputil.h	Sun Jan 25 13:18:59 2009
+++ dsputil.h	Sun Jan 25 13:07:20 2009
@@ -431,24 +431,26 @@
 #define FF_LIBMPEG2_IDCT_PERM 2
 #define FF_SIMPLE_IDCT_PERM 3
 #define FF_TRANSPOSE_IDCT_PERM 4
 #define FF_PARTTRANS_IDCT_PERM 5
 #define FF_SSE2_IDCT_PERM 6
 
     int (*try_8x8basis)(int16_t rem[64], int16_t weight[64], int16_t basis[64], int scale);
     void (*add_8x8basis)(int16_t rem[64], int16_t basis[64], int scale);
 #define BASIS_SHIFT 16
 #define RECON_SHIFT 6
 
-    void (*draw_edges)(uint8_t *buf, int wrap, int width, int height, int w);
+    void (*draw_edges)(uint8_t *buf, int wrap, int width, int height, int w, int sides);
 #define EDGE_WIDTH 16
+#define EDGE_TOP    1
+#define EDGE_BOTTOM 2
 
     /* h264 functions */
     /* NOTE!!! if you implement any of h264_idct8_add, h264_idct8_add4 then you must implement all of them
        NOTE!!! if you implement any of h264_idct_add, h264_idct_add16, h264_idct_add16intra, h264_idct_add8 then you must implement all of them
         The reason for above, is that no 2 out of one list may use a different permutation.
     */
     void (*h264_idct_add)(uint8_t *dst/*align 4*/, DCTELEM *block/*align 16*/, int stride);
     void (*h264_idct8_add)(uint8_t *dst/*align 8*/, DCTELEM *block/*align 16*/, int stride);
     void (*h264_idct_dc_add)(uint8_t *dst/*align 4*/, DCTELEM *block/*align 16*/, int stride);
     void (*h264_idct8_dc_add)(uint8_t *dst/*align 8*/, DCTELEM *block/*align 16*/, int stride);
     void (*h264_dct)(DCTELEM block[4][4]);
@@ -666,47 +668,35 @@
     int inverse;
     uint16_t *revtab;
     FFTComplex *exptab;
     FFTComplex *exptab1; /* only used by SSE code */
     FFTComplex *tmp_buf;
     void (*fft_permute)(struct FFTContext *s, FFTComplex *z);
     void (*fft_calc)(struct FFTContext *s, FFTComplex *z);
     void (*imdct_calc)(struct MDCTContext *s, FFTSample *output, const FFTSample *input);
     void (*imdct_half)(struct MDCTContext *s, FFTSample *output, const FFTSample *input);
 } FFTContext;
 
-/**
- * Sets up a complex FFT.
- * @param nbits           log2 of the length of the input array
- * @param inverse         if 0 perform the forward transform, if 1 perform the inverse
- */
 int ff_fft_init(FFTContext *s, int nbits, int inverse);
 void ff_fft_permute_c(FFTContext *s, FFTComplex *z);
 void ff_fft_permute_sse(FFTContext *s, FFTComplex *z);
 void ff_fft_calc_c(FFTContext *s, FFTComplex *z);
 void ff_fft_calc_sse(FFTContext *s, FFTComplex *z);
 void ff_fft_calc_3dn(FFTContext *s, FFTComplex *z);
 void ff_fft_calc_3dn2(FFTContext *s, FFTComplex *z);
 void ff_fft_calc_altivec(FFTContext *s, FFTComplex *z);
 
-/**
- * Do the permutation needed BEFORE calling ff_fft_calc().
- */
 static inline void ff_fft_permute(FFTContext *s, FFTComplex *z)
 {
     s->fft_permute(s, z);
 }
-/**
- * Do a complex FFT with the parameters defined in ff_fft_init(). The
- * input data must be permuted before. No 1.0/sqrt(n) normalization is done.
- */
 static inline void ff_fft_calc(FFTContext *s, FFTComplex *z)
 {
     s->fft_calc(s, z);
 }
 void ff_fft_end(FFTContext *s);
 
 /* MDCT computation */
 
 typedef struct MDCTContext {
     int n;  /* size of MDCT (i.e. number of input data * 2) */
     int nbits; /* n = 2^nbits */
--- fft.c	Sun Jan 25 13:18:59 2009
+++ fft.c	Sun Jan 25 13:07:21 2009
@@ -50,23 +50,27 @@
 static int split_radix_permutation(int i, int n, int inverse)
 {
     int m;
     if(n <= 2) return i&1;
     m = n >> 1;
     if(!(i&m))            return split_radix_permutation(i, m, inverse)*2;
     m >>= 1;
     if(inverse == !(i&m)) return split_radix_permutation(i, m, inverse)*4 + 1;
     else                  return split_radix_permutation(i, m, inverse)*4 - 1;
 }
 
-av_cold int ff_fft_init(FFTContext *s, int nbits, int inverse)
+/**
+ * The size of the FFT is 2^nbits. If inverse is TRUE, inverse FFT is
+ * done
+ */
+int ff_fft_init(FFTContext *s, int nbits, int inverse)
 {
     int i, j, m, n;
     float alpha, c1, s1, s2;
     int split_radix = 1;
     int av_unused has_vectors;
 
     if (nbits < 2 || nbits > 16)
         goto fail;
     s->nbits = nbits;
     n = 1 << nbits;
 
@@ -173,22 +177,25 @@
     }
 
     return 0;
  fail:
     av_freep(&s->revtab);
     av_freep(&s->exptab);
     av_freep(&s->exptab1);
     av_freep(&s->tmp_buf);
     return -1;
 }
 
+/**
+ * Do the permutation needed BEFORE calling ff_fft_calc()
+ */
 void ff_fft_permute_c(FFTContext *s, FFTComplex *z)
 {
     int j, k, np;
     FFTComplex tmp;
     const uint16_t *revtab = s->revtab;
     np = 1 << s->nbits;
 
     if (s->tmp_buf) {
         /* TODO: handle split-radix permute in a more optimal way, probably in-place */
         for(j=0;j<np;j++) s->tmp_buf[revtab[j]] = z[j];
         memcpy(z, s->tmp_buf, np * sizeof(FFTComplex));
@@ -359,16 +366,21 @@
 DECL_FFT(4096,2048,1024)
 DECL_FFT(8192,4096,2048)
 DECL_FFT(16384,8192,4096)
 DECL_FFT(32768,16384,8192)
 DECL_FFT(65536,32768,16384)
 
 static void (*fft_dispatch[])(FFTComplex*) = {
     fft4, fft8, fft16, fft32, fft64, fft128, fft256, fft512, fft1024,
     fft2048, fft4096, fft8192, fft16384, fft32768, fft65536,
 };
 
+/**
+ * Do a complex FFT with the parameters defined in ff_fft_init(). The
+ * input data must be permuted before with s->revtab table. No
+ * 1.0/sqrt(n) normalization is done.
+ */
 void ff_fft_calc_c(FFTContext *s, FFTComplex *z)
 {
     fft_dispatch[s->nbits-2](z);
 }
 
--- flac.h	Sun Jan 25 13:18:59 2009
+++ flac.h	Sun Jan 25 13:07:21 2009
@@ -21,47 +21,33 @@
 
 /**
  * @file flac.h
  * FLAC (Free Lossless Audio Codec) decoder/demuxer common functions
  */
 
 #ifndef AVCODEC_FLAC_H
 #define AVCODEC_FLAC_H
 
 #include "avcodec.h"
 
-#define FLAC_STREAMINFO_SIZE 34
-
-enum {
-    FLAC_METADATA_TYPE_STREAMINFO = 0,
-    FLAC_METADATA_TYPE_PADDING,
-    FLAC_METADATA_TYPE_APPLICATION,
-    FLAC_METADATA_TYPE_SEEKTABLE,
-    FLAC_METADATA_TYPE_VORBIS_COMMENT,
-    FLAC_METADATA_TYPE_CUESHEET,
-    FLAC_METADATA_TYPE_PICTURE,
-    FLAC_METADATA_TYPE_INVALID = 127
-};
-
 /**
  * Data needed from the Streaminfo header for use by the raw FLAC demuxer
  * and/or the FLAC decoder.
  */
 #define FLACSTREAMINFO \
     int min_blocksize;      /**< minimum block size, in samples          */\
     int max_blocksize;      /**< maximum block size, in samples          */\
     int max_framesize;      /**< maximum frame size, in bytes            */\
     int samplerate;         /**< sample rate                             */\
     int channels;           /**< number of channels                      */\
     int bps;                /**< bits-per-sample                         */\
-    int64_t samples;        /**< total number of samples                 */\
 
 typedef struct FLACStreaminfo {
     FLACSTREAMINFO
 } FLACStreaminfo;
 
 /**
  * Parse the Streaminfo metadata block
  * @param[out] avctx   codec context to set basic stream parameters
  * @param[out] s       where parsed information is stored
  * @param[in]  buffer  pointer to start of 34-byte streaminfo data
  */
--- h263.c	Sun Jan 25 13:18:59 2009
+++ h263.c	Sun Jan 25 13:07:21 2009
@@ -32,22 +32,23 @@
  */
 
 //#define DEBUG
 #include <limits.h>
 
 #include "dsputil.h"
 #include "avcodec.h"
 #include "mpegvideo.h"
 #include "h263data.h"
 #include "mpeg4data.h"
 #include "mathops.h"
+#include "thread.h"
 
 //#undef NDEBUG
 //#include <assert.h>
 
 #define INTRA_MCBPC_VLC_BITS 6
 #define INTER_MCBPC_VLC_BITS 7
 #define CBPY_VLC_BITS 6
 #define MV_VLC_BITS 9
 #define DC_VLC_BITS 9
 #define SPRITE_TRAJ_VLC_BITS 6
 #define MB_TYPE_B_VLC_BITS 4
@@ -887,33 +888,32 @@
                 skip_put_bits(&s->pb, mpeg4_get_block_length(s, block[i], i, 0, s->intra_scantable.permutated));
             }
         }else{
             /* encode each block */
             for (i = 0; i < 6; i++) {
                 mpeg4_encode_block(s, block[i], i, 0, s->intra_scantable.permutated, dc_pb, ac_pb);
             }
         }
     }
 }
 
-static const int dquant_code[5]= {1,0,9,2,3};
-
 void mpeg4_encode_mb(MpegEncContext * s,
                     DCTELEM block[6][64],
                     int motion_x, int motion_y)
 {
     int cbpc, cbpy, pred_x, pred_y;
     PutBitContext * const pb2    = s->data_partitioning                         ? &s->pb2    : &s->pb;
     PutBitContext * const tex_pb = s->data_partitioning && s->pict_type!=FF_B_TYPE ? &s->tex_pb : &s->pb;
     PutBitContext * const dc_pb  = s->data_partitioning && s->pict_type!=FF_I_TYPE ? &s->pb2    : &s->pb;
     const int interleaved_stats= (s->flags&CODEC_FLAG_PASS1) && !s->data_partitioning ? 1 : 0;
+    const int dquant_code[5]= {1,0,9,2,3};
 
     //    printf("**mb x=%d y=%d\n", s->mb_x, s->mb_y);
     if (!s->mb_intra) {
         int i, cbp;
 
         if(s->pict_type==FF_B_TYPE){
             static const int mb_type_table[8]= {-1, 3, 2, 1,-1,-1,-1, 0}; /* convert from mv_dir to type */
             int mb_type=  mb_type_table[s->mv_dir];
 
             if(s->mb_x==0){
                 for(i=0; i<2; i++){
@@ -1261,22 +1261,23 @@
 }
 
 void h263_encode_mb(MpegEncContext * s,
                     DCTELEM block[6][64],
                     int motion_x, int motion_y)
 {
     int cbpc, cbpy, i, cbp, pred_x, pred_y;
     int16_t pred_dc;
     int16_t rec_intradc[6];
     int16_t *dc_ptr[6];
     const int interleaved_stats= (s->flags&CODEC_FLAG_PASS1);
+    const int dquant_code[5]= {1,0,9,2,3};
 
     //printf("**mb x=%d y=%d\n", s->mb_x, s->mb_y);
     if (!s->mb_intra) {
         /* compute cbp */
         cbp= get_p_cbp(s, block, motion_x, motion_y);
 
         if ((cbp | motion_x | motion_y | s->dquant | (s->mv_type - MV_TYPE_16X16)) == 0) {
             /* skip macroblock */
             put_bits(&s->pb, 1, 1);
             if(interleaved_stats){
                 s->misc_bits++;
@@ -3194,23 +3195,30 @@
     if(s->shape != RECT_SHAPE){
         header_extension= get_bits1(&s->gb);
         //FIXME more stuff here
     }
 
     mb_num= get_bits(&s->gb, mb_num_bits);
     if(mb_num>=s->mb_num){
         av_log(s->avctx, AV_LOG_ERROR, "illegal mb_num in video packet (%d %d) \n", mb_num, s->mb_num);
         return -1;
     }
     if(s->pict_type == FF_B_TYPE){
-        while(s->next_picture.mbskip_table[ s->mb_index2xy[ mb_num ] ]) mb_num++;
+        int mb_x = 0, mb_y = 0;
+
+        while(s->next_picture.mbskip_table[ s->mb_index2xy[ mb_num ] ]) {
+            if (!mb_x) ff_await_frame_progress((AVFrame*)s->next_picture_ptr, mb_y++);
+            mb_num++;
+            if (++mb_x == s->mb_width) mb_x = 0;
+        }
+
         if(mb_num >= s->mb_num) return -1; // slice contains just skipped MBs which where already decoded
     }
 
     s->mb_x= mb_num % s->mb_width;
     s->mb_y= mb_num / s->mb_width;
 
     if(s->shape != BIN_ONLY_SHAPE){
         int qscale= get_bits(&s->gb, s->quant_precision);
         if(qscale)
             s->chroma_qscale=s->qscale= qscale;
     }
@@ -4284,22 +4292,24 @@
 
         s->mb_intra = 0; //B-frames never contain intra blocks
         s->mcsel=0;      //     ...               true gmc blocks
 
         if(s->mb_x==0){
             for(i=0; i<2; i++){
                 s->last_mv[i][0][0]=
                 s->last_mv[i][0][1]=
                 s->last_mv[i][1][0]=
                 s->last_mv[i][1][1]= 0;
             }
+
+            ff_await_frame_progress((AVFrame*)s->next_picture_ptr, s->mb_y);
         }
 
         /* if we skipped it in the future P Frame than skip it now too */
         s->mb_skipped= s->next_picture.mbskip_table[s->mb_y * s->mb_stride + s->mb_x]; // Note, skiptab=0 if last was GMC
 
         if(s->mb_skipped){
                 /* skip mb */
             for(i=0;i<6;i++)
                 s->block_last_index[i] = -1;
 
             s->mv_dir = MV_DIR_FORWARD;
@@ -4463,22 +4473,28 @@
     for (i = 0; i < 6; i++) {
         if (mpeg4_decode_block(s, block[i], i, cbp&32, 0, 0) < 0)
             return -1;
         cbp+=cbp;
     }
 end:
 
         /* per-MB end of slice check */
     if(s->codec_id==CODEC_ID_MPEG4){
         if(mpeg4_is_resync(s)){
             const int delta= s->mb_x + 1 == s->mb_width ? 2 : 1;
+
+            if(s->pict_type==FF_B_TYPE){
+                ff_await_frame_progress((AVFrame*)s->next_picture_ptr,
+                                         (s->mb_x + delta >= s->mb_width) ? FFMIN(s->mb_y+1, s->mb_height-1) : s->mb_y);
+            }
+
             if(s->pict_type==FF_B_TYPE && s->next_picture.mbskip_table[xy + delta])
                 return SLICE_OK;
             return SLICE_END;
         }
     }
 
     return SLICE_OK;
 }
 
 static int h263_decode_motion(MpegEncContext * s, int pred, int f_code)
 {
--- h263dec.c	Sun Jan 25 13:18:59 2009
+++ h263dec.c	Sun Jan 25 13:07:21 2009
@@ -23,22 +23,23 @@
 /**
  * @file h263dec.c
  * H.263 decoder.
  */
 
 #include "avcodec.h"
 #include "dsputil.h"
 #include "mpegvideo.h"
 #include "h263_parser.h"
 #include "mpeg4video_parser.h"
 #include "msmpeg4.h"
+#include "thread.h"
 
 //#define DEBUG
 //#define PRINT_FRAME_TIME
 
 av_cold int ff_h263_decode_init(AVCodecContext *avctx)
 {
     MpegEncContext *s = avctx->priv_data;
 
     s->avctx = avctx;
     s->out_format = FMT_H263;
 
@@ -218,42 +219,44 @@
                     if(s->loop_filter)
                         ff_h263_loop_filter(s);
 
 //printf("%d %d %d %06X\n", s->mb_x, s->mb_y, s->gb.size*8 - get_bits_count(&s->gb), show_bits(&s->gb, 24));
                     ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x, s->mb_y, (AC_END|DC_END|MV_END)&part_mask);
 
                     s->padding_bug_score--;
 
                     if(++s->mb_x >= s->mb_width){
                         s->mb_x=0;
                         ff_draw_horiz_band(s, s->mb_y*mb_size, mb_size);
+                        MPV_report_decode_progress(s);
                         s->mb_y++;
                     }
                     return 0;
                 }else if(ret==SLICE_NOEND){
                     av_log(s->avctx, AV_LOG_ERROR, "Slice mismatch at MB: %d\n", xy);
                     ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x+1, s->mb_y, (AC_END|DC_END|MV_END)&part_mask);
                     return -1;
                 }
                 av_log(s->avctx, AV_LOG_ERROR, "Error at MB: %d\n", xy);
                 ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x, s->mb_y, (AC_ERROR|DC_ERROR|MV_ERROR)&part_mask);
 
                 return -1;
             }
 
             MPV_decode_mb(s, s->block);
             if(s->loop_filter)
                 ff_h263_loop_filter(s);
         }
 
         ff_draw_horiz_band(s, s->mb_y*mb_size, mb_size);
+        MPV_report_decode_progress(s);
 
         s->mb_x= 0;
     }
 
     assert(s->mb_x==0 && s->mb_y==s->mb_height);
 
     /* try to detect the padding bug */
     if(      s->codec_id==CODEC_ID_MPEG4
        &&   (s->workaround_bugs&FF_BUG_AUTODETECT)
        &&    s->gb.size_in_bits - get_bits_count(&s->gb) >=0
        &&    s->gb.size_in_bits - get_bits_count(&s->gb) < 48
@@ -604,22 +607,24 @@
     }else if((!s->no_rounding) || s->pict_type==FF_B_TYPE){
         s->me.qpel_put= s->dsp.put_qpel_pixels_tab;
         s->me.qpel_avg= s->dsp.avg_qpel_pixels_tab;
     }else{
         s->me.qpel_put= s->dsp.put_no_rnd_qpel_pixels_tab;
         s->me.qpel_avg= s->dsp.avg_qpel_pixels_tab;
     }
 
     if(MPV_frame_start(s, avctx) < 0)
         return -1;
 
+    if (!s->divx_packed) ff_report_frame_setup_done(avctx);
+
 #ifdef DEBUG
     av_log(avctx, AV_LOG_DEBUG, "qscale=%d\n", s->qscale);
 #endif
 
     ff_er_frame_start(s);
 
     //the second part of the wmv2 header contains the MB skip bits which are stored in current_picture->mb_type
     //which is not available before MPV_frame_start()
     if (CONFIG_WMV2_DECODER && s->msmpeg4_version==5){
         ret = ff_wmv2_decode_secondary_picture_header(s);
         if(ret<0) return ret;
@@ -710,25 +715,26 @@
 }
 
 AVCodec mpeg4_decoder = {
     "mpeg4",
     CODEC_TYPE_VIDEO,
     CODEC_ID_MPEG4,
     sizeof(MpegEncContext),
     ff_h263_decode_init,
     NULL,
     ff_h263_decode_end,
     ff_h263_decode_frame,
-    CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 | CODEC_CAP_TRUNCATED | CODEC_CAP_DELAY,
+    CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 | CODEC_CAP_TRUNCATED | CODEC_CAP_DELAY | CODEC_CAP_FRAME_THREADS,
     .flush= ff_mpeg_flush,
     .long_name= NULL_IF_CONFIG_SMALL("MPEG-4 part 2"),
+    .update_context= ONLY_IF_THREADS_ENABLED(ff_mpeg_update_context)
 };
 
 AVCodec h263_decoder = {
     "h263",
     CODEC_TYPE_VIDEO,
     CODEC_ID_H263,
     sizeof(MpegEncContext),
     ff_h263_decode_init,
     NULL,
     ff_h263_decode_end,
     ff_h263_decode_frame,
--- h264.c	Sun Jan 25 13:18:59 2009
+++ h264.c	Sun Jan 25 13:07:21 2009
@@ -26,22 +26,23 @@
  */
 
 #include "dsputil.h"
 #include "avcodec.h"
 #include "mpegvideo.h"
 #include "h264.h"
 #include "h264data.h"
 #include "h264_parser.h"
 #include "golomb.h"
 #include "mathops.h"
 #include "rectangle.h"
+#include "thread.h"
 #include "vdpau_internal.h"
 
 #include "cabac.h"
 #if ARCH_X86
 #include "x86/h264_i386.h"
 #endif
 
 //#undef NDEBUG
 #include <assert.h>
 
 /**
@@ -1599,22 +1600,157 @@
     block[stride*1 + xStride*1]= (e-b);
 }
 #endif
 
 /**
  * gets the chroma qp.
  */
 static inline int get_chroma_qp(H264Context *h, int t, int qscale){
     return h->pps.chroma_qp_table[t][qscale];
 }
 
+static inline int mc_dir_part_y(H264Context *h, Picture *pic, int n, int height,
+                                 int y_offset, int list){
+    int my= h->mv_cache[list][ scan8[n] ][1] + 4*y_offset;
+    int filter_height= 6;
+    int extra_height= h->emu_edge_height;
+    const int full_my= my>>2;
+
+    if(!pic->data[0]) return -1;
+
+    if(full_my < (extra_height + filter_height)){
+        my = abs(my) + extra_height*4;
+    }
+
+    return (my + height * 4 + filter_height * 4 + 1) >> 2;
+}
+
+static inline void mc_part_y(H264Context *h, int refs[2][48], int n, int height,
+                               int y_offset, int list0, int list1){
+    MpegEncContext * const s = &h->s;
+    int my;
+
+    y_offset += 16*(s->mb_y >> FIELD_PICTURE);
+
+    if(list0){
+        int ref_n = h->ref_cache[0][ scan8[n] ], my;
+        Picture *ref= &h->ref_list[0][ref_n];
+
+        // Error resilience puts the current picture in the ref list.
+        // Don't try to wait on these as it will cause a deadlock.
+        // Fields can wait on each other, though.
+        if(ref->thread_opaque != s->current_picture.thread_opaque ||
+           (ref->reference&3) != s->picture_structure) {
+            my = mc_dir_part_y(h, ref, n, height, y_offset, 0);
+            refs[0][ref_n] = FFMAX(refs[0][ref_n], my);
+        }
+    }
+
+    if(list1){
+        int ref_n = h->ref_cache[1][ scan8[n] ];
+        Picture *ref= &h->ref_list[1][ref_n];
+
+        if(ref->thread_opaque != s->current_picture.thread_opaque ||
+           (ref->reference&3) != s->picture_structure) {
+            my = mc_dir_part_y(h, ref, n, height, y_offset, 1);
+            refs[1][ref_n] = FFMAX(refs[1][ref_n], my);
+        }
+    }
+}
+
+/**
+ * Wait until all reference frames are available for MC operations.
+ *
+ * @param h the H264 context
+ */
+static void avail_motion(H264Context *h){
+    MpegEncContext * const s = &h->s;
+    const int mb_xy= h->mb_xy;
+    const int mb_type= s->current_picture.mb_type[mb_xy];
+    int refs[2][48];
+    int ref, list;
+
+    memset(refs, -1, sizeof(refs));
+
+    if(IS_16X16(mb_type)){
+        mc_part_y(h, refs, 0, 16, 0,
+                  IS_DIR(mb_type, 0, 0), IS_DIR(mb_type, 0, 1));
+    }else if(IS_16X8(mb_type)){
+        mc_part_y(h, refs, 0, 8, 0,
+                  IS_DIR(mb_type, 0, 0), IS_DIR(mb_type, 0, 1));
+        mc_part_y(h, refs, 8, 8, 8,
+                  IS_DIR(mb_type, 1, 0), IS_DIR(mb_type, 1, 1));
+    }else if(IS_8X16(mb_type)){
+        mc_part_y(h, refs, 0, 16, 0,
+                  IS_DIR(mb_type, 0, 0), IS_DIR(mb_type, 0, 1));
+        mc_part_y(h, refs, 4, 16, 0,
+                  IS_DIR(mb_type, 1, 0), IS_DIR(mb_type, 1, 1));
+    }else{
+        int i;
+
+        assert(IS_8X8(mb_type));
+
+        for(i=0; i<4; i++){
+            const int sub_mb_type= h->sub_mb_type[i];
+            const int n= 4*i;
+            int y_offset= (i&2)<<2;
+
+            if(IS_SUB_8X8(sub_mb_type)){
+                mc_part_y(h, refs, n  , 8, y_offset,
+                          IS_DIR(sub_mb_type, 0, 0), IS_DIR(sub_mb_type, 0, 1));
+            }else if(IS_SUB_8X4(sub_mb_type)){
+                mc_part_y(h, refs, n  , 4, y_offset,
+                          IS_DIR(sub_mb_type, 0, 0), IS_DIR(sub_mb_type, 0, 1));
+                mc_part_y(h, refs, n+2, 4, y_offset+4,
+                          IS_DIR(sub_mb_type, 0, 0), IS_DIR(sub_mb_type, 0, 1));
+            }else if(IS_SUB_4X8(sub_mb_type)){
+                mc_part_y(h, refs, n  , 8, y_offset,
+                          IS_DIR(sub_mb_type, 0, 0), IS_DIR(sub_mb_type, 0, 1));
+                mc_part_y(h, refs, n+1, 8, y_offset,
+                          IS_DIR(sub_mb_type, 0, 0), IS_DIR(sub_mb_type, 0, 1));
+            }else{
+                int j;
+                assert(IS_SUB_4X4(sub_mb_type));
+                for(j=0; j<4; j++){
+                    int sub_y_offset= y_offset + 2*(j&2);
+                    mc_part_y(h, refs, n+j, 4, sub_y_offset,
+                              IS_DIR(sub_mb_type, 0, 0), IS_DIR(sub_mb_type, 0, 1));
+                }
+            }
+        }
+    }
+
+    for(list=1; list>=0; list--){
+        for(ref=0; ref<48; ref++){
+            int row = refs[list][ref];
+            if(row >= 0){
+                Picture *ref_pic = &h->ref_list[list][ref];
+                int ref_field = ref_pic->reference - 1;
+                int ref_field_picture = ref_pic->field_picture;
+                int pic_height = 16*s->mb_height >> ref_field_picture;
+
+                if(!FIELD_PICTURE && ref_field_picture){ // frame referencing two fields
+                    ff_await_field_progress((AVFrame*)ref_pic, FFMIN((row >> 1) - !(row&1), pic_height-1), 1);
+                    ff_await_field_progress((AVFrame*)ref_pic, FFMIN((row >> 1)           , pic_height-1), 0);
+                }else if(FIELD_PICTURE && !ref_field_picture){ // field referencing one field of a frame
+                    ff_await_field_progress((AVFrame*)ref_pic, FFMIN(row*2 + ref_field    , pic_height-1), 0);
+                }else if(FIELD_PICTURE){
+                    ff_await_field_progress((AVFrame*)ref_pic, FFMIN(row, pic_height-1), ref_field);
+                }else{
+                    ff_await_field_progress((AVFrame*)ref_pic, FFMIN(row, pic_height-1), 0);
+                }
+            }
+        }
+    }
+}
+
 static inline void mc_dir_part(H264Context *h, Picture *pic, int n, int square, int chroma_height, int delta, int list,
                            uint8_t *dest_y, uint8_t *dest_cb, uint8_t *dest_cr,
                            int src_x_offset, int src_y_offset,
                            qpel_mc_func *qpix_op, h264_chroma_mc_func chroma_op){
     MpegEncContext * const s = &h->s;
     const int mx= h->mv_cache[list][ scan8[n] ][0] + src_x_offset*8;
     int my=       h->mv_cache[list][ scan8[n] ][1] + src_y_offset*8;
     const int luma_xy= (mx&3) + ((my&3)<<2);
     uint8_t * src_y = pic->data[0] + (mx>>2) + (my>>2)*h->mb_linesize;
     uint8_t * src_cb, * src_cr;
     int extra_width= h->emu_edge_width;
@@ -1802,22 +1938,23 @@
 
 static void hl_motion(H264Context *h, uint8_t *dest_y, uint8_t *dest_cb, uint8_t *dest_cr,
                       qpel_mc_func (*qpix_put)[16], h264_chroma_mc_func (*chroma_put),
                       qpel_mc_func (*qpix_avg)[16], h264_chroma_mc_func (*chroma_avg),
                       h264_weight_func *weight_op, h264_biweight_func *weight_avg){
     MpegEncContext * const s = &h->s;
     const int mb_xy= h->mb_xy;
     const int mb_type= s->current_picture.mb_type[mb_xy];
 
     assert(IS_INTER(mb_type));
 
+    if(USE_FRAME_THREADING(s->avctx)) avail_motion(h);
     prefetch_motion(h, 0);
 
     if(IS_16X16(mb_type)){
         mc_part(h, 0, 1, 8, 0, dest_y, dest_cb, dest_cr, 0, 0,
                 qpix_put[0], chroma_put[0], qpix_avg[0], chroma_avg[0],
                 &weight_op[0], &weight_avg[0],
                 IS_DIR(mb_type, 0, 0), IS_DIR(mb_type, 0, 1));
     }else if(IS_16X8(mb_type)){
         mc_part(h, 0, 0, 4, 8, dest_y, dest_cb, dest_cr, 0, 0,
                 qpix_put[1], chroma_put[0], qpix_avg[1], chroma_avg[0],
                 &weight_op[1], &weight_avg[1],
@@ -2192,33 +2329,128 @@
         avctx->pix_fmt= PIX_FMT_YUVJ420P;
     else if(s->avctx->codec->capabilities&CODEC_CAP_HWACCEL_VDPAU)
         avctx->pix_fmt= PIX_FMT_VDPAU_H264;
     else
         avctx->pix_fmt= PIX_FMT_YUV420P;
 
     decode_init_vlc();
 
     if(avctx->extradata_size > 0 && avctx->extradata &&
        *(char *)avctx->extradata == 1){
         h->is_avc = 1;
-        h->got_avcC = 0;
+        h->got_extradata = 0;
     } else {
         h->is_avc = 0;
     }
 
     h->thread_context[0] = h;
     h->outputed_poc = INT_MIN;
     h->prev_poc_msb= 1<<16;
     return 0;
 }
 
+static void copy_picture_range(Picture **to, Picture **from, int count, MpegEncContext *new_base, MpegEncContext *old_base)
+{
+    int i;
+
+    for (i=0; i<count; i++){
+        to[i] = REBASE_PICTURE(from[i], new_base, old_base);
+    }
+}
+
+static void copy_parameter_set(void **to, void **from, int count, int size)
+{
+    int i;
+
+    for (i=0; i<count; i++){
+        if (to[i] && !from[i]) av_freep(&to[i]);
+        else if (from[i] && !to[i]) to[i] = av_malloc(size);
+
+        if (from[i]) memcpy(to[i], from[i], size);
+    }
+}
+
+static int execute_ref_pic_marking(H264Context *h, MMCO *mmco, int mmco_count);
+
+#define copy_fields(to, from, start_field, end_field) memcpy(&to->start_field, &from->start_field, (char*)&to->end_field - (char*)&to->start_field)
+static int decode_update_context(AVCodecContext *dst, AVCodecContext *src){
+    H264Context *h= dst->priv_data, *h1= src->priv_data;
+    MpegEncContext * const s = &h->s, * const s1 = &h1->s;
+    int inited = s->context_initialized, err;
+
+    if(!s1->context_initialized) return 0;
+
+    err = ff_mpeg_update_context(dst, src);
+    if(err) return err;
+
+    //FIXME handle width/height changing
+    if(!inited){
+        int i;
+        memcpy(&h->s + 1, &h1->s + 1, sizeof(H264Context) - sizeof(MpegEncContext)); //copy all fields after MpegEnc
+        memset(h->sps_buffers, 0, sizeof(h->sps_buffers));
+        memset(h->pps_buffers, 0, sizeof(h->pps_buffers));
+        alloc_tables(h);
+        context_init(h);
+
+        for(i=0; i<2; i++){
+            h->rbsp_buffer[i] = NULL;
+            h->rbsp_buffer_size[i] = 0;
+        }
+
+        h->thread_context[0] = h;
+
+        // frame_start may not be called for the next thread (if it's decoding a bottom field)
+        // so this has to be allocated here
+        h->s.obmc_scratchpad = av_malloc(16*2*s->linesize + 8*2*s->uvlinesize);
+    }
+
+    //extradata/NAL handling
+    h->is_avc          = h1->is_avc;
+    h->got_extradata   = h1->got_extradata;
+
+    //SPS/PPS
+    copy_parameter_set((void**)h->sps_buffers, (void**)h1->sps_buffers, MAX_SPS_COUNT, sizeof(SPS));
+    h->sps             = h1->sps;
+    copy_parameter_set((void**)h->pps_buffers, (void**)h1->pps_buffers, MAX_PPS_COUNT, sizeof(PPS));
+    h->pps             = h1->pps;
+
+    //Dequantization matrices
+    //FIXME these are big - can they be only copied when PPS changes?
+    copy_fields(h, h1, dequant4_buffer, dequant4_coeff);
+
+    //POC timing
+    copy_fields(h, h1, poc_lsb, use_weight);
+
+    //reference lists
+    copy_fields(h, h1, ref_count, intra_gb);
+
+    copy_picture_range(h->short_ref,   h1->short_ref,   32, s, s1);
+    copy_picture_range(h->long_ref,    h1->long_ref,    32,  s, s1);
+    copy_picture_range(h->delayed_pic, h1->delayed_pic, MAX_DELAYED_PIC_COUNT+2, s, s1);
+
+    h->last_slice_type = h1->last_slice_type;
+
+    if(!s->current_picture_ptr) return 0;
+
+    if(!s->dropable) {
+        execute_ref_pic_marking(h, h->mmco, h->mmco_index);
+        h->prev_poc_msb     = h->poc_msb;
+        h->prev_poc_lsb     = h->poc_lsb;
+    }
+    h->prev_frame_num_offset= h->frame_num_offset;
+    h->prev_frame_num       = h->frame_num;
+    if(h->next_output_pic) h->outputed_poc = h->next_output_pic->poc;
+
+    return 0;
+}
+
 static int frame_start(H264Context *h){
     MpegEncContext * const s = &h->s;
     int i;
 
     if(MPV_frame_start(s, s->avctx) < 0)
         return -1;
     ff_er_frame_start(s);
     /*
      * MPV_frame_start uses pict_type to derive key_frame.
      * This is incorrect for H.264; IDR markings must be used.
      * Zero here; IDR markings per slice in frame or fields are ORed in later.
@@ -2234,47 +2466,190 @@
     }
     for(i=0; i<4; i++){
         h->block_offset[16+i]=
         h->block_offset[20+i]= 4*((scan8[i] - scan8[0])&7) + 4*s->uvlinesize*((scan8[i] - scan8[0])>>3);
         h->block_offset[24+16+i]=
         h->block_offset[24+20+i]= 4*((scan8[i] - scan8[0])&7) + 8*s->uvlinesize*((scan8[i] - scan8[0])>>3);
     }
 
     /* can't be in alloc_tables because linesize isn't known there.
      * FIXME: redo bipred weight to not require extra buffer? */
     for(i = 0; i < s->avctx->thread_count; i++)
-        if(!h->thread_context[i]->s.obmc_scratchpad)
+        if(h->thread_context[i] && !h->thread_context[i]->s.obmc_scratchpad)
             h->thread_context[i]->s.obmc_scratchpad = av_malloc(16*2*s->linesize + 8*2*s->uvlinesize);
 
     /* some macroblocks will be accessed before they're available */
-    if(FRAME_MBAFF || s->avctx->thread_count > 1)
+    if(FRAME_MBAFF || USE_AVCODEC_EXECUTE(s->avctx))
         memset(h->slice_table, -1, (s->mb_height*s->mb_stride-1) * sizeof(*h->slice_table));
 
 //    s->decode= (s->flags&CODEC_FLAG_PSNR) || !s->encoding || s->current_picture.reference /*|| h->contains_intra*/ || 1;
 
     // We mark the current picture as non-reference after allocating it, so
     // that if we break out due to an error it can be released automatically
     // in the next MPV_frame_start().
     // SVQ3 as well as most other codecs have only last/next/current and thus
     // get released even with set reference, besides SVQ3 and others do not
     // mark frames as reference later "naturally".
     if(s->codec_id != CODEC_ID_SVQ3)
         s->current_picture_ptr->reference= 0;
 
     s->current_picture_ptr->field_poc[0]=
     s->current_picture_ptr->field_poc[1]= INT_MAX;
+
+    h->next_output_pic = NULL;
+
     assert(s->current_picture_ptr->long_ref==0);
 
     return 0;
 }
 
+/**
+  * Run setup operations that must be run after slice header decoding.
+  * This includes finding the next displayed frame.
+  *
+  * @param h h264 master context
+  */
+static void decode_postinit(H264Context *h){
+    MpegEncContext * const s = &h->s;
+    Picture *out = s->current_picture_ptr;
+    Picture *cur = s->current_picture_ptr;
+    int i, pics, cross_idr, out_of_order, out_idx;
+
+    s->current_picture_ptr->qscale_type= FF_QSCALE_TYPE_H264;
+    s->current_picture_ptr->pict_type= s->pict_type;
+
+    // Don't do anything if it's the first field or we've already been called.
+    // FIXME the first field should call ff_report_frame_setup_done() even if it skips the rest
+    if (h->next_output_pic) return;
+
+    if (cur->field_poc[0]==INT_MAX || cur->field_poc[1]==INT_MAX) {
+        ff_report_frame_setup_done(s->avctx);
+        return;
+    }
+
+    cur->repeat_pict = 0;
+
+    /* Signal interlacing information externally. */
+    /* Prioritize picture timing SEI information over used decoding process if it exists. */
+    if(h->sps.pic_struct_present_flag){
+        switch (h->sei_pic_struct)
+        {
+            case SEI_PIC_STRUCT_FRAME:
+                cur->interlaced_frame = 0;
+                break;
+            case SEI_PIC_STRUCT_TOP_FIELD:
+            case SEI_PIC_STRUCT_BOTTOM_FIELD:
+            case SEI_PIC_STRUCT_TOP_BOTTOM:
+            case SEI_PIC_STRUCT_BOTTOM_TOP:
+                cur->interlaced_frame = 1;
+                break;
+            case SEI_PIC_STRUCT_TOP_BOTTOM_TOP:
+            case SEI_PIC_STRUCT_BOTTOM_TOP_BOTTOM:
+                // Signal the possibility of telecined film externally (pic_struct 5,6)
+                // From these hints, let the applications decide if they apply deinterlacing.
+                cur->repeat_pict = 1;
+                cur->interlaced_frame = FIELD_OR_MBAFF_PICTURE;
+                break;
+            case SEI_PIC_STRUCT_FRAME_DOUBLING:
+                // Force progressive here, as doubling interlaced frame is a bad idea.
+                cur->interlaced_frame = 0;
+                cur->repeat_pict = 2;
+                break;
+            case SEI_PIC_STRUCT_FRAME_TRIPLING:
+                cur->interlaced_frame = 0;
+                cur->repeat_pict = 4;
+                break;
+        }
+    }else{
+        /* Derive interlacing flag from used decoding process. */
+        cur->interlaced_frame = FIELD_OR_MBAFF_PICTURE;
+    }
+
+    if (cur->field_poc[0] != cur->field_poc[1]){
+        /* Derive top_field_first from field pocs. */
+        cur->top_field_first = cur->field_poc[0] < cur->field_poc[1];
+    }else{
+        if(cur->interlaced_frame || h->sps.pic_struct_present_flag){
+            /* Use picture timing SEI information. Even if it is a information of a past frame, better than nothing. */
+            if(h->sei_pic_struct == SEI_PIC_STRUCT_TOP_BOTTOM
+               || h->sei_pic_struct == SEI_PIC_STRUCT_TOP_BOTTOM_TOP)
+                cur->top_field_first = 1;
+            else
+                cur->top_field_first = 0;
+        }else{
+            /* Most likely progressive */
+            cur->top_field_first = 0;
+        }
+    }
+
+    //FIXME do something with unavailable reference frames
+
+    /* Sort B-frames into display order */
+
+    if(h->sps.bitstream_restriction_flag
+       && s->avctx->has_b_frames < h->sps.num_reorder_frames){
+        s->avctx->has_b_frames = h->sps.num_reorder_frames;
+        s->low_delay = 0;
+    }
+
+    if(   s->avctx->strict_std_compliance >= FF_COMPLIANCE_STRICT
+       && !h->sps.bitstream_restriction_flag){
+        s->avctx->has_b_frames= MAX_DELAYED_PIC_COUNT;
+        s->low_delay= 0;
+    }
+
+    pics = 0;
+    while(h->delayed_pic[pics]) pics++;
+
+    assert(pics <= MAX_DELAYED_PIC_COUNT);
+
+    h->delayed_pic[pics++] = cur;
+    if(cur->reference == 0)
+        cur->reference = DELAYED_PIC_REF;
+
+    out = h->delayed_pic[0];
+    out_idx = 0;
+    for(i=1; h->delayed_pic[i] && (h->delayed_pic[i]->poc && !h->delayed_pic[i]->key_frame); i++)
+        if(h->delayed_pic[i]->poc < out->poc){
+            out = h->delayed_pic[i];
+            out_idx = i;
+        }
+    cross_idr = !h->delayed_pic[0]->poc || !!h->delayed_pic[i] || h->delayed_pic[0]->key_frame;
+
+    out_of_order = !cross_idr && out->poc < h->outputed_poc;
+
+    if(h->sps.bitstream_restriction_flag && s->avctx->has_b_frames >= h->sps.num_reorder_frames)
+        { }
+    else if((out_of_order && pics-1 == s->avctx->has_b_frames && s->avctx->has_b_frames < MAX_DELAYED_PIC_COUNT)
+       || (s->low_delay &&
+        ((!cross_idr && out->poc > h->outputed_poc + 2)
+         || cur->pict_type == FF_B_TYPE)))
+    {
+        s->low_delay = 0;
+        s->avctx->has_b_frames++;
+    }
+
+    if(out_of_order || pics > s->avctx->has_b_frames){
+        out->reference &= ~DELAYED_PIC_REF;
+        for(i=out_idx; h->delayed_pic[i]; i++)
+            h->delayed_pic[i] = h->delayed_pic[i+1];
+    }
+    if(!out_of_order && pics > s->avctx->has_b_frames){
+        h->next_output_pic = out;
+    }else{
+        av_log(s->avctx, AV_LOG_DEBUG, "no picture\n");
+    }
+
+    ff_report_frame_setup_done(s->avctx);
+}
+
 static inline void backup_mb_border(H264Context *h, uint8_t *src_y, uint8_t *src_cb, uint8_t *src_cr, int linesize, int uvlinesize, int simple){
     MpegEncContext * const s = &h->s;
     int i;
     int step    = 1;
     int offset  = 1;
     int uvoffset= 1;
     int top_idx = 1;
     int skiplast= 0;
 
     src_y  -=   linesize;
     src_cb -= uvlinesize;
@@ -3555,39 +3930,39 @@
         memcpy(h->zigzag_scan, zigzag_scan, 16*sizeof(uint8_t));
         memcpy(h-> field_scan,  field_scan, 16*sizeof(uint8_t));
     }else{
         for(i=0; i<16; i++){
 #define T(x) (x>>2) | ((x<<2) & 0xF)
             h->zigzag_scan[i] = T(zigzag_scan[i]);
             h-> field_scan[i] = T( field_scan[i]);
 #undef T
         }
     }
     if(s->dsp.h264_idct8_add == ff_h264_idct8_add_c){
-        memcpy(h->zigzag_scan8x8,       ff_zigzag_direct,     64*sizeof(uint8_t));
+        memcpy(h->zigzag_scan8x8,       zigzag_scan8x8,       64*sizeof(uint8_t));
         memcpy(h->zigzag_scan8x8_cavlc, zigzag_scan8x8_cavlc, 64*sizeof(uint8_t));
         memcpy(h->field_scan8x8,        field_scan8x8,        64*sizeof(uint8_t));
         memcpy(h->field_scan8x8_cavlc,  field_scan8x8_cavlc,  64*sizeof(uint8_t));
     }else{
         for(i=0; i<64; i++){
 #define T(x) (x>>3) | ((x&7)<<3)
-            h->zigzag_scan8x8[i]       = T(ff_zigzag_direct[i]);
+            h->zigzag_scan8x8[i]       = T(zigzag_scan8x8[i]);
             h->zigzag_scan8x8_cavlc[i] = T(zigzag_scan8x8_cavlc[i]);
             h->field_scan8x8[i]        = T(field_scan8x8[i]);
             h->field_scan8x8_cavlc[i]  = T(field_scan8x8_cavlc[i]);
 #undef T
         }
     }
     if(h->sps.transform_bypass){ //FIXME same ugly
         h->zigzag_scan_q0          = zigzag_scan;
-        h->zigzag_scan8x8_q0       = ff_zigzag_direct;
+        h->zigzag_scan8x8_q0       = zigzag_scan8x8;
         h->zigzag_scan8x8_cavlc_q0 = zigzag_scan8x8_cavlc;
         h->field_scan_q0           = field_scan;
         h->field_scan8x8_q0        = field_scan8x8;
         h->field_scan8x8_cavlc_q0  = field_scan8x8_cavlc;
     }else{
         h->zigzag_scan_q0          = h->zigzag_scan;
         h->zigzag_scan8x8_q0       = h->zigzag_scan8x8;
         h->zigzag_scan8x8_cavlc_q0 = h->zigzag_scan8x8_cavlc;
         h->field_scan_q0           = h->field_scan;
         h->field_scan8x8_q0        = h->field_scan8x8;
         h->field_scan8x8_cavlc_q0  = h->field_scan8x8_cavlc;
@@ -3728,36 +4103,41 @@
     }
     if (!s->context_initialized) {
         if(h != h0)
             return -1;  // we cant (re-)initialize context during parallel decoding
         if (MPV_common_init(s) < 0)
             return -1;
         s->first_field = 0;
 
         init_scan_tables(h);
         alloc_tables(h);
 
-        for(i = 1; i < s->avctx->thread_count; i++) {
-            H264Context *c;
-            c = h->thread_context[i] = av_malloc(sizeof(H264Context));
-            memcpy(c, h->s.thread_context[i], sizeof(MpegEncContext));
-            memset(&c->s + 1, 0, sizeof(H264Context) - sizeof(MpegEncContext));
-            c->sps = h->sps;
-            c->pps = h->pps;
-            init_scan_tables(c);
-            clone_tables(c, h);
-        }
-
-        for(i = 0; i < s->avctx->thread_count; i++)
-            if(context_init(h->thread_context[i]) < 0)
+        if (!USE_AVCODEC_EXECUTE(s->avctx)) {
+            if (context_init(h) < 0)
                 return -1;
+        } else {
+            for(i = 1; i < s->avctx->thread_count; i++) {
+                H264Context *c;
+                c = h->thread_context[i] = av_malloc(sizeof(H264Context));
+                memcpy(c, h->s.thread_context[i], sizeof(MpegEncContext));
+                memset(&c->s + 1, 0, sizeof(H264Context) - sizeof(MpegEncContext));
+                c->sps = h->sps;
+                c->pps = h->pps;
+                init_scan_tables(c);
+                clone_tables(c, h);
+            }
+
+            for(i = 0; i < s->avctx->thread_count; i++)
+                if(context_init(h->thread_context[i]) < 0)
+                    return -1;
+        }
 
         s->avctx->width = s->width;
         s->avctx->height = s->height;
         s->avctx->sample_aspect_ratio= h->sps.sar;
         if(!s->avctx->sample_aspect_ratio.den)
             s->avctx->sample_aspect_ratio.den = 1;
 
         if(h->sps.timing_info_present_flag){
             s->avctx->time_base= (AVRational){h->sps.num_units_in_tick * 2, h->sps.time_scale};
             if(h->x264_build > 0 && h->x264_build < 44)
                 s->avctx->time_base.den *= 2;
@@ -3776,29 +4156,35 @@
     }else{
         if(get_bits1(&s->gb)) { //field_pic_flag
             s->picture_structure= PICT_TOP_FIELD + get_bits1(&s->gb); //bottom_field_flag
         } else {
             s->picture_structure= PICT_FRAME;
             h->mb_aff_frame = h->sps.mb_aff;
         }
     }
     h->mb_field_decoding_flag= s->picture_structure != PICT_FRAME;
 
     if(h0->current_slice == 0){
+        if(h->frame_num != h->prev_frame_num &&
+          (h->prev_frame_num+1)%(1<<h->sps.log2_max_frame_num) < (h->frame_num - h->sps.ref_frame_count))
+            h->prev_frame_num = h->frame_num - h->sps.ref_frame_count - 1;
+
         while(h->frame_num !=  h->prev_frame_num &&
               h->frame_num != (h->prev_frame_num+1)%(1<<h->sps.log2_max_frame_num)){
             av_log(NULL, AV_LOG_DEBUG, "Frame num gap %d %d\n", h->frame_num, h->prev_frame_num);
             frame_start(h);
             h->prev_frame_num++;
             h->prev_frame_num %= 1<<h->sps.log2_max_frame_num;
             s->current_picture_ptr->frame_num= h->prev_frame_num;
+            ff_report_field_progress((AVFrame*)s->current_picture_ptr, INT_MAX, 0);
+            ff_report_field_progress((AVFrame*)s->current_picture_ptr, INT_MAX, 1);
             execute_ref_pic_marking(h, NULL, 0);
         }
 
         /* See if we have a decoded first field looking for a pair... */
         if (s0->first_field) {
             assert(s0->current_picture_ptr);
             assert(s0->current_picture_ptr->data[0]);
             assert(s0->current_picture_ptr->reference != DELAYED_PIC_REF);
 
             /* figure out if we have a complementary field pair */
             if (!FIELD_PICTURE || s->picture_structure == last_pic_structure) {
@@ -3916,30 +4302,32 @@
             h->list_count= 1;
     }else
         h->list_count= 0;
 
     if(!default_ref_list_done){
         fill_default_ref_list(h);
     }
 
     if(h->slice_type_nos!=FF_I_TYPE && decode_ref_pic_list_reordering(h) < 0)
         return -1;
 
+    /*
     if(h->slice_type_nos!=FF_I_TYPE){
         s->last_picture_ptr= &h->ref_list[0][0];
         ff_copy_picture(&s->last_picture, s->last_picture_ptr);
     }
     if(h->slice_type_nos==FF_B_TYPE){
         s->next_picture_ptr= &h->ref_list[1][0];
         ff_copy_picture(&s->next_picture, s->next_picture_ptr);
     }
+    */
 
     if(   (h->pps.weighted_pred          && h->slice_type_nos == FF_P_TYPE )
        ||  (h->pps.weighted_bipred_idc==1 && h->slice_type_nos== FF_B_TYPE ) )
         pred_weight_table(h);
     else if(h->pps.weighted_bipred_idc==2 && h->slice_type_nos== FF_B_TYPE)
         implicit_weight_table(h);
     else
         h->use_weight = 0;
 
     if(h->nal_ref_idc)
         decode_ref_pic_marking(h0, &s->gb);
@@ -4035,27 +4423,31 @@
         ref2frm[1]= -1;
         for(i=0; i<16; i++)
             ref2frm[i+2]= 4*h->ref_list[j][i].frame_num
                           +(h->ref_list[j][i].reference&3);
         ref2frm[18+0]=
         ref2frm[18+1]= -1;
         for(i=16; i<48; i++)
             ref2frm[i+4]= 4*h->ref_list[j][i].frame_num
                           +(h->ref_list[j][i].reference&3);
     }
 
-    h->emu_edge_width= (s->flags&CODEC_FLAG_EMU_EDGE) ? 0 : 16;
+    //FIXME: fix draw_edges+PAFF+frame threads
+    h->emu_edge_width= (s->flags&CODEC_FLAG_EMU_EDGE || (!h->sps.frame_mbs_only_flag && USE_FRAME_THREADING(s->avctx))) ? 0 : 16;
     h->emu_edge_height= (FRAME_MBAFF || FIELD_PICTURE) ? 0 : h->emu_edge_width;
 
     s->avctx->refs= h->sps.ref_frame_count;
 
+    if(!(s->flags2 & CODEC_FLAG2_CHUNKS) && h->slice_num==1)
+        decode_postinit(h);
+
     if(s->avctx->debug&FF_DEBUG_PICT_INFO){
         av_log(h->s.avctx, AV_LOG_DEBUG, "slice:%d %s mb:%d %c%s%s pps:%u frame:%d poc:%d/%d ref:%d/%d qp:%d loop:%d:%d:%d weight:%d%s %s\n",
                h->slice_num,
                (s->picture_structure==PICT_FRAME ? "F" : s->picture_structure==PICT_TOP_FIELD ? "T" : "B"),
                first_mb_in_slice,
                av_get_pict_type_char(h->slice_type), h->slice_type_fixed ? " fix" : "", h->nal_unit_type == NAL_IDR_SLICE ? " IDR" : "",
                pps_id, h->frame_num,
                s->current_picture_ptr->field_poc[0], s->current_picture_ptr->field_poc[1],
                h->ref_count[0], h->ref_count[1],
                s->qscale,
                h->deblocking_filter, h->slice_alpha_c0_offset/2, h->slice_beta_offset/2,
@@ -6574,22 +6966,56 @@
     }
 
 #if CONFIG_SMALL
     for( dir = 0; dir < 2; dir++ )
         filter_mb_dir(h, mb_x, mb_y, img_y, img_cb, img_cr, linesize, uvlinesize, mb_xy, mb_type, mvy_limit, dir ? 0 : first_vertical_edge_done, dir);
 #else
     filter_mb_dir(h, mb_x, mb_y, img_y, img_cb, img_cr, linesize, uvlinesize, mb_xy, mb_type, mvy_limit, first_vertical_edge_done, 0);
     filter_mb_dir(h, mb_x, mb_y, img_y, img_cb, img_cr, linesize, uvlinesize, mb_xy, mb_type, mvy_limit, 0, 1);
 #endif
 }
 
+/**
+ * Draw edges and report progress for the last MB row.
+ */
+static void decode_finish_row(H264Context *h){
+    MpegEncContext * const s = &h->s;
+    int top = 16*(s->mb_y >> FIELD_PICTURE);
+    int height = 16 << FRAME_MBAFF;
+    int deblock_border = (16 + 4) << FRAME_MBAFF;
+    int pic_height = 16*s->mb_height >> FIELD_PICTURE;
+
+    if (h->deblocking_filter) {
+        if((top + height) >= pic_height)
+            height += deblock_border;
+
+        top -= deblock_border;
+    }
+
+    if (top >= pic_height || (top + height) < h->emu_edge_height)
+        return;
+
+    height = FFMIN(height, pic_height - top);
+    if (top < h->emu_edge_height) {
+        height = top+height;
+        top = 0;
+    }
+
+    ff_draw_horiz_band(s, top, height);
+
+    if (s->dropable) return;
+
+    ff_report_field_progress((AVFrame*)s->current_picture_ptr, top + height - 1,
+                             s->picture_structure==PICT_BOTTOM_FIELD);
+}
+
 static int decode_slice(struct AVCodecContext *avctx, void *arg){
     H264Context *h = *(void**)arg;
     MpegEncContext * const s = &h->s;
     const int part_mask= s->partitioned_frame ? (AC_END|AC_ERROR) : 0x7F;
 
     s->mb_skip_run= -1;
 
     h->is_complex = FRAME_MBAFF || s->picture_structure != PICT_FRAME || s->codec_id != CODEC_ID_H264 ||
                     (CONFIG_GRAY && (s->flags&CODEC_FLAG_GRAY)) || (CONFIG_H264_ENCODER && s->encoding);
 
     if( h->pps.cabac ) {
@@ -6635,23 +7061,23 @@
             }
             eos = get_cabac_terminate( &h->cabac );
 
             if( ret < 0 || h->cabac.bytestream > h->cabac.bytestream_end + 2) {
                 av_log(h->s.avctx, AV_LOG_ERROR, "error while decoding MB %d %d, bytestream (%td)\n", s->mb_x, s->mb_y, h->cabac.bytestream_end - h->cabac.bytestream);
                 ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x, s->mb_y, (AC_ERROR|DC_ERROR|MV_ERROR)&part_mask);
                 return -1;
             }
 
             if( ++s->mb_x >= s->mb_width ) {
                 s->mb_x = 0;
-                ff_draw_horiz_band(s, 16*s->mb_y, 16);
+                decode_finish_row(h);
                 ++s->mb_y;
                 if(FIELD_OR_MBAFF_PICTURE) {
                     ++s->mb_y;
                 }
             }
 
             if( eos || s->mb_y >= s->mb_height ) {
                 tprintf(s->avctx, "slice end %d %d\n", get_bits_count(&s->gb), s->gb.size_in_bits);
                 ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x-1, s->mb_y, (AC_END|DC_END|MV_END)&part_mask);
                 return 0;
             }
@@ -6672,23 +7098,23 @@
             }
 
             if(ret<0){
                 av_log(h->s.avctx, AV_LOG_ERROR, "error while decoding MB %d %d\n", s->mb_x, s->mb_y);
                 ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x, s->mb_y, (AC_ERROR|DC_ERROR|MV_ERROR)&part_mask);
 
                 return -1;
             }
 
             if(++s->mb_x >= s->mb_width){
                 s->mb_x=0;
-                ff_draw_horiz_band(s, 16*s->mb_y, 16);
+                decode_finish_row(h);
                 ++s->mb_y;
                 if(FIELD_OR_MBAFF_PICTURE) {
                     ++s->mb_y;
                 }
                 if(s->mb_y >= s->mb_height){
                     tprintf(s->avctx, "slice end %d %d\n", get_bits_count(&s->gb), s->gb.size_in_bits);
 
                     if(get_bits_count(&s->gb) == s->gb.size_in_bits ) {
                         ff_er_add_slice(s, s->resync_mb_x, s->resync_mb_y, s->mb_x-1, s->mb_y, (AC_END|DC_END|MV_END)&part_mask);
 
                         return 0;
@@ -6973,23 +7399,23 @@
             return -1;
         }
     }
 
     return 0;
 }
 
 static void decode_scaling_list(H264Context *h, uint8_t *factors, int size,
                                 const uint8_t *jvt_list, const uint8_t *fallback_list){
     MpegEncContext * const s = &h->s;
     int i, last = 8, next = 8;
-    const uint8_t *scan = size == 16 ? zigzag_scan : ff_zigzag_direct;
+    const uint8_t *scan = size == 16 ? zigzag_scan : zigzag_scan8x8;
     if(!get_bits1(&s->gb)) /* matrix not written, we use the predicted one */
         memcpy(factors, fallback_list, size*sizeof(uint8_t));
     else
     for(i=0;i<size;i++){
         if(next)
             next = (last + get_se_golomb(&s->gb)) & 0xff;
         if(!i && !next){ /* matrix not written, we use the preset one */
             memcpy(factors, jvt_list, size*sizeof(uint8_t));
             break;
         }
         last = factors[scan[i]] = next ? next : last;
@@ -7319,23 +7745,23 @@
     }
 }
 
 
 static int decode_nal_units(H264Context *h, const uint8_t *buf, int buf_size){
     MpegEncContext * const s = &h->s;
     AVCodecContext * const avctx= s->avctx;
     int buf_index=0;
     H264Context *hx; ///< thread context
     int context_count = 0;
 
-    h->max_contexts = avctx->thread_count;
+    h->max_contexts = USE_AVCODEC_EXECUTE(s->avctx) ? avctx->thread_count : 1;
 #if 0
     int i;
     for(i=0; i<50; i++){
         av_log(NULL, AV_LOG_ERROR,"%02X ", buf[i]);
     }
 #endif
     if(!(s->flags2 & CODEC_FLAG2_CHUNKS)){
         h->current_slice = 0;
         if (!s->first_field)
             s->current_picture_ptr= NULL;
     }
@@ -7530,43 +7956,45 @@
     AVFrame *pict = data;
     int buf_index;
 
     s->flags= avctx->flags;
     s->flags2= avctx->flags2;
 
    /* end of stream, output what is still in the buffers */
     if (buf_size == 0) {
         Picture *out;
         int i, out_idx;
 
-//FIXME factorize this with the output code below
+        s->current_picture_ptr = NULL;
+
+//FIXME factorize this with the output code
         out = h->delayed_pic[0];
         out_idx = 0;
         for(i=1; h->delayed_pic[i] && (h->delayed_pic[i]->poc && !h->delayed_pic[i]->key_frame); i++)
             if(h->delayed_pic[i]->poc < out->poc){
                 out = h->delayed_pic[i];
                 out_idx = i;
             }
 
         for(i=out_idx; h->delayed_pic[i]; i++)
             h->delayed_pic[i] = h->delayed_pic[i+1];
 
         if(out){
             *data_size = sizeof(AVFrame);
             *pict= *(AVFrame*)out;
         }
 
         return 0;
     }
 
-    if(h->is_avc && !h->got_avcC) {
+    if(h->is_avc && !h->got_extradata) {
         int i, cnt, nalsize;
         unsigned char *p = avctx->extradata;
         if(avctx->extradata_size < 7) {
             av_log(avctx, AV_LOG_ERROR, "avcC too short\n");
             return -1;
         }
         if(*p != 1) {
             av_log(avctx, AV_LOG_ERROR, "Unknown avcC version %d\n", *p);
             return -1;
         }
         /* sps and pps in the avcC always have length coded with 2 bytes,
@@ -7588,202 +8016,90 @@
         for (i = 0; i < cnt; i++) {
             nalsize = AV_RB16(p) + 2;
             if(decode_nal_units(h, p, nalsize)  != nalsize) {
                 av_log(avctx, AV_LOG_ERROR, "Decoding pps %d from avcC failed\n", i);
                 return -1;
             }
             p += nalsize;
         }
         // Now store right nal length size, that will be use to parse all other nals
         h->nal_length_size = ((*(((char*)(avctx->extradata))+4))&0x03)+1;
         // Do not reparse avcC
-        h->got_avcC = 1;
+        h->got_extradata = 1;
     }
 
-    if(!h->got_avcC && !h->is_avc && s->avctx->extradata_size){
+    if(!h->got_extradata && !h->is_avc && s->avctx->extradata_size){
         if(decode_nal_units(h, s->avctx->extradata, s->avctx->extradata_size) < 0)
             return -1;
-        h->got_avcC = 1;
+        h->got_extradata = 1;
     }
 
     buf_index=decode_nal_units(h, buf, buf_size);
     if(buf_index < 0)
         return -1;
 
     if(!(s->flags2 & CODEC_FLAG2_CHUNKS) && !s->current_picture_ptr){
         if (avctx->skip_frame >= AVDISCARD_NONREF || s->hurry_up) return 0;
         av_log(avctx, AV_LOG_ERROR, "no frame!\n");
         return -1;
     }
 
     if(!(s->flags2 & CODEC_FLAG2_CHUNKS) || (s->mb_y >= s->mb_height && s->mb_height)){
-        Picture *out = s->current_picture_ptr;
-        Picture *cur = s->current_picture_ptr;
-        int i, pics, cross_idr, out_of_order, out_idx;
 
-        s->mb_y= 0;
+        if(s->flags2 & CODEC_FLAG2_CHUNKS) decode_postinit(h);
 
-        s->current_picture_ptr->qscale_type= FF_QSCALE_TYPE_H264;
-        s->current_picture_ptr->pict_type= s->pict_type;
+        s->mb_y= 0;
+        ff_report_field_progress((AVFrame*)s->current_picture_ptr, (16*s->mb_height >> FIELD_PICTURE) - 1,
+                                 s->picture_structure==PICT_BOTTOM_FIELD);
 
         if (CONFIG_H264_VDPAU_DECODER && s->avctx->codec->capabilities&CODEC_CAP_HWACCEL_VDPAU)
             ff_vdpau_h264_set_reference_frames(s);
 
-        if(!s->dropable) {
-            execute_ref_pic_marking(h, h->mmco, h->mmco_index);
-            h->prev_poc_msb= h->poc_msb;
-            h->prev_poc_lsb= h->poc_lsb;
+        if(!USE_FRAME_THREADING(avctx)){
+            if(!s->dropable) {
+                execute_ref_pic_marking(h, h->mmco, h->mmco_index);
+                h->prev_poc_msb= h->poc_msb;
+                h->prev_poc_lsb= h->poc_lsb;
+            }
+            h->prev_frame_num_offset= h->frame_num_offset;
+            h->prev_frame_num= h->frame_num;
+            if(h->next_output_pic) h->outputed_poc = h->next_output_pic->poc;
         }
-        h->prev_frame_num_offset= h->frame_num_offset;
-        h->prev_frame_num= h->frame_num;
 
         if (CONFIG_H264_VDPAU_DECODER && s->avctx->codec->capabilities&CODEC_CAP_HWACCEL_VDPAU)
             ff_vdpau_h264_picture_complete(s);
 
         /*
          * FIXME: Error handling code does not seem to support interlaced
          * when slices span multiple rows
          * The ff_er_add_slice calls don't work right for bottom
          * fields; they cause massive erroneous error concealing
          * Error marking covers both fields (top and bottom).
          * This causes a mismatched s->error_count
          * and a bad error table. Further, the error count goes to
          * INT_MAX when called for bottom field, because mb_y is
          * past end by one (callers fault) and resync_mb_y != 0
          * causes problems for the first MB line, too.
          */
         if (!FIELD_PICTURE)
             ff_er_frame_end(s);
 
         MPV_frame_end(s);
 
-        if (cur->field_poc[0]==INT_MAX || cur->field_poc[1]==INT_MAX) {
+        if (!h->next_output_pic) {
             /* Wait for second field. */
             *data_size = 0;
 
         } else {
-            cur->repeat_pict = 0;
-
-            /* Signal interlacing information externally. */
-            /* Prioritize picture timing SEI information over used decoding process if it exists. */
-            if(h->sps.pic_struct_present_flag){
-                switch (h->sei_pic_struct)
-                {
-                case SEI_PIC_STRUCT_FRAME:
-                    cur->interlaced_frame = 0;
-                    break;
-                case SEI_PIC_STRUCT_TOP_FIELD:
-                case SEI_PIC_STRUCT_BOTTOM_FIELD:
-                case SEI_PIC_STRUCT_TOP_BOTTOM:
-                case SEI_PIC_STRUCT_BOTTOM_TOP:
-                    cur->interlaced_frame = 1;
-                    break;
-                case SEI_PIC_STRUCT_TOP_BOTTOM_TOP:
-                case SEI_PIC_STRUCT_BOTTOM_TOP_BOTTOM:
-                    // Signal the possibility of telecined film externally (pic_struct 5,6)
-                    // From these hints, let the applications decide if they apply deinterlacing.
-                    cur->repeat_pict = 1;
-                    cur->interlaced_frame = FIELD_OR_MBAFF_PICTURE;
-                    break;
-                case SEI_PIC_STRUCT_FRAME_DOUBLING:
-                    // Force progressive here, as doubling interlaced frame is a bad idea.
-                    cur->interlaced_frame = 0;
-                    cur->repeat_pict = 2;
-                    break;
-                case SEI_PIC_STRUCT_FRAME_TRIPLING:
-                    cur->interlaced_frame = 0;
-                    cur->repeat_pict = 4;
-                    break;
-                }
-            }else{
-                /* Derive interlacing flag from used decoding process. */
-                cur->interlaced_frame = FIELD_OR_MBAFF_PICTURE;
-            }
-
-            if (cur->field_poc[0] != cur->field_poc[1]){
-                /* Derive top_field_first from field pocs. */
-                cur->top_field_first = cur->field_poc[0] < cur->field_poc[1];
-            }else{
-                if(cur->interlaced_frame || h->sps.pic_struct_present_flag){
-                    /* Use picture timing SEI information. Even if it is a information of a past frame, better than nothing. */
-                    if(h->sei_pic_struct == SEI_PIC_STRUCT_TOP_BOTTOM
-                      || h->sei_pic_struct == SEI_PIC_STRUCT_TOP_BOTTOM_TOP)
-                        cur->top_field_first = 1;
-                    else
-                        cur->top_field_first = 0;
-                }else{
-                    /* Most likely progressive */
-                    cur->top_field_first = 0;
-                }
-            }
-
-        //FIXME do something with unavailable reference frames
-
-            /* Sort B-frames into display order */
-
-            if(h->sps.bitstream_restriction_flag
-               && s->avctx->has_b_frames < h->sps.num_reorder_frames){
-                s->avctx->has_b_frames = h->sps.num_reorder_frames;
-                s->low_delay = 0;
-            }
-
-            if(   s->avctx->strict_std_compliance >= FF_COMPLIANCE_STRICT
-               && !h->sps.bitstream_restriction_flag){
-                s->avctx->has_b_frames= MAX_DELAYED_PIC_COUNT;
-                s->low_delay= 0;
-            }
-
-            pics = 0;
-            while(h->delayed_pic[pics]) pics++;
-
-            assert(pics <= MAX_DELAYED_PIC_COUNT);
-
-            h->delayed_pic[pics++] = cur;
-            if(cur->reference == 0)
-                cur->reference = DELAYED_PIC_REF;
-
-            out = h->delayed_pic[0];
-            out_idx = 0;
-            for(i=1; h->delayed_pic[i] && (h->delayed_pic[i]->poc && !h->delayed_pic[i]->key_frame); i++)
-                if(h->delayed_pic[i]->poc < out->poc){
-                    out = h->delayed_pic[i];
-                    out_idx = i;
-                }
-            cross_idr = !h->delayed_pic[0]->poc || !!h->delayed_pic[i] || h->delayed_pic[0]->key_frame;
-
-            out_of_order = !cross_idr && out->poc < h->outputed_poc;
-
-            if(h->sps.bitstream_restriction_flag && s->avctx->has_b_frames >= h->sps.num_reorder_frames)
-                { }
-            else if((out_of_order && pics-1 == s->avctx->has_b_frames && s->avctx->has_b_frames < MAX_DELAYED_PIC_COUNT)
-               || (s->low_delay &&
-                ((!cross_idr && out->poc > h->outputed_poc + 2)
-                 || cur->pict_type == FF_B_TYPE)))
-            {
-                s->low_delay = 0;
-                s->avctx->has_b_frames++;
-            }
-
-            if(out_of_order || pics > s->avctx->has_b_frames){
-                out->reference &= ~DELAYED_PIC_REF;
-                for(i=out_idx; h->delayed_pic[i]; i++)
-                    h->delayed_pic[i] = h->delayed_pic[i+1];
-            }
-            if(!out_of_order && pics > s->avctx->has_b_frames){
-                *data_size = sizeof(AVFrame);
-
-                h->outputed_poc = out->poc;
-                *pict= *(AVFrame*)out;
-            }else{
-                av_log(avctx, AV_LOG_DEBUG, "no picture\n");
-            }
+            *data_size = sizeof(AVFrame);
+            *pict = *(AVFrame*)h->next_output_pic;
         }
     }
 
     assert(pict->data[0] || !*data_size);
     ff_print_debug_info(s, pict);
 //printf("out %d\n", (int)pict->data[0]);
 #if 0 //?
 
     /* Return the Picture timestamp as the frame number */
     /* we subtract 1 because it is added on utils.c     */
     avctx->frame_number = s->picture_number - 1;
@@ -8011,25 +8327,26 @@
 
 
 AVCodec h264_decoder = {
     "h264",
     CODEC_TYPE_VIDEO,
     CODEC_ID_H264,
     sizeof(H264Context),
     decode_init,
     NULL,
     decode_end,
     decode_frame,
-    /*CODEC_CAP_DRAW_HORIZ_BAND |*/ CODEC_CAP_DR1 | CODEC_CAP_DELAY,
+    /*CODEC_CAP_DRAW_HORIZ_BAND |*/ CODEC_CAP_DR1 | CODEC_CAP_DELAY | CODEC_CAP_FRAME_THREADS,
     .flush= flush_dpb,
     .long_name = NULL_IF_CONFIG_SMALL("H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10"),
+    .update_context = ONLY_IF_THREADS_ENABLED(decode_update_context)
 };
 
 #if CONFIG_H264_VDPAU_DECODER
 AVCodec h264_vdpau_decoder = {
     "h264_vdpau",
     CODEC_TYPE_VIDEO,
     CODEC_ID_H264,
     sizeof(H264Context),
     decode_init,
     NULL,
     decode_end,
--- h264.h	Sun Jan 25 13:18:59 2009
+++ h264.h	Sun Jan 25 13:07:21 2009
@@ -231,23 +231,23 @@
 typedef struct H264Context{
     MpegEncContext s;
     int nal_ref_idc;
     int nal_unit_type;
     uint8_t *rbsp_buffer[2];
     unsigned int rbsp_buffer_size[2];
 
     /**
       * Used to parse AVC variant of h264
       */
     int is_avc; ///< this flag is != 0 if codec is avc1
-    int got_avcC; ///< flag used to parse avcC data only once
+    int got_extradata; ///< flag used to parse extradata only once
     int nal_length_size; ///< Number of bytes used for nal length (1, 2 or 4)
 
     int chroma_qp[2]; //QPc
 
     int prev_mb_skipped;
     int next_mb_skipped;
 
     //prediction stuff
     int chroma_pred_mode;
     int intra16x16_pred_mode;
 
@@ -392,22 +392,23 @@
      */
     unsigned int ref_count[2];   ///< counts frames or fields, depending on current mb mode
     unsigned int list_count;
     Picture *short_ref[32];
     Picture *long_ref[32];
     Picture default_ref_list[2][32]; ///< base reference list for all slices of a coded picture
     Picture ref_list[2][48];         /**< 0..15: frame refs, 16..47: mbaff field refs.
                                           Reordered version of default_ref_list
                                           according to picture reordering in slice header */
     int ref2frm[MAX_SLICES][2][64];  ///< reference to frame number lists, used in the loop filter, the first 2 are for -2,-1
     Picture *delayed_pic[MAX_DELAYED_PIC_COUNT+2]; //FIXME size?
+    Picture *next_output_pic;
     int outputed_poc;
 
     /**
      * memory management control operations buffer.
      */
     MMCO mmco[MAX_MMCO_COUNT];
     int mmco_index;
 
     int long_ref_count;  ///< number of actual long term references
     int short_ref_count; ///< number of actual short term references
 
--- h264data.h	Sun Jan 25 13:18:59 2009
+++ h264data.h	Sun Jan 25 13:07:21 2009
@@ -278,22 +278,41 @@
  0*16 + 0*64, 2*16 + 0*64, 1*16 + 0*64, 0*16 + 2*64,
  2*16 + 2*64, 3*16 + 0*64, 1*16 + 2*64, 3*16 + 2*64,
  0*16 + 1*64, 2*16 + 1*64, 0*16 + 3*64, 2*16 + 3*64,
  1*16 + 1*64, 3*16 + 1*64, 1*16 + 3*64, 3*16 + 3*64,
 };
 
 static const uint8_t chroma_dc_scan[4]={
  (0+0*2)*16, (1+0*2)*16,
  (0+1*2)*16, (1+1*2)*16,  //FIXME
 };
 
+static const uint8_t zigzag_scan8x8[64]={
+ 0+0*8, 1+0*8, 0+1*8, 0+2*8,
+ 1+1*8, 2+0*8, 3+0*8, 2+1*8,
+ 1+2*8, 0+3*8, 0+4*8, 1+3*8,
+ 2+2*8, 3+1*8, 4+0*8, 5+0*8,
+ 4+1*8, 3+2*8, 2+3*8, 1+4*8,
+ 0+5*8, 0+6*8, 1+5*8, 2+4*8,
+ 3+3*8, 4+2*8, 5+1*8, 6+0*8,
+ 7+0*8, 6+1*8, 5+2*8, 4+3*8,
+ 3+4*8, 2+5*8, 1+6*8, 0+7*8,
+ 1+7*8, 2+6*8, 3+5*8, 4+4*8,
+ 5+3*8, 6+2*8, 7+1*8, 7+2*8,
+ 6+3*8, 5+4*8, 4+5*8, 3+6*8,
+ 2+7*8, 3+7*8, 4+6*8, 5+5*8,
+ 6+4*8, 7+3*8, 7+4*8, 6+5*8,
+ 5+6*8, 4+7*8, 5+7*8, 6+6*8,
+ 7+5*8, 7+6*8, 6+7*8, 7+7*8,
+};
+
 // zigzag_scan8x8_cavlc[i] = zigzag_scan8x8[(i/4) + 16*(i%4)]
 static const uint8_t zigzag_scan8x8_cavlc[64]={
  0+0*8, 1+1*8, 1+2*8, 2+2*8,
  4+1*8, 0+5*8, 3+3*8, 7+0*8,
  3+4*8, 1+7*8, 5+3*8, 6+3*8,
  2+7*8, 6+4*8, 5+6*8, 7+5*8,
  1+0*8, 2+0*8, 0+3*8, 3+1*8,
  3+2*8, 0+6*8, 4+2*8, 6+1*8,
  2+5*8, 2+6*8, 6+2*8, 5+4*8,
  3+7*8, 7+3*8, 4+7*8, 7+6*8,
  0+1*8, 3+0*8, 0+4*8, 4+0*8,
--- Makefile	Sun Jan 25 13:18:58 2009
+++ Makefile	Sun Jan 25 13:07:19 2009
@@ -74,23 +74,23 @@
 OBJS-$(CONFIG_EACMV_DECODER)           += eacmv.o
 OBJS-$(CONFIG_EATGQ_DECODER)           += eatgq.o eaidct.o
 OBJS-$(CONFIG_EATGV_DECODER)           += eatgv.o
 OBJS-$(CONFIG_EIGHTBPS_DECODER)        += 8bps.o
 OBJS-$(CONFIG_EIGHTSVX_EXP_DECODER)    += 8svx.o
 OBJS-$(CONFIG_EIGHTSVX_FIB_DECODER)    += 8svx.o
 OBJS-$(CONFIG_ESCAPE124_DECODER)       += escape124.o
 OBJS-$(CONFIG_FFV1_DECODER)            += ffv1.o rangecoder.o
 OBJS-$(CONFIG_FFV1_ENCODER)            += ffv1.o rangecoder.o
 OBJS-$(CONFIG_FFVHUFF_DECODER)         += huffyuv.o
 OBJS-$(CONFIG_FFVHUFF_ENCODER)         += huffyuv.o
-OBJS-$(CONFIG_FLAC_DECODER)            += flacdec.o
+OBJS-$(CONFIG_FLAC_DECODER)            += flac.o
 OBJS-$(CONFIG_FLAC_ENCODER)            += flacenc.o lpc.o
 OBJS-$(CONFIG_FLASHSV_DECODER)         += flashsv.o
 OBJS-$(CONFIG_FLASHSV_ENCODER)         += flashsvenc.o
 OBJS-$(CONFIG_FLIC_DECODER)            += flicvideo.o
 OBJS-$(CONFIG_FLV_DECODER)             += h263dec.o h263.o mpeg12data.o mpegvideo.o error_resilience.o
 OBJS-$(CONFIG_FLV_ENCODER)             += mpegvideo_enc.o motion_est.o ratecontrol.o h263.o mpeg12data.o mpegvideo.o error_resilience.o
 OBJS-$(CONFIG_FOURXM_DECODER)          += 4xm.o
 OBJS-$(CONFIG_FRAPS_DECODER)           += fraps.o huffman.o
 OBJS-$(CONFIG_GIF_DECODER)             += gifdec.o lzw.o
 OBJS-$(CONFIG_GIF_ENCODER)             += gif.o
 OBJS-$(CONFIG_H261_DECODER)            += h261dec.o h261.o mpegvideo.o error_resilience.o
--- mdec.c	Sun Jan 25 13:18:59 2009
+++ mdec.c	Sun Jan 25 13:07:22 2009
@@ -226,34 +226,47 @@
     mdec_common_init(avctx);
     ff_mpeg12_init_vlcs();
     ff_init_scantable(a->dsp.idct_permutation, &a->scantable, ff_zigzag_direct);
 
     p->qstride= a->mb_width;
     p->qscale_table= av_mallocz( p->qstride * a->mb_height);
     avctx->pix_fmt= PIX_FMT_YUV420P;
 
     return 0;
 }
 
+static av_cold int decode_init_copy(AVCodecContext *avctx){
+    MDECContext * const a = avctx->priv_data;
+    AVFrame *p = (AVFrame*)&a->picture;
+
+    avctx->coded_frame= p;
+    a->avctx= avctx;
+
+    p->qscale_table= av_mallocz( p->qstride * a->mb_height);
+
+    return 0;
+}
+
 static av_cold int decode_end(AVCodecContext *avctx){
     MDECContext * const a = avctx->priv_data;
 
     av_freep(&a->bitstream_buffer);
     av_freep(&a->picture.qscale_table);
     a->bitstream_buffer_size=0;
 
     return 0;
 }
 
 AVCodec mdec_decoder = {
     "mdec",
     CODEC_TYPE_VIDEO,
     CODEC_ID_MDEC,
     sizeof(MDECContext),
     decode_init,
     NULL,
     decode_end,
     decode_frame,
-    CODEC_CAP_DR1,
+    CODEC_CAP_DR1 | CODEC_CAP_FRAME_THREADS,
     .long_name= NULL_IF_CONFIG_SMALL("Sony PlayStation MDEC (Motion DECoder)"),
+    .init_copy= ONLY_IF_THREADS_ENABLED(decode_init_copy)
 };
 
--- mimic.c	Sun Jan 25 13:18:59 2009
+++ mimic.c	Sun Jan 25 13:07:22 2009
@@ -19,22 +19,23 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
 #include <stdlib.h>
 #include <string.h>
 #include <stdint.h>
 
 #include "avcodec.h"
 #include "bitstream.h"
 #include "bytestream.h"
 #include "dsputil.h"
+#include "thread.h"
 
 #define MIMIC_HEADER_SIZE   20
 
 typedef struct {
     AVCodecContext *avctx;
 
     int             num_vblocks[3];
     int             num_hblocks[3];
 
     uint8_t        *swap_buf;
     int             swap_buf_size;
@@ -43,22 +44,26 @@
     int             prev_index;
 
     AVFrame         buf_ptrs    [16];
     AVPicture       flipped_ptrs[16];
 
     DECLARE_ALIGNED_16(DCTELEM, dct_block[64]);
 
     GetBitContext   gb;
     ScanTable       scantable;
     DSPContext      dsp;
     VLC             vlc;
+
+    /* Kept in the context so multithreading can have a constant to read from */
+    int             next_cur_index;
+    int             next_prev_index;
 } MimicContext;
 
 static const uint32_t huffcodes[] = {
     0x0000000a, 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000,
     0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000,
     0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x0000000b,
     0x0000001b, 0x00000038, 0x00000078, 0x00000079, 0x0000007a, 0x000000f9,
     0x000000fa, 0x000003fb, 0x000007f8, 0x000007f9, 0x000007fa, 0x000007fb,
     0x00000ff8, 0x00000ff9, 0x00000001, 0x00000039, 0x0000007b, 0x000000fb,
     0x000001f8, 0x000001f9, 0x00000ffa, 0x00000ffb, 0x00001ff8, 0x00001ff9,
     0x00001ffa, 0x00001ffb, 0x00003ff8, 0x00003ff9, 0x00003ffa, 0x00000000,
@@ -113,22 +118,37 @@
     if(init_vlc(&ctx->vlc, 11, FF_ARRAY_ELEMS(huffbits),
                  huffbits, 1, 1, huffcodes, 4, 4, 0)) {
         av_log(avctx, AV_LOG_ERROR, "error initializing vlc table\n");
         return -1;
     }
     dsputil_init(&ctx->dsp, avctx);
     ff_init_scantable(ctx->dsp.idct_permutation, &ctx->scantable, col_zag);
 
     return 0;
 }
 
+static int mimic_decode_update_context(AVCodecContext *avctx, AVCodecContext *avctx_from)
+{
+    MimicContext *dst = avctx->priv_data, *src = avctx_from->priv_data;
+
+    dst->cur_index  = src->next_cur_index;
+    dst->prev_index = src->next_prev_index;
+
+    memcpy(dst->buf_ptrs, src->buf_ptrs, sizeof(src->buf_ptrs));
+    memcpy(dst->flipped_ptrs, src->flipped_ptrs, sizeof(src->flipped_ptrs));
+
+    memset(&dst->buf_ptrs[dst->cur_index], 0, sizeof(AVFrame));
+
+    return 0;
+}
+
 const static int8_t vlcdec_lookup[9][64] = {
     {    0, },
     {   -1,   1, },
     {   -3,   3,   -2,   2, },
     {   -7,   7,   -6,   6,   -5,   5,   -4,   4, },
     {  -15,  15,  -14,  14,  -13,  13,  -12,  12,
        -11,  11,  -10,  10,   -9,   9,   -8,   8, },
     {  -31,  31,  -30,  30,  -29,  29,  -28,  28,
        -27,  27,  -26,  26,  -25,  25,  -24,  24,
        -23,  23,  -22,  22,  -21,  21,  -20,  20,
        -19,  19,  -18,  18,  -17,  17,  -16,  16, },
@@ -197,23 +217,23 @@
             coeff = (coeff * qscale) / 1001;
 
         block[ctx->scantable.permutated[pos]] = coeff;
     }
 
     return 1;
 }
 
 static int decode(MimicContext *ctx, int quality, int num_coeffs,
                   int is_iframe)
 {
-    int y, x, plane;
+    int y, x, plane, cur_row = 0;
 
     for(plane = 0; plane < 3; plane++) {
         const int is_chroma = !!plane;
         const int qscale = av_clip(10000-quality,is_chroma?1000:2000,10000)<<2;
         const int stride = ctx->flipped_ptrs[ctx->cur_index].linesize[plane];
         const uint8_t *src = ctx->flipped_ptrs[ctx->prev_index].data[plane];
         uint8_t       *dst = ctx->flipped_ptrs[ctx->cur_index ].data[plane];
 
         for(y = 0; y < ctx->num_vblocks[plane]; y++) {
             for(x = 0; x < ctx->num_hblocks[plane]; x++) {
 
@@ -228,39 +248,43 @@
                      * Chroma planes don't use backreferences. */
                     if(is_chroma || is_iframe || !get_bits1(&ctx->gb)) {
 
                         if(!vlc_decode_block(ctx, num_coeffs, qscale))
                             return 0;
                         ctx->dsp.idct_put(dst, stride, ctx->dct_block);
                     } else {
                         unsigned int backref = get_bits(&ctx->gb, 4);
                         int index = (ctx->cur_index+backref)&15;
                         uint8_t *p = ctx->flipped_ptrs[index].data[0];
 
+                        ff_await_frame_progress(&ctx->buf_ptrs[index], cur_row);
                         if(p) {
                             p += src -
                                 ctx->flipped_ptrs[ctx->prev_index].data[plane];
                             ctx->dsp.put_pixels_tab[1][0](dst, p, stride, 8);
                         } else {
                             av_log(ctx->avctx, AV_LOG_ERROR,
                                      "No such backreference! Buggy sample.\n");
                         }
                     }
                 } else {
+                    ff_await_frame_progress(&ctx->buf_ptrs[ctx->prev_index], cur_row);
                     ctx->dsp.put_pixels_tab[1][0](dst, src, stride, 8);
                 }
                 src += 8;
                 dst += 8;
             }
             src += (stride - ctx->num_hblocks[plane])<<3;
             dst += (stride - ctx->num_hblocks[plane])<<3;
+
+            ff_report_frame_progress(&ctx->buf_ptrs[ctx->cur_index], cur_row++);
         }
     }
 
     return 1;
 }
 
 /**
  * Flip the buffer upside-down and put it in the YVU order to match the
  * way Mimic encodes frames.
  */
 static void prepare_avpic(MimicContext *ctx, AVPicture *dst, AVPicture *src)
@@ -316,75 +340,83 @@
     } else if(width != ctx->avctx->width || height != ctx->avctx->height) {
         av_log(avctx, AV_LOG_ERROR, "resolution changing is not supported\n");
         return -1;
     }
 
     if(is_pframe && !ctx->buf_ptrs[ctx->prev_index].data[0]) {
         av_log(avctx, AV_LOG_ERROR, "decoding must start with keyframe\n");
         return -1;
     }
 
     ctx->buf_ptrs[ctx->cur_index].reference = 1;
-    if(avctx->get_buffer(avctx, &ctx->buf_ptrs[ctx->cur_index])) {
+    if(ff_get_buffer(avctx, &ctx->buf_ptrs[ctx->cur_index])) {
         av_log(avctx, AV_LOG_ERROR, "get_buffer() failed\n");
         return -1;
     }
 
+    ctx->next_prev_index = ctx->cur_index;
+    ctx->next_cur_index  = (ctx->cur_index - 1) & 15;
+
     prepare_avpic(ctx, &ctx->flipped_ptrs[ctx->cur_index],
                   (AVPicture*) &ctx->buf_ptrs[ctx->cur_index]);
 
+    ff_report_frame_setup_done(avctx);
+
     ctx->swap_buf = av_fast_realloc(ctx->swap_buf, &ctx->swap_buf_size,
                                  swap_buf_size + FF_INPUT_BUFFER_PADDING_SIZE);
     if(!ctx->swap_buf)
         return AVERROR_NOMEM;
 
     ctx->dsp.bswap_buf((uint32_t*)ctx->swap_buf,
                         (const uint32_t*) buf,
                         swap_buf_size>>2);
     init_get_bits(&ctx->gb, ctx->swap_buf, swap_buf_size << 3);
 
     if(!decode(ctx, quality, num_coeffs, !is_pframe)) {
-        avctx->release_buffer(avctx, &ctx->buf_ptrs[ctx->cur_index]);
+        ff_release_buffer(avctx, &ctx->buf_ptrs[ctx->cur_index]);
         return -1;
     }
 
     ctx->buf_ptrs[ctx->cur_index].pict_type = is_pframe ? FF_P_TYPE:FF_I_TYPE;
     *(AVFrame*)data = ctx->buf_ptrs[ctx->cur_index];
     *data_size = sizeof(AVFrame);
 
-    ctx->prev_index = ctx->cur_index;
-    ctx->cur_index--;
-    ctx->cur_index &= 15;
+    ctx->prev_index = ctx->next_prev_index;
+    ctx->cur_index  = ctx->next_cur_index;
 
     /* Only release frames that aren't used for backreferences anymore */
     if(ctx->buf_ptrs[ctx->cur_index].data[0])
-        avctx->release_buffer(avctx, &ctx->buf_ptrs[ctx->cur_index]);
+        ff_release_buffer(avctx, &ctx->buf_ptrs[ctx->cur_index]);
 
     return buf_size;
 }
 
 static av_cold int mimic_decode_end(AVCodecContext *avctx)
 {
     MimicContext *ctx = avctx->priv_data;
     int i;
 
     av_free(ctx->swap_buf);
+
+    if(avctx->is_copy) return 0;
+
     for(i = 0; i < 16; i++)
         if(ctx->buf_ptrs[i].data[0])
-            avctx->release_buffer(avctx, &ctx->buf_ptrs[i]);
+            ff_release_buffer(avctx, &ctx->buf_ptrs[i]);
     free_vlc(&ctx->vlc);
 
     return 0;
 }
 
 AVCodec mimic_decoder = {
     "mimic",
     CODEC_TYPE_VIDEO,
     CODEC_ID_MIMIC,
     sizeof(MimicContext),
     mimic_decode_init,
     NULL,
     mimic_decode_end,
     mimic_decode_frame,
-    CODEC_CAP_DR1,
+    CODEC_CAP_DR1 | CODEC_CAP_FRAME_THREADS,
     .long_name = NULL_IF_CONFIG_SMALL("Mimic"),
+    .update_context = ONLY_IF_THREADS_ENABLED(mimic_decode_update_context)
 };
--- mpeg12.c	Sun Jan 25 13:18:59 2009
+++ mpeg12.c	Sun Jan 25 13:07:22 2009
@@ -27,22 +27,23 @@
 
 //#define DEBUG
 #include "avcodec.h"
 #include "dsputil.h"
 #include "mpegvideo.h"
 
 #include "mpeg12.h"
 #include "mpeg12data.h"
 #include "mpeg12decdata.h"
 #include "bytestream.h"
 #include "vdpau_internal.h"
+#include "thread.h"
 
 //#undef NDEBUG
 //#include <assert.h>
 
 
 #define MV_VLC_BITS 9
 #define MBINCR_VLC_BITS 9
 #define MB_PAT_VLC_BITS 9
 #define MB_PTYPE_VLC_BITS 6
 #define MB_BTYPE_VLC_BITS 6
 
@@ -1193,22 +1194,43 @@
     s->mpeg_enc_ctx.flags2= avctx->flags2;
     ff_mpeg12_common_init(&s->mpeg_enc_ctx);
     ff_mpeg12_init_vlcs();
 
     s->mpeg_enc_ctx_allocated = 0;
     s->mpeg_enc_ctx.picture_number = 0;
     s->repeat_field = 0;
     s->mpeg_enc_ctx.codec_id= avctx->codec->id;
     return 0;
 }
 
+static int mpeg_decode_update_context(AVCodecContext *avctx, AVCodecContext *avctx_from)
+{
+    Mpeg1Context *ctx = avctx->priv_data, *ctx_from = avctx_from->priv_data;
+    MpegEncContext *s = &ctx->mpeg_enc_ctx, *s1 = &ctx_from->mpeg_enc_ctx;
+    int err;
+
+    if(!ctx_from->mpeg_enc_ctx_allocated || !s1->context_initialized)
+        return 0;
+
+    err = ff_mpeg_update_context(avctx, avctx_from);
+    if(err) return err;
+
+    if(!ctx->mpeg_enc_ctx_allocated)
+        memcpy(s + 1, s1 + 1, sizeof(Mpeg1Context) - sizeof(MpegEncContext));
+
+    if(!(s->pict_type == FF_B_TYPE || s->low_delay))
+        s->picture_number++;
+
+    return 0;
+}
+
 static void quant_matrix_rebuild(uint16_t *matrix, const uint8_t *old_perm,
                                      const uint8_t *new_perm){
     uint16_t temp_matrix[64];
     int i;
 
     memcpy(temp_matrix,matrix,64*sizeof(uint16_t));
 
     for(i=0;i<64;i++){
         matrix[new_perm[i]] = temp_matrix[old_perm[i]];
     }
 }
@@ -1628,22 +1650,24 @@
             if (s->progressive_sequence) {
                 if (s->top_field_first)
                     s->current_picture_ptr->repeat_pict = 4;
                 else
                     s->current_picture_ptr->repeat_pict = 2;
             } else if (s->progressive_frame) {
                 s->current_picture_ptr->repeat_pict = 1;
             }
         }
 
         *s->current_picture_ptr->pan_scan= s1->pan_scan;
+
+        if (USE_FRAME_THREADING(avctx)) ff_report_frame_setup_done(avctx);
     }else{ //second field
             int i;
 
             if(!s->current_picture_ptr){
                 av_log(s->avctx, AV_LOG_ERROR, "first field missing\n");
                 return -1;
             }
 
             for(i=0; i<4; i++){
                 s->current_picture.data[i] = s->current_picture_ptr->data[i];
                 if(s->picture_structure == PICT_BOTTOM_FIELD){
@@ -1784,22 +1808,23 @@
 
         s->dest[0] += 16 >> lowres;
         s->dest[1] +=(16 >> lowres) >> s->chroma_x_shift;
         s->dest[2] +=(16 >> lowres) >> s->chroma_x_shift;
 
         MPV_decode_mb(s, s->block);
 
         if (++s->mb_x >= s->mb_width) {
             const int mb_size= 16>>s->avctx->lowres;
 
             ff_draw_horiz_band(s, mb_size*s->mb_y, mb_size);
+            MPV_report_decode_progress(s);
 
             s->mb_x = 0;
             s->mb_y++;
 
             if(s->mb_y<<field_pic >= s->mb_height){
                 int left= s->gb.size_in_bits - get_bits_count(&s->gb);
                 int is_d10= s->chroma_format==2 && s->pict_type==FF_I_TYPE && avctx->profile==0 && avctx->level==5
                             && s->intra_dc_precision == 2 && s->q_scale_type == 1 && s->alternate_scan == 0
                             && s->progressive_frame == 0 /* vbv_delay == 0xBBB || 0xE10*/;
 
                 if(left < 0 || (left && show_bits(&s->gb, FFMIN(left, 23)) && !is_d10)
@@ -1934,23 +1959,23 @@
 
         s->current_picture_ptr->qscale_type= FF_QSCALE_TYPE_MPEG2;
 
         ff_er_frame_end(s);
 
         MPV_frame_end(s);
 
         if (s->pict_type == FF_B_TYPE || s->low_delay) {
             *pict= *(AVFrame*)s->current_picture_ptr;
             ff_print_debug_info(s, pict);
         } else {
-            s->picture_number++;
+            if (!USE_FRAME_THREADING(avctx)) s->picture_number++;
             /* latency of 1 frame for I- and P-frames */
             /* XXX: use another variable than picture_number */
             if (s->last_picture_ptr != NULL) {
                 *pict= *(AVFrame*)s->last_picture_ptr;
                  ff_print_debug_info(s, pict);
             }
         }
 
         return 1;
     } else {
         return 0;
@@ -2297,23 +2322,23 @@
     MpegEncContext *s2 = &s->mpeg_enc_ctx;
     const uint8_t *buf_ptr = buf;
     const uint8_t *buf_end = buf + buf_size;
     int ret, input_size;
 
     for(;;) {
         /* find next start code */
         uint32_t start_code = -1;
         buf_ptr = ff_find_start_code(buf_ptr,buf_end, &start_code);
         if (start_code > 0x1ff){
             if(s2->pict_type != FF_B_TYPE || avctx->skip_frame <= AVDISCARD_DEFAULT){
-                if(avctx->thread_count > 1){
+                if(USE_AVCODEC_EXECUTE(avctx)){
                     int i;
 
                     avctx->execute(avctx, slice_decode_thread,  (void**)&(s2->thread_context[0]), NULL, s->slice_count, sizeof(void*));
                     for(i=0; i<s->slice_count; i++)
                         s2->error_count += s2->thread_context[i]->error_count;
                 }
 
                 if (CONFIG_MPEG_VDPAU_DECODER && avctx->codec->capabilities&CODEC_CAP_HWACCEL_VDPAU)
                     ff_vdpau_mpeg_picture_complete(s2, buf, buf_size, s->slice_count);
 
                 if (slice_end(avctx, picture)) {
@@ -2398,23 +2423,23 @@
                     }
                 if(!s2->current_picture_ptr){
                     av_log(avctx, AV_LOG_ERROR, "current_picture not initialized\n");
                     return -1;
                 }
 
                 if (avctx->codec->capabilities&CODEC_CAP_HWACCEL_VDPAU) {
                     s->slice_count++;
                     break;
                 }
 
-                if(avctx->thread_count > 1){
+                if(USE_AVCODEC_EXECUTE(avctx)){
                     int threshold= (s2->mb_height*s->slice_count + avctx->thread_count/2) / avctx->thread_count;
                     if(threshold <= mb_y){
                         MpegEncContext *thread_context= s2->thread_context[s->slice_count];
 
                         thread_context->start_mb_y= mb_y;
                         thread_context->end_mb_y  = s2->mb_height;
                         if(s->slice_count){
                             s2->thread_context[s->slice_count-1]->end_mb_y= mb_y;
                             ff_update_duplicate_context(thread_context, s2);
                         }
                         init_get_bits(&thread_context->gb, buf_ptr, input_size*8);
@@ -2448,25 +2473,26 @@
 }
 
 AVCodec mpeg1video_decoder = {
     "mpeg1video",
     CODEC_TYPE_VIDEO,
     CODEC_ID_MPEG1VIDEO,
     sizeof(Mpeg1Context),
     mpeg_decode_init,
     NULL,
     mpeg_decode_end,
     mpeg_decode_frame,
-    CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 | CODEC_CAP_TRUNCATED | CODEC_CAP_DELAY,
+    CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 | CODEC_CAP_TRUNCATED | CODEC_CAP_DELAY | CODEC_CAP_FRAME_THREADS,
     .flush= ff_mpeg_flush,
     .long_name= NULL_IF_CONFIG_SMALL("MPEG-1 video"),
+    .update_context= ONLY_IF_THREADS_ENABLED(mpeg_decode_update_context)
 };
 
 AVCodec mpeg2video_decoder = {
     "mpeg2video",
     CODEC_TYPE_VIDEO,
     CODEC_ID_MPEG2VIDEO,
     sizeof(Mpeg1Context),
     mpeg_decode_init,
     NULL,
     mpeg_decode_end,
     mpeg_decode_frame,
@@ -2486,23 +2512,23 @@
     mpeg_decode_end,
     mpeg_decode_frame,
     CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 | CODEC_CAP_TRUNCATED | CODEC_CAP_DELAY,
     .flush= ff_mpeg_flush,
     .long_name= NULL_IF_CONFIG_SMALL("MPEG-1 video"),
 };
 
 #if CONFIG_XVMC
 static av_cold int mpeg_mc_decode_init(AVCodecContext *avctx){
     Mpeg1Context *s;
 
-    if( avctx->thread_count > 1)
+    if( USE_AVCODEC_EXECUTE(avctx) )
         return -1;
     if( !(avctx->slice_flags & SLICE_FLAG_CODED_ORDER) )
         return -1;
     if( !(avctx->slice_flags & SLICE_FLAG_ALLOW_FIELD) ){
         dprintf(avctx, "mpeg12.c: XvMC decoder will work better if SLICE_FLAG_ALLOW_FIELD is set\n");
     }
     mpeg_decode_init(avctx);
     s = avctx->priv_data;
 
     avctx->pix_fmt = PIX_FMT_XVMC_MPEG2_IDCT;
     avctx->xvmc_acceleration = 2;//2 - the blocks are packed!
--- mpegaudio.h	Sun Jan 25 13:18:59 2009
+++ mpegaudio.h	Sun Jan 25 13:07:22 2009
@@ -82,46 +82,39 @@
 #if FRAC_BITS <= 15
 typedef int16_t MPA_INT;
 #else
 typedef int32_t MPA_INT;
 #endif
 
 #define BACKSTEP_SIZE 512
 #define EXTRABYTES 24
 
 struct GranuleDef;
 
-#define MPA_DECODE_HEADER \
-    int frame_size; \
-    int error_protection; \
-    int layer; \
-    int sample_rate; \
-    int sample_rate_index; /* between 0 and 8 */ \
-    int bit_rate; \
-    int nb_channels; \
-    int mode; \
-    int mode_ext; \
-    int lsf;
-
-typedef struct MPADecodeHeader {
-  MPA_DECODE_HEADER
-} MPADecodeHeader;
-
 typedef struct MPADecodeContext {
-    MPA_DECODE_HEADER
     DECLARE_ALIGNED_8(uint8_t, last_buf[2*BACKSTEP_SIZE + EXTRABYTES]);
     int last_buf_size;
+    int frame_size;
     /* next header (used in free format parsing) */
     uint32_t free_format_next_header;
+    int error_protection;
+    int layer;
+    int sample_rate;
+    int sample_rate_index; /* between 0 and 8 */
+    int bit_rate;
     GetBitContext gb;
     GetBitContext in_gb;
+    int nb_channels;
+    int mode;
+    int mode_ext;
+    int lsf;
     DECLARE_ALIGNED_16(MPA_INT, synth_buf[MPA_MAX_CHANNELS][512 * 2]);
     int synth_buf_offset[MPA_MAX_CHANNELS];
     DECLARE_ALIGNED_16(int32_t, sb_samples[MPA_MAX_CHANNELS][36][SBLIMIT]);
     int32_t mdct_buf[MPA_MAX_CHANNELS][SBLIMIT * 18]; /* previous samples, for layer 3 MDCT */
 #ifdef DEBUG
     int frame_count;
 #endif
     void (*compute_antialias)(struct MPADecodeContext *s, struct GranuleDef *g);
     int adu_mode; ///< 0 for standard mp3, 1 for adu formatted mp3
     int dither_state;
     int error_recognition;
--- mpegaudio_parser.c	Sun Jan 25 13:18:59 2009
+++ mpegaudio_parser.c	Sun Jan 25 13:07:22 2009
@@ -38,23 +38,24 @@
 #define MPA_HEADER_SIZE 4
 
 /* header + layer + bitrate + freq + lsf/mpeg25 */
 #undef SAME_HEADER_MASK /* mpegaudio.h defines different version */
 #define SAME_HEADER_MASK \
    (0xffe00000 | (3 << 17) | (3 << 10) | (3 << 19))
 
 /* useful helper to get mpeg audio stream infos. Return -1 if error in
    header, otherwise the coded frame size in bytes */
 int ff_mpa_decode_header(AVCodecContext *avctx, uint32_t head, int *sample_rate, int *channels, int *frame_size, int *bit_rate)
 {
-    MPADecodeHeader s1, *s = &s1;
+    MPADecodeContext s1, *s = &s1;
+    s1.avctx = avctx;
 
     if (ff_mpa_check_header(head) != 0)
         return -1;
 
     if (ff_mpegaudio_decode_header(s, head) != 0) {
         return -1;
     }
 
     switch(s->layer) {
     case 1:
         avctx->codec_id = CODEC_ID_MP1;
@@ -136,23 +137,23 @@
                        to get a new bitrate */
                     s->free_format_frame_size = 0;
                 } else {
                     if((header&SAME_HEADER_MASK) != (s->header&SAME_HEADER_MASK) && s->header)
                         s->header_count= -3;
                     s->header= header;
                     s->header_count++;
                     s->frame_size = ret;
 
 #if 0
                     /* free format: prepare to compute frame size */
-                    if (ff_mpegaudio_decode_header((MPADecodeHeader *)s, header) == 1) {
+                    if (ff_mpegaudio_decode_header(s, header) == 1) {
                         s->frame_size = -1;
                     }
 #endif
                     if(s->header_count > 1){
                         avctx->sample_rate= sr;
                         avctx->channels   = channels;
                         avctx->frame_size = frame_size;
                         avctx->bit_rate   = bit_rate;
                     }
                 }
             }
@@ -191,23 +192,23 @@
                         s->inbuf_ptr = p;
                         /* compute frame size */
                         s->free_format_next_header = header;
                         s->free_format_frame_size = s->inbuf_ptr - s->inbuf;
                         padding = (header1 >> 9) & 1;
                         if (s->layer == 1)
                             s->free_format_frame_size -= padding * 4;
                         else
                             s->free_format_frame_size -= padding;
                         dprintf(avctx, "free frame size=%d padding=%d\n",
                                 s->free_format_frame_size, padding);
-                        ff_mpegaudio_decode_header((MPADecodeHeader *)s, header1);
+                        ff_mpegaudio_decode_header(s, header1);
                         goto next_data;
                     }
                     p++;
                 }
                 /* not found: simply increase pointers */
                 buf_ptr += len;
                 s->inbuf_ptr += len;
                 buf_size -= len;
             }
         } else
 #endif
--- mpegaudiodata.c	Sun Jan 25 13:18:59 2009
+++ mpegaudiodata.c	Sun Jan 25 13:07:22 2009
@@ -91,22 +91,52 @@
 
 /* we use a negative value if grouped */
 const int ff_mpa_quant_bits[17] = {
     -5,  -7,  3, -10, 4,
      5,  6,  7,  8,  9,
     10, 11, 12, 13, 14,
     15, 16
 };
 
 /* encoding tables which give the quantization index. Note how it is
    possible to store them efficiently ! */
+static const unsigned char alloc_table_0[] = {
+ 4,  0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
+ 4,  0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
+ 4,  0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 3,  0,  1,  2,  3,  4,  5, 16,
+ 2,  0,  1, 16,
+ 2,  0,  1, 16,
+ 2,  0,  1, 16,
+ 2,  0,  1, 16,
+};
+
 static const unsigned char alloc_table_1[] = {
  4,  0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
  4,  0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
  4,  0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
  4,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 16,
@@ -124,22 +154,33 @@
  3,  0,  1,  2,  3,  4,  5, 16,
  3,  0,  1,  2,  3,  4,  5, 16,
  2,  0,  1, 16,
  2,  0,  1, 16,
  2,  0,  1, 16,
  2,  0,  1, 16,
  2,  0,  1, 16,
  2,  0,  1, 16,
  2,  0,  1, 16,
 };
 
+static const unsigned char alloc_table_2[] = {
+ 4,  0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
+ 4,  0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
+ 3,  0,  1,  3,  4,  5,  6,  7,
+ 3,  0,  1,  3,  4,  5,  6,  7,
+ 3,  0,  1,  3,  4,  5,  6,  7,
+ 3,  0,  1,  3,  4,  5,  6,  7,
+ 3,  0,  1,  3,  4,  5,  6,  7,
+ 3,  0,  1,  3,  4,  5,  6,  7,
+};
+
 static const unsigned char alloc_table_3[] = {
  4,  0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
  4,  0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
  3,  0,  1,  3,  4,  5,  6,  7,
@@ -173,12 +214,12 @@
  2,  0,  1,  3,
  2,  0,  1,  3,
  2,  0,  1,  3,
  2,  0,  1,  3,
  2,  0,  1,  3,
  2,  0,  1,  3,
  2,  0,  1,  3,
  2,  0,  1,  3,
 };
 
 const unsigned char * const ff_mpa_alloc_tables[5] =
-{ alloc_table_1, alloc_table_1, alloc_table_3, alloc_table_3, alloc_table_4, };
+{ alloc_table_0, alloc_table_1, alloc_table_2, alloc_table_3, alloc_table_4, };
--- mpegaudiodec.c	Sun Jan 25 13:19:00 2009
+++ mpegaudiodec.c	Sun Jan 25 13:07:22 2009
@@ -2269,23 +2269,23 @@
     if(buf_size < HEADER_SIZE)
         return -1;
 
     header = AV_RB32(buf);
     if(ff_mpa_check_header(header) < 0){
         buf++;
 //        buf_size--;
         av_log(avctx, AV_LOG_ERROR, "Header missing skipping one byte.\n");
         goto retry;
     }
 
-    if (ff_mpegaudio_decode_header((MPADecodeHeader *)s, header) == 1) {
+    if (ff_mpegaudio_decode_header(s, header) == 1) {
         /* free format: prepare to compute frame size */
         s->frame_size = -1;
         return -1;
     }
     /* update codec info */
     avctx->channels = s->nb_channels;
     avctx->bit_rate = s->bit_rate;
     avctx->sub_id = s->layer;
 
     if(s->frame_size<=0 || s->frame_size > buf_size){
         av_log(avctx, AV_LOG_ERROR, "incomplete frame\n");
@@ -2334,23 +2334,23 @@
     if (len > MPA_MAX_CODED_FRAME_SIZE)
         len = MPA_MAX_CODED_FRAME_SIZE;
 
     // Get header and restore sync word
     header = AV_RB32(buf) | 0xffe00000;
 
     if (ff_mpa_check_header(header) < 0) { // Bad header, discard frame
         *data_size = 0;
         return buf_size;
     }
 
-    ff_mpegaudio_decode_header((MPADecodeHeader *)s, header);
+    ff_mpegaudio_decode_header(s, header);
     /* update codec info */
     avctx->sample_rate = s->sample_rate;
     avctx->channels = s->nb_channels;
     avctx->bit_rate = s->bit_rate;
     avctx->sub_id = s->layer;
 
     s->frame_size = len;
 
     if (avctx->parse_only) {
         out_size = buf_size;
     } else {
@@ -2483,23 +2483,23 @@
     for (fr = 0; fr < s->frames; fr++) {
         fsize = AV_RB16(buf) >> 4;
         fsize = FFMIN3(fsize, len, MPA_MAX_CODED_FRAME_SIZE);
         m = s->mp3decctx[fr];
         assert (m != NULL);
 
         header = (AV_RB32(buf) & 0x000fffff) | s->syncword; // patch header
 
         if (ff_mpa_check_header(header) < 0) // Bad header, discard block
             break;
 
-        ff_mpegaudio_decode_header((MPADecodeHeader *)m, header);
+        ff_mpegaudio_decode_header(m, header);
         out_size += mp_decode_frame(m, outptr, buf, fsize);
         buf += fsize;
         len -= fsize;
 
         if(s->frames > 1) {
             n = m->avctx->frame_size*m->nb_channels;
             /* interleave output data */
             bp = out_samples + s->coff[fr];
             if(m->nb_channels == 1) {
                 for(j = 0; j < n; j++) {
                     *bp = decoded_buf[j];
--- mpegaudiodecheader.c	Sun Jan 25 13:19:00 2009
+++ mpegaudiodecheader.c	Sun Jan 25 13:07:22 2009
@@ -22,23 +22,23 @@
 /**
  * @file mpegaudiodecheader.c
  * MPEG Audio header decoder.
  */
 
 //#define DEBUG
 #include "avcodec.h"
 #include "mpegaudio.h"
 #include "mpegaudiodata.h"
 
 
-int ff_mpegaudio_decode_header(MPADecodeHeader *s, uint32_t header)
+int ff_mpegaudio_decode_header(MPADecodeContext *s, uint32_t header)
 {
     int sample_rate, frame_size, mpeg25, padding;
     int sample_rate_index, bitrate_index;
     if (header & (1<<20)) {
         s->lsf = (header & (1<<19)) ? 0 : 1;
         mpeg25 = 0;
     } else {
         s->lsf = 1;
         mpeg25 = 1;
     }
 
--- mpegaudiodecheader.h	Sun Jan 25 13:19:00 2009
+++ mpegaudiodecheader.h	Sun Jan 25 13:07:22 2009
@@ -26,14 +26,14 @@
 
 #ifndef AVCODEC_MPEGAUDIODECHEADER_H
 #define AVCODEC_MPEGAUDIODECHEADER_H
 
 #include "libavutil/common.h"
 #include "mpegaudio.h"
 
 
 /* header decoding. MUST check the header before because no
    consistency check is done there. Return 1 if free format found and
    that the frame size must be computed externally */
-int ff_mpegaudio_decode_header(MPADecodeHeader *s, uint32_t header);
+int ff_mpegaudio_decode_header(MPADecodeContext *s, uint32_t header);
 
 #endif /* AVCODEC_MPEGAUDIODECHEADER_H */
--- mpegvideo.c	Sun Jan 25 13:19:00 2009
+++ mpegvideo.c	Sun Jan 25 13:07:22 2009
@@ -26,22 +26,23 @@
  * @file mpegvideo.c
  * The simplest mpeg encoder (well, it was the simplest!).
  */
 
 #include "avcodec.h"
 #include "dsputil.h"
 #include "mpegvideo.h"
 #include "mpegvideo_common.h"
 #include "mjpegenc.h"
 #include "msmpeg4.h"
 #include "faandct.h"
+#include "thread.h"
 #include <limits.h>
 
 //#undef NDEBUG
 //#include <assert.h>
 
 static void dct_unquantize_mpeg1_intra_c(MpegEncContext *s,
                                    DCTELEM *block, int n, int qscale);
 static void dct_unquantize_mpeg1_inter_c(MpegEncContext *s,
                                    DCTELEM *block, int n, int qscale);
 static void dct_unquantize_mpeg2_intra_c(MpegEncContext *s,
                                    DCTELEM *block, int n, int qscale);
@@ -169,38 +170,38 @@
     const int b4_array_size= s->b4_stride*s->mb_height*4;
     int i;
     int r= -1;
 
     if(shared){
         assert(pic->data[0]);
         assert(pic->type == 0 || pic->type == FF_BUFFER_TYPE_SHARED);
         pic->type= FF_BUFFER_TYPE_SHARED;
     }else{
         assert(!pic->data[0]);
 
-        r= s->avctx->get_buffer(s->avctx, (AVFrame*)pic);
+        r= ff_get_buffer(s->avctx, (AVFrame*)pic);
 
         if(r<0 || !pic->age || !pic->type || !pic->data[0]){
             av_log(s->avctx, AV_LOG_ERROR, "get_buffer() failed (%d %d %d %p)\n", r, pic->age, pic->type, pic->data[0]);
             return -1;
         }
 
         if(s->linesize && (s->linesize != pic->linesize[0] || s->uvlinesize != pic->linesize[1])){
             av_log(s->avctx, AV_LOG_ERROR, "get_buffer() failed (stride changed)\n");
-            s->avctx->release_buffer(s->avctx, (AVFrame*)pic);
+            ff_release_buffer(s->avctx, (AVFrame*)pic);
             return -1;
         }
 
         if(pic->linesize[1] != pic->linesize[2]){
             av_log(s->avctx, AV_LOG_ERROR, "get_buffer() failed (uv stride mismatch)\n");
-            s->avctx->release_buffer(s->avctx, (AVFrame*)pic);
+            ff_release_buffer(s->avctx, (AVFrame*)pic);
             return -1;
         }
 
         s->linesize  = pic->linesize[0];
         s->uvlinesize= pic->linesize[1];
     }
 
     if(pic->qscale_table==NULL){
         if (s->encoding) {
             CHECKED_ALLOCZ(pic->mb_var   , mb_array_size * sizeof(int16_t))
             CHECKED_ALLOCZ(pic->mc_mb_var, mb_array_size * sizeof(int16_t))
@@ -235,34 +236,34 @@
 
     /* It might be nicer if the application would keep track of these
      * but it would require an API change. */
     memmove(s->prev_pict_types+1, s->prev_pict_types, PREV_PICT_TYPES_BUFFER_SIZE-1);
     s->prev_pict_types[0]= s->dropable ? FF_B_TYPE : s->pict_type;
     if(pic->age < PREV_PICT_TYPES_BUFFER_SIZE && s->prev_pict_types[pic->age] == FF_B_TYPE)
         pic->age= INT_MAX; // Skipped MBs in B-frames are quite rare in MPEG-1/2 and it is a bit tricky to skip them anyway.
 
     return 0;
 fail: //for the CHECKED_ALLOCZ macro
     if(r>=0)
-        s->avctx->release_buffer(s->avctx, (AVFrame*)pic);
+        ff_release_buffer(s->avctx, (AVFrame*)pic);
     return -1;
 }
 
 /**
  * deallocates a picture
  */
 static void free_picture(MpegEncContext *s, Picture *pic){
     int i;
 
     if(pic->data[0] && pic->type!=FF_BUFFER_TYPE_SHARED){
-        s->avctx->release_buffer(s->avctx, (AVFrame*)pic);
+        ff_release_buffer(s->avctx, (AVFrame*)pic);
     }
 
     av_freep(&pic->mb_var);
     av_freep(&pic->mc_mb_var);
     av_freep(&pic->mb_mean);
     av_freep(&pic->mbskip_table);
     av_freep(&pic->qscale_table);
     av_freep(&pic->mb_type_base);
     av_freep(&pic->dct_coeff);
     av_freep(&pic->pan_scan);
     pic->mb_type= NULL;
@@ -357,63 +358,141 @@
     //FIXME copy only needed parts
 //START_TIMER
     backup_duplicate_context(&bak, dst);
     memcpy(dst, src, sizeof(MpegEncContext));
     backup_duplicate_context(dst, &bak);
     for(i=0;i<12;i++){
         dst->pblocks[i] = (short *)(&dst->block[i]);
     }
 //STOP_TIMER("update_duplicate_context") //about 10k cycles / 0.01 sec for 1000frames on 1ghz with 2 threads
 }
 
+int ff_mpeg_update_context(AVCodecContext *dst, AVCodecContext *src)
+{
+    MpegEncContext *s = dst->priv_data, *s1 = src->priv_data;
+
+    if(!s1->context_initialized) return 0;
+
+    //FIXME can parameters change on I-frames? in that case dst may need a reinit
+    if(!s->context_initialized){
+        memcpy(s, s1, sizeof(MpegEncContext));
+
+        s->avctx                 = dst;
+        s->picture_range_start  += MAX_PICTURE_COUNT;
+        s->picture_range_end    += MAX_PICTURE_COUNT;
+        s->bitstream_buffer      = NULL;
+        s->bitstream_buffer_size = s->allocated_bitstream_buffer_size = 0;
+
+        MPV_common_init(s);
+    }
+
+    s->avctx->coded_height  = s1->avctx->coded_height;
+    s->avctx->coded_width   = s1->avctx->coded_width;
+    s->avctx->width         = s1->avctx->width;
+    s->avctx->height        = s1->avctx->height;
+
+    s->coded_picture_number = s1->coded_picture_number;
+    s->picture_number       = s1->picture_number;
+    s->input_picture_number = s1->input_picture_number;
+
+    memcpy(s->picture, s1->picture, s1->picture_count * sizeof(Picture));
+    memcpy(&s->last_picture, &s1->last_picture, (char*)&s1->last_picture_ptr - (char*)&s1->last_picture);
+
+    s->last_picture_ptr     = REBASE_PICTURE(s1->last_picture_ptr,    s, s1);
+    s->current_picture_ptr  = REBASE_PICTURE(s1->current_picture_ptr, s, s1);
+    s->next_picture_ptr     = REBASE_PICTURE(s1->next_picture_ptr,    s, s1);
+
+    memcpy(s->prev_pict_types, s1->prev_pict_types, PREV_PICT_TYPES_BUFFER_SIZE);
+
+    //Error/bug resilience
+    s->next_p_frame_damaged = s1->next_p_frame_damaged;
+    s->workaround_bugs      = s1->workaround_bugs;
+
+    //MPEG4 timing info
+    memcpy(&s->time_increment_bits, &s1->time_increment_bits, (char*)&s1->shape - (char*)&s1->time_increment_bits);
+
+    //B-frame info
+    s->max_b_frames         = s1->max_b_frames;
+    s->low_delay            = s1->low_delay;
+    s->dropable             = s1->dropable;
+
+    //DivX handling (doesn't work)
+    s->divx_packed          = s1->divx_packed;
+
+    if(s1->bitstream_buffer){
+        s->bitstream_buffer       = av_fast_realloc(s->bitstream_buffer, &s->allocated_bitstream_buffer_size, s1->allocated_bitstream_buffer_size+FF_INPUT_BUFFER_PADDING_SIZE);
+        s->bitstream_buffer_size  = s1->bitstream_buffer_size;
+        memcpy(s->bitstream_buffer, s1->bitstream_buffer, s1->bitstream_buffer_size);
+        memset(s->bitstream_buffer+s->bitstream_buffer_size, 0, FF_INPUT_BUFFER_PADDING_SIZE);
+    }
+
+    //MPEG2/interlacing info
+    memcpy(&s->progressive_sequence, &s1->progressive_sequence, (char*)&s1->rtp_mode - (char*)&s1->progressive_sequence);
+
+    if(!s1->first_field){
+        s->last_pict_type= s1->pict_type;
+        if (s1->current_picture_ptr) s->last_lambda_for[s1->pict_type] = s1->current_picture_ptr->quality;
+
+        if(s1->pict_type!=FF_B_TYPE){
+            s->last_non_b_pict_type= s1->pict_type;
+        }
+    }
+
+    return 0;
+}
+
 /**
  * sets the given MpegEncContext to common defaults (same for encoding and decoding).
  * the changed fields will not depend upon the prior state of the MpegEncContext.
  */
 void MPV_common_defaults(MpegEncContext *s){
     s->y_dc_scale_table=
     s->c_dc_scale_table= ff_mpeg1_dc_scale_table;
     s->chroma_qscale_table= ff_default_chroma_qscale_table;
     s->progressive_frame= 1;
     s->progressive_sequence= 1;
     s->picture_structure= PICT_FRAME;
 
     s->coded_picture_number = 0;
     s->picture_number = 0;
     s->input_picture_number = 0;
 
     s->picture_in_gop_number = 0;
 
     s->f_code = 1;
     s->b_code = 1;
+
+    s->picture_range_start = 0;
+    s->picture_range_end = MAX_PICTURE_COUNT;
 }
 
 /**
  * sets the given MpegEncContext to defaults for decoding.
  * the changed fields will not depend upon the prior state of the MpegEncContext.
  */
 void MPV_decode_defaults(MpegEncContext *s){
     MPV_common_defaults(s);
 }
 
 /**
  * init common structure for both encoder and decoder.
  * this assumes that some variables like width/height are already set
  */
 int MPV_common_init(MpegEncContext *s)
 {
     int y_size, c_size, yc_size, i, mb_array_size, mv_table_size, x, y, threads;
 
     s->mb_height = (s->height + 15) / 16;
 
-    if(s->avctx->thread_count > MAX_THREADS || (s->avctx->thread_count > s->mb_height && s->mb_height)){
+    if(USE_AVCODEC_EXECUTE(s->avctx) &&
+      (s->avctx->thread_count > MAX_THREADS || (s->avctx->thread_count > s->mb_height && s->mb_height))){
         av_log(s->avctx, AV_LOG_ERROR, "too many threads\n");
         return -1;
     }
 
     if((s->width || s->height) && avcodec_check_dimensions(s->avctx, s->width, s->height))
         return -1;
 
     dsputil_init(&s->dsp, s->avctx);
     ff_dct_common_init(s);
 
     s->flags= s->avctx->flags;
@@ -496,23 +575,24 @@
         CHECKED_ALLOCZ(s->q_intra_matrix, 64*32 * sizeof(int))
         CHECKED_ALLOCZ(s->q_inter_matrix, 64*32 * sizeof(int))
         CHECKED_ALLOCZ(s->q_intra_matrix16, 64*32*2 * sizeof(uint16_t))
         CHECKED_ALLOCZ(s->q_inter_matrix16, 64*32*2 * sizeof(uint16_t))
         CHECKED_ALLOCZ(s->input_picture, MAX_PICTURE_COUNT * sizeof(Picture*))
         CHECKED_ALLOCZ(s->reordered_input_picture, MAX_PICTURE_COUNT * sizeof(Picture*))
 
         if(s->avctx->noise_reduction){
             CHECKED_ALLOCZ(s->dct_offset, 2 * 64 * sizeof(uint16_t))
         }
     }
-    CHECKED_ALLOCZ(s->picture, MAX_PICTURE_COUNT * sizeof(Picture))
+    s->picture_count = MAX_PICTURE_COUNT * FFMAX(1, s->avctx->thread_count);
+    CHECKED_ALLOCZ(s->picture, s->picture_count * sizeof(Picture))
 
     CHECKED_ALLOCZ(s->error_status_table, mb_array_size*sizeof(uint8_t))
 
     if(s->codec_id==CODEC_ID_MPEG4 || (s->flags & CODEC_FLAG_INTERLACED_ME)){
         /* interlaced direct mode decoding tables */
             for(i=0; i<2; i++){
                 int j, k;
                 for(j=0; j<2; j++){
                     for(k=0; k<2; k++){
                         CHECKED_ALLOCZ(s->b_field_mv_table_base[i][j][k]     , mv_table_size * 2 * sizeof(int16_t))
                         s->b_field_mv_table[i][j][k]    = s->b_field_mv_table_base[i][j][k]     + s->mb_stride + 1;
@@ -560,55 +640,63 @@
     //Note the +1 is for a quicker mpeg4 slice_end detection
     CHECKED_ALLOCZ(s->prev_pict_types, PREV_PICT_TYPES_BUFFER_SIZE);
 
     s->parse_context.state= -1;
     if((s->avctx->debug&(FF_DEBUG_VIS_QP|FF_DEBUG_VIS_MB_TYPE)) || (s->avctx->debug_mv)){
        s->visualization_buffer[0] = av_malloc((s->mb_width*16 + 2*EDGE_WIDTH) * s->mb_height*16 + 2*EDGE_WIDTH);
        s->visualization_buffer[1] = av_malloc((s->mb_width*16 + 2*EDGE_WIDTH) * s->mb_height*16 + 2*EDGE_WIDTH);
        s->visualization_buffer[2] = av_malloc((s->mb_width*16 + 2*EDGE_WIDTH) * s->mb_height*16 + 2*EDGE_WIDTH);
     }
 
     s->context_initialized = 1;
-
     s->thread_context[0]= s;
-    threads = s->avctx->thread_count;
 
-    for(i=1; i<threads; i++){
-        s->thread_context[i]= av_malloc(sizeof(MpegEncContext));
-        memcpy(s->thread_context[i], s, sizeof(MpegEncContext));
-    }
+    if (USE_AVCODEC_EXECUTE(s->avctx)) {
+        threads = s->avctx->thread_count;
 
-    for(i=0; i<threads; i++){
-        if(init_duplicate_context(s->thread_context[i], s) < 0)
-           goto fail;
-        s->thread_context[i]->start_mb_y= (s->mb_height*(i  ) + s->avctx->thread_count/2) / s->avctx->thread_count;
-        s->thread_context[i]->end_mb_y  = (s->mb_height*(i+1) + s->avctx->thread_count/2) / s->avctx->thread_count;
+        for(i=1; i<threads; i++){
+            s->thread_context[i]= av_malloc(sizeof(MpegEncContext));
+            memcpy(s->thread_context[i], s, sizeof(MpegEncContext));
+        }
+
+        for(i=0; i<threads; i++){
+            if(init_duplicate_context(s->thread_context[i], s) < 0)
+               goto fail;
+            s->thread_context[i]->start_mb_y= (s->mb_height*(i  ) + s->avctx->thread_count/2) / s->avctx->thread_count;
+            s->thread_context[i]->end_mb_y  = (s->mb_height*(i+1) + s->avctx->thread_count/2) / s->avctx->thread_count;
+        }
+    } else {
+        if(init_duplicate_context(s, s) < 0) goto fail;
+        s->start_mb_y = 0;
+        s->end_mb_y   = s->mb_height;
     }
 
     return 0;
  fail:
     MPV_common_end(s);
     return -1;
 }
 
 /* init common structure for both encoder and decoder */
 void MPV_common_end(MpegEncContext *s)
 {
     int i, j, k;
 
-    for(i=0; i<s->avctx->thread_count; i++){
-        free_duplicate_context(s->thread_context[i]);
-    }
-    for(i=1; i<s->avctx->thread_count; i++){
-        av_freep(&s->thread_context[i]);
-    }
+    if (USE_AVCODEC_EXECUTE(s->avctx)) {
+        for(i=0; i<s->avctx->thread_count; i++){
+            free_duplicate_context(s->thread_context[i]);
+        }
+        for(i=1; i<s->avctx->thread_count; i++){
+            av_freep(&s->thread_context[i]);
+        }
+    } else free_duplicate_context(s);
 
     av_freep(&s->parse_context.buffer);
     s->parse_context.buffer_size=0;
 
     av_freep(&s->mb_type);
     av_freep(&s->p_mv_table_base);
     av_freep(&s->b_forw_mv_table_base);
     av_freep(&s->b_back_mv_table_base);
     av_freep(&s->b_bidir_forw_mv_table_base);
     av_freep(&s->b_bidir_back_mv_table_base);
     av_freep(&s->b_direct_mv_table_base);
@@ -648,24 +736,24 @@
     av_freep(&s->error_status_table);
     av_freep(&s->mb_index2xy);
     av_freep(&s->lambda_table);
     av_freep(&s->q_intra_matrix);
     av_freep(&s->q_inter_matrix);
     av_freep(&s->q_intra_matrix16);
     av_freep(&s->q_inter_matrix16);
     av_freep(&s->input_picture);
     av_freep(&s->reordered_input_picture);
     av_freep(&s->dct_offset);
 
-    if(s->picture){
-        for(i=0; i<MAX_PICTURE_COUNT; i++){
+    if(s->picture && !s->avctx->is_copy){
+        for(i=0; i<s->picture_count; i++){
             free_picture(s, &s->picture[i]);
         }
     }
     av_freep(&s->picture);
     s->context_initialized = 0;
     s->last_picture_ptr=
     s->next_picture_ptr=
     s->current_picture_ptr= NULL;
     s->linesize= s->uvlinesize= 0;
 
     for(i=0; i<3; i++)
@@ -761,30 +849,30 @@
             rl->rl_vlc[q][i].len= len;
             rl->rl_vlc[q][i].level= level;
             rl->rl_vlc[q][i].run= run;
         }
     }
 }
 
 int ff_find_unused_picture(MpegEncContext *s, int shared){
     int i;
 
     if(shared){
-        for(i=0; i<MAX_PICTURE_COUNT; i++){
+        for(i=s->picture_range_start; i<s->picture_range_end; i++){
             if(s->picture[i].data[0]==NULL && s->picture[i].type==0) return i;
         }
     }else{
-        for(i=0; i<MAX_PICTURE_COUNT; i++){
+        for(i=s->picture_range_start; i<s->picture_range_end; i++){
             if(s->picture[i].data[0]==NULL && s->picture[i].type!=0) return i; //FIXME
         }
-        for(i=0; i<MAX_PICTURE_COUNT; i++){
+        for(i=s->picture_range_start; i<s->picture_range_end; i++){
             if(s->picture[i].data[0]==NULL) return i;
         }
     }
 
     av_log(s->avctx, AV_LOG_FATAL, "Internal error, picture buffer overflow\n");
     /* We could return -1, but the codec would crash trying to draw into a
      * non-existing frame anyway. This is safer than waiting for a random crash.
      * Also the return of this is never useful, an encoder must only allocate
      * as much as allowed in the specification. This has no relationship to how
      * much libavcodec could allocate (and MAX_PICTURE_COUNT is always large
      * enough for such valid streams).
@@ -820,42 +908,42 @@
 int MPV_frame_start(MpegEncContext *s, AVCodecContext *avctx)
 {
     int i;
     AVFrame *pic;
     s->mb_skipped = 0;
 
     assert(s->last_picture_ptr==NULL || s->out_format != FMT_H264 || s->codec_id == CODEC_ID_SVQ3);
 
     /* mark&release old frames */
     if (s->pict_type != FF_B_TYPE && s->last_picture_ptr && s->last_picture_ptr != s->next_picture_ptr && s->last_picture_ptr->data[0]) {
       if(s->out_format != FMT_H264 || s->codec_id == CODEC_ID_SVQ3){
-        avctx->release_buffer(avctx, (AVFrame*)s->last_picture_ptr);
+        ff_release_buffer(avctx, (AVFrame*)s->last_picture_ptr);
 
         /* release forgotten pictures */
         /* if(mpeg124/h263) */
         if(!s->encoding){
-            for(i=0; i<MAX_PICTURE_COUNT; i++){
+            for(i=0; i<s->picture_count; i++){
                 if(s->picture[i].data[0] && &s->picture[i] != s->next_picture_ptr && s->picture[i].reference){
                     av_log(avctx, AV_LOG_ERROR, "releasing zombie picture\n");
-                    avctx->release_buffer(avctx, (AVFrame*)&s->picture[i]);
+                    ff_release_buffer(avctx, (AVFrame*)&s->picture[i]);
                 }
             }
         }
       }
     }
 alloc:
     if(!s->encoding){
         /* release non reference frames */
-        for(i=0; i<MAX_PICTURE_COUNT; i++){
+        for(i=0; i<s->picture_count; i++){
             if(s->picture[i].data[0] && !s->picture[i].reference /*&& s->picture[i].type!=FF_BUFFER_TYPE_SHARED*/){
-                s->avctx->release_buffer(s->avctx, (AVFrame*)&s->picture[i]);
+                ff_release_buffer(s->avctx, (AVFrame*)&s->picture[i]);
             }
         }
 
         if(s->current_picture_ptr && s->current_picture_ptr->data[0]==NULL)
             pic= (AVFrame*)s->current_picture_ptr; //we already have a unused image (maybe it was set before reading the header)
         else{
             i= ff_find_unused_picture(s, 0);
             pic= (AVFrame*)&s->picture[i];
         }
 
         pic->reference= 0;
@@ -866,22 +954,23 @@
                 pic->reference = 3;
         }
 
         pic->coded_picture_number= s->coded_picture_number++;
 
         if( alloc_picture(s, (Picture*)pic, 0) < 0)
             return -1;
 
         s->current_picture_ptr= (Picture*)pic;
         s->current_picture_ptr->top_field_first= s->top_field_first; //FIXME use only the vars from current_pic
         s->current_picture_ptr->interlaced_frame= !s->progressive_frame && !s->progressive_sequence;
+        s->current_picture_ptr->field_picture= s->picture_structure != PICT_FRAME;
     }
 
     s->current_picture_ptr->pict_type= s->pict_type;
 //    if(s->flags && CODEC_FLAG_QSCALE)
   //      s->current_picture_ptr->quality= s->new_picture_ptr->quality;
     s->current_picture_ptr->key_frame= s->pict_type == FF_I_TYPE;
 
     ff_copy_picture(&s->current_picture, s->current_picture_ptr);
 
     if (s->pict_type != FF_B_TYPE) {
         s->last_picture_ptr= s->next_picture_ptr;
@@ -942,61 +1031,66 @@
 #if CONFIG_XVMC
     if(s->avctx->xvmc_acceleration)
         return XVMC_field_start(s, avctx);
 #endif
     return 0;
 }
 
 /* generic function for encode/decode called after a frame has been coded/decoded */
 void MPV_frame_end(MpegEncContext *s)
 {
     int i;
-    /* draw edge for correct motion prediction if outside */
 #if CONFIG_XVMC
 //just to make sure that all data is rendered.
     if(s->avctx->xvmc_acceleration){
         XVMC_field_end(s);
     }else
 #endif
+    /* redraw edges for the frame if decoding didn't complete */
     if(!(s->avctx->codec->capabilities&CODEC_CAP_HWACCEL_VDPAU)
+       && s->error_count
        && s->unrestricted_mv
        && s->current_picture.reference
        && !s->intra_only
        && !(s->flags&CODEC_FLAG_EMU_EDGE)) {
-            s->dsp.draw_edges(s->current_picture.data[0], s->linesize  , s->h_edge_pos   , s->v_edge_pos   , EDGE_WIDTH  );
-            s->dsp.draw_edges(s->current_picture.data[1], s->uvlinesize, s->h_edge_pos>>1, s->v_edge_pos>>1, EDGE_WIDTH/2);
-            s->dsp.draw_edges(s->current_picture.data[2], s->uvlinesize, s->h_edge_pos>>1, s->v_edge_pos>>1, EDGE_WIDTH/2);
+        int edges = EDGE_BOTTOM | EDGE_TOP, h = s->v_edge_pos;
+
+            s->dsp.draw_edges(s->current_picture_ptr->data[0], s->linesize  , s->h_edge_pos   , h   , EDGE_WIDTH  , edges);
+            s->dsp.draw_edges(s->current_picture_ptr->data[1], s->uvlinesize, s->h_edge_pos>>1, h>>1, EDGE_WIDTH/2, edges);
+            s->dsp.draw_edges(s->current_picture_ptr->data[2], s->uvlinesize, s->h_edge_pos>>1, h>>1, EDGE_WIDTH/2, edges);
+
     }
+
     emms_c();
 
     s->last_pict_type    = s->pict_type;
     s->last_lambda_for[s->pict_type]= s->current_picture_ptr->quality;
     if(s->pict_type!=FF_B_TYPE){
         s->last_non_b_pict_type= s->pict_type;
     }
 #if 0
         /* copy back current_picture variables */
     for(i=0; i<MAX_PICTURE_COUNT; i++){
         if(s->picture[i].data[0] == s->current_picture.data[0]){
             s->picture[i]= s->current_picture;
             break;
         }
     }
     assert(i<MAX_PICTURE_COUNT);
 #endif
 
     if(s->encoding){
         /* release non-reference frames */
-        for(i=0; i<MAX_PICTURE_COUNT; i++){
+        for(i=0; i<s->picture_count; i++){
             if(s->picture[i].data[0] && !s->picture[i].reference /*&& s->picture[i].type!=FF_BUFFER_TYPE_SHARED*/){
-                s->avctx->release_buffer(s->avctx, (AVFrame*)&s->picture[i]);
+                 ff_release_buffer(s->avctx, (AVFrame*)&s->picture[i]);
             }
         }
     }
     // clear copies, to avoid confusion
 #if 0
     memset(&s->last_picture, 0, sizeof(Picture));
     memset(&s->next_picture, 0, sizeof(Picture));
     memset(&s->current_picture, 0, sizeof(Picture));
 #endif
     s->avctx->coded_frame= (AVFrame*)s->current_picture_ptr;
 }
@@ -1652,22 +1746,59 @@
                 //opposite parity is always in the same frame if this is second field
                 if(!s->first_field){
                     ref_picture = s->current_picture_ptr->data;
                 }
             }
         }
     break;
     default: assert(0);
     }
 }
 
+/**
+ * find the lowest MB row referenced in the MVs
+ */
+int MPV_lowest_referenced_row(MpegEncContext *s, int dir)
+{
+    int my_max = INT_MIN, my_min = INT_MAX, qpel_shift = !s->quarter_sample;
+    int my, off, i, mvs;
+
+    if (s->picture_structure != PICT_FRAME) goto unhandled;
+
+    switch (s->mv_type) {
+        case MV_TYPE_16X16:
+            mvs = 1;
+            break;
+        case MV_TYPE_16X8:
+            mvs = 2;
+            break;
+        case MV_TYPE_8X8:
+            mvs = 4;
+            break;
+        default:
+            goto unhandled;
+    }
+
+    for (i = 0; i < mvs; i++) {
+        my = s->mv[dir][i][1]<<qpel_shift;
+        my_max = FFMAX(my_max, my);
+        my_min = FFMIN(my_min, my);
+    }
+
+    off = (FFMAX(-my_min, my_max) + 63) >> 6;
+
+    return FFMIN(FFMAX(s->mb_y + off, 0), s->mb_height-1);
+unhandled:
+    return s->mb_height-1;
+}
+
 /* put block[] to dest[] */
 static inline void put_dct(MpegEncContext *s,
                            DCTELEM *block, int i, uint8_t *dest, int line_size, int qscale)
 {
     s->dct_unquantize_intra(s, block, i, qscale);
     s->dsp.idct_put (dest, line_size, block);
 }
 
 /* add block[] to dest[] */
 static inline void add_dct(MpegEncContext *s,
                            DCTELEM *block, int i, uint8_t *dest, int line_size)
@@ -1817,22 +1948,32 @@
             dest_cr= s->dest[2];
         }else{
             dest_y = s->b_scratchpad;
             dest_cb= s->b_scratchpad+16*linesize;
             dest_cr= s->b_scratchpad+32*linesize;
         }
 
         if (!s->mb_intra) {
             /* motion handling */
             /* decoding or more than one mb_type (MC was already done otherwise) */
             if(!s->encoding){
+
+                if(USE_FRAME_THREADING(s->avctx)) {
+                    if (s->mv_dir & MV_DIR_FORWARD) {
+                        ff_await_frame_progress((AVFrame*)s->last_picture_ptr, MPV_lowest_referenced_row(s, 0));
+                    }
+                    if (s->mv_dir & MV_DIR_BACKWARD) {
+                        ff_await_frame_progress((AVFrame*)s->next_picture_ptr, MPV_lowest_referenced_row(s, 1));
+                    }
+                }
+
                 if(lowres_flag){
                     h264_chroma_mc_func *op_pix = s->dsp.put_h264_chroma_pixels_tab;
 
                     if (s->mv_dir & MV_DIR_FORWARD) {
                         MPV_motion_lowres(s, dest_y, dest_cb, dest_cr, 0, s->last_picture.data, op_pix);
                         op_pix = s->dsp.avg_h264_chroma_pixels_tab;
                     }
                     if (s->mv_dir & MV_DIR_BACKWARD) {
                         MPV_motion_lowres(s, dest_y, dest_cb, dest_cr, 1, s->next_picture.data, op_pix);
                     }
                 }else{
@@ -1981,34 +2122,47 @@
     } else
 #endif
     if(s->avctx->lowres) MPV_decode_mb_internal(s, block, 1, 0);
     else                  MPV_decode_mb_internal(s, block, 0, 0);
 }
 
 /**
  *
  * @param h is the normal height, this will be reduced automatically if needed for the last row
  */
 void ff_draw_horiz_band(MpegEncContext *s, int y, int h){
+    if(s->picture_structure != PICT_FRAME){
+        h <<= 1;
+        y <<= 1;
+    }
+
+    if (s->unrestricted_mv && !s->intra_only && !(s->flags&CODEC_FLAG_EMU_EDGE)) {
+        int sides = 0, edge_h;
+        if (y==0) sides |= EDGE_TOP;
+        if (y + h >= s->v_edge_pos) sides |= EDGE_BOTTOM;
+
+        edge_h= FFMIN(h, s->v_edge_pos - y);
+
+        s->dsp.draw_edges(s->current_picture_ptr->data[0] +  y    *s->linesize  , s->linesize  , s->h_edge_pos   , edge_h   , EDGE_WIDTH  , sides);
+        s->dsp.draw_edges(s->current_picture_ptr->data[1] + (y>>1)*s->uvlinesize, s->uvlinesize, s->h_edge_pos>>1, edge_h>>1, EDGE_WIDTH/2, sides);
+        s->dsp.draw_edges(s->current_picture_ptr->data[2] + (y>>1)*s->uvlinesize, s->uvlinesize, s->h_edge_pos>>1, edge_h>>1, EDGE_WIDTH/2, sides);
+    }
+
+    h= FFMIN(h, s->avctx->height - y);
+
+    if(s->picture_structure != PICT_FRAME && s->first_field && !(s->avctx->slice_flags&SLICE_FLAG_ALLOW_FIELD)) return;
+
     if (s->avctx->draw_horiz_band) {
         AVFrame *src;
         int offset[4];
 
-        if(s->picture_structure != PICT_FRAME){
-            h <<= 1;
-            y <<= 1;
-            if(s->first_field  && !(s->avctx->slice_flags&SLICE_FLAG_ALLOW_FIELD)) return;
-        }
-
-        h= FFMIN(h, s->avctx->height - y);
-
         if(s->pict_type==FF_B_TYPE || s->low_delay || (s->avctx->slice_flags&SLICE_FLAG_CODED_ORDER))
             src= (AVFrame*)s->current_picture_ptr;
         else if(s->last_picture_ptr)
             src= (AVFrame*)s->last_picture_ptr;
         else
             return;
 
         if(s->pict_type==FF_B_TYPE && s->picture_structure == PICT_FRAME && s->out_format != FMT_H264){
             offset[0]=
             offset[1]=
             offset[2]=
@@ -2051,26 +2205,26 @@
         s->dest[2] += s->mb_y * uvlinesize << (mb_size - s->chroma_y_shift);
     }
 }
 
 void ff_mpeg_flush(AVCodecContext *avctx){
     int i;
     MpegEncContext *s = avctx->priv_data;
 
     if(s==NULL || s->picture==NULL)
         return;
 
-    for(i=0; i<MAX_PICTURE_COUNT; i++){
+    for(i=0; i<s->picture_count; i++){
        if(s->picture[i].data[0] && (   s->picture[i].type == FF_BUFFER_TYPE_INTERNAL
                                     || s->picture[i].type == FF_BUFFER_TYPE_USER))
-        avctx->release_buffer(avctx, (AVFrame*)&s->picture[i]);
+        ff_release_buffer(s->avctx, (AVFrame*)&s->picture[i]);
     }
     s->current_picture_ptr = s->last_picture_ptr = s->next_picture_ptr = NULL;
 
     s->mb_x= s->mb_y= 0;
 
     s->parse_context.state= -1;
     s->parse_context.frame_start_found= 0;
     s->parse_context.overread= 0;
     s->parse_context.overread_index= 0;
     s->parse_context.index= 0;
     s->parse_context.last_index= 0;
@@ -2303,12 +2457,18 @@
 {
     if (qscale < 1)
         qscale = 1;
     else if (qscale > 31)
         qscale = 31;
 
     s->qscale = qscale;
     s->chroma_qscale= s->chroma_qscale_table[qscale];
 
     s->y_dc_scale= s->y_dc_scale_table[ qscale ];
     s->c_dc_scale= s->c_dc_scale_table[ s->chroma_qscale ];
+}
+
+void MPV_report_decode_progress(MpegEncContext *s)
+{
+    if (s->pict_type != FF_B_TYPE && !s->partitioned_frame)
+        ff_report_frame_progress((AVFrame*)s->current_picture_ptr, s->mb_y);
 }
--- mpegvideo.h	Sun Jan 25 13:19:00 2009
+++ mpegvideo.h	Sun Jan 25 13:07:22 2009
@@ -113,22 +113,23 @@
 #define HAS_CBP(a)        ((a)&MB_TYPE_CBP)
 
     int field_poc[2];           ///< h264 top/bottom POC
     int poc;                    ///< h264 frame POC
     int frame_num;              ///< h264 frame_num (raw frame_num from slice header)
     int pic_id;                 /**< h264 pic_num (short -> no wrap version of pic_num,
                                      pic_num & max_pic_num; long -> long_pic_num) */
     int long_ref;               ///< 1->long term reference 0->short term reference
     int ref_poc[2][2][16];      ///< h264 POCs of the frames used as reference (FIXME need per slice)
     int ref_count[2][2];        ///< number of entries in ref_poc              (FIXME need per slice)
     int mbaff;                  ///< h264 1 -> MBAFF frame 0-> not MBAFF
+    int field_picture;          ///< whether or not the picture was encoded in seperate fields
 
     int mb_var_sum;             ///< sum of MB variance for current frame
     int mc_mb_var_sum;          ///< motion compensated MB variance for current frame
     uint16_t *mb_var;           ///< Table for MB variances
     uint16_t *mc_mb_var;        ///< Table for motion compensated MB variances
     uint8_t *mb_mean;           ///< Table for MB luminance
     int32_t *mb_cmp_score;      ///< Table for MB cmp scores, for mb decision FIXME remove
     int b_frame_score;          /* */
 } Picture;
 
 struct MpegEncContext;
@@ -236,22 +237,25 @@
     int mb_stride;             ///< mb_width+1 used for some arrays to allow simple addressing of left & top MBs without sig11
     int b8_stride;             ///< 2*mb_width+1 used for some 8x8 block arrays to allow simple addressing
     int b4_stride;             ///< 4*mb_width+1 used for some 4x4 block arrays to allow simple addressing
     int h_edge_pos, v_edge_pos;///< horizontal / vertical position of the right/bottom edge (pixel replication)
     int mb_num;                ///< number of MBs of a picture
     int linesize;              ///< line size, in bytes, may be different from width
     int uvlinesize;            ///< line size, for chroma in bytes, may be different from width
     Picture *picture;          ///< main picture buffer
     Picture **input_picture;   ///< next pictures on display order for encoding
     Picture **reordered_input_picture; ///< pointer to the next pictures in codedorder for encoding
 
+    int picture_count;         ///< number of allocated pictures (MAX_PICTURE_COUNT * avctx->thread_count)
+    int picture_range_start, picture_range_end; ///< the part of picture that this context can allocate in
+
     int start_mb_y;            ///< start mb_y of this thread (so current thread should process start_mb_y <= row < end_mb_y)
     int end_mb_y;              ///< end   mb_y of this thread (so current thread should process start_mb_y <= row < end_mb_y)
     struct MpegEncContext *thread_context[MAX_THREADS];
 
     /**
      * copy of the previous picture structure.
      * note, linesize & data, might not match the previous picture (for field pictures)
      */
     Picture last_picture;
 
     /**
@@ -662,22 +666,23 @@
     void (*dct_unquantize_h261_inter)(struct MpegEncContext *s,
                            DCTELEM *block/*align 16*/, int n, int qscale);
     void (*dct_unquantize_intra)(struct MpegEncContext *s, // unquantizer to use (mpeg4 can use both)
                            DCTELEM *block/*align 16*/, int n, int qscale);
     void (*dct_unquantize_inter)(struct MpegEncContext *s, // unquantizer to use (mpeg4 can use both)
                            DCTELEM *block/*align 16*/, int n, int qscale);
     int (*dct_quantize)(struct MpegEncContext *s, DCTELEM *block/*align 16*/, int n, int qscale, int *overflow);
     int (*fast_dct_quantize)(struct MpegEncContext *s, DCTELEM *block/*align 16*/, int n, int qscale, int *overflow);
     void (*denoise_dct)(struct MpegEncContext *s, DCTELEM *block);
 } MpegEncContext;
 
+#define REBASE_PICTURE(pic, new_ctx, old_ctx) (pic ? &new_ctx->picture[pic - old_ctx->picture] : NULL)
 
 void MPV_decode_defaults(MpegEncContext *s);
 int MPV_common_init(MpegEncContext *s);
 void MPV_common_end(MpegEncContext *s);
 void MPV_decode_mb(MpegEncContext *s, DCTELEM block[12][64]);
 int MPV_frame_start(MpegEncContext *s, AVCodecContext *avctx);
 void MPV_frame_end(MpegEncContext *s);
 int MPV_encode_init(AVCodecContext *avctx);
 int MPV_encode_end(AVCodecContext *avctx);
 int MPV_encode_picture(AVCodecContext *avctx, unsigned char *buf, int buf_size, void *data);
 void MPV_common_init_mmx(MpegEncContext *s);
@@ -686,22 +691,25 @@
 void MPV_common_init_mmi(MpegEncContext *s);
 void MPV_common_init_arm(MpegEncContext *s);
 void MPV_common_init_altivec(MpegEncContext *s);
 void ff_clean_intra_table_entries(MpegEncContext *s);
 void ff_draw_horiz_band(MpegEncContext *s, int y, int h);
 void ff_mpeg_flush(AVCodecContext *avctx);
 void ff_print_debug_info(MpegEncContext *s, AVFrame *pict);
 void ff_write_quant_matrix(PutBitContext *pb, uint16_t *matrix);
 int ff_find_unused_picture(MpegEncContext *s, int shared);
 void ff_denoise_dct(MpegEncContext *s, DCTELEM *block);
 void ff_update_duplicate_context(MpegEncContext *dst, MpegEncContext *src);
+int MPV_lowest_referenced_row(MpegEncContext *s, int dir);
+void MPV_report_decode_progress(MpegEncContext *s);
+int ff_mpeg_update_context(AVCodecContext *dst, AVCodecContext *src);
 const uint8_t *ff_find_start_code(const uint8_t *p, const uint8_t *end, uint32_t *state);
 
 void ff_er_frame_start(MpegEncContext *s);
 void ff_er_frame_end(MpegEncContext *s);
 void ff_er_add_slice(MpegEncContext *s, int startx, int starty, int endx, int endy, int status);
 
 int ff_dct_common_init(MpegEncContext *s);
 void ff_convert_matrix(DSPContext *dsp, int (*qmat)[64], uint16_t (*qmat16)[2][64],
                        const uint16_t *quant_matrix, int bias, int qmin, int qmax, int intra);
 
 void ff_init_block_index(MpegEncContext *s);
--- mpegvideo_enc.c	Sun Jan 25 13:19:00 2009
+++ mpegvideo_enc.c	Sun Jan 25 13:07:22 2009
@@ -27,22 +27,23 @@
  * The simplest mpeg encoder (well, it was the simplest!).
  */
 
 #include "avcodec.h"
 #include "dsputil.h"
 #include "mpegvideo.h"
 #include "mpegvideo_common.h"
 #include "mjpegenc.h"
 #include "msmpeg4.h"
 #include "h263.h"
 #include "faandct.h"
+#include "thread.h"
 #include "aandcttab.h"
 #include <limits.h>
 
 //#undef NDEBUG
 //#include <assert.h>
 
 static int encode_picture(MpegEncContext *s, int picture_number);
 static int dct_quantize_refine(MpegEncContext *s, DCTELEM *block, int16_t *weight, DCTELEM *orig, int n, int qscale);
 static int sse_mb(MpegEncContext *s);
 
 /* enable all paranoid tests for rounding, overflows, etc... */
@@ -1177,25 +1178,25 @@
 //printf("dpn:%d\n", s->picture_number);
     }else{
        memset(&s->new_picture, 0, sizeof(Picture));
     }
 }
 
 int MPV_encode_picture(AVCodecContext *avctx,
                        unsigned char *buf, int buf_size, void *data)
 {
     MpegEncContext *s = avctx->priv_data;
     AVFrame *pic_arg = data;
-    int i, stuffing_count;
+    int i, stuffing_count, context_count = USE_AVCODEC_EXECUTE(avctx) ? avctx->thread_count : 1;
 
-    for(i=0; i<avctx->thread_count; i++){
+    for(i=0; i<context_count; i++){
         int start_y= s->thread_context[i]->start_mb_y;
         int   end_y= s->thread_context[i]->  end_mb_y;
         int h= s->mb_height;
         uint8_t *start= buf + (size_t)(((int64_t) buf_size)*start_y/h);
         uint8_t *end  = buf + (size_t)(((int64_t) buf_size)*  end_y/h);
 
         init_put_bits(&s->thread_context[i]->pb, start, end - start);
     }
 
     s->picture_in_gop_number++;
 
@@ -1242,23 +1243,23 @@
                 }
                 s->mb_skipped = 0;        //done in MPV_frame_start()
                 if(s->pict_type==FF_P_TYPE){ //done in encode_picture() so we must undo it
                     if(s->flipflop_rounding || s->codec_id == CODEC_ID_H263P || s->codec_id == CODEC_ID_MPEG4)
                         s->no_rounding ^= 1;
                 }
                 if(s->pict_type!=FF_B_TYPE){
                     s->time_base= s->last_time_base;
                     s->last_non_b_time= s->time - s->pp_time;
                 }
 //                av_log(NULL, AV_LOG_ERROR, "R:%d ", s->next_lambda);
-                for(i=0; i<avctx->thread_count; i++){
+                for(i=0; i<context_count; i++){
                     PutBitContext *pb= &s->thread_context[i]->pb;
                     init_put_bits(pb, pb->buf, pb->buf_end - pb->buf);
                 }
                 goto vbv_retry;
             }
 
             assert(s->avctx->rc_max_rate);
         }
 
         if(s->flags&CODEC_FLAG_PASS1)
             ff_write_pass1_stats(s);
@@ -2697,22 +2698,23 @@
     }else{
         s->pp_time= s->time - s->last_non_b_time;
         s->last_non_b_time= s->time;
         assert(s->picture_number==0 || s->pp_time > 0);
     }
 }
 
 static int encode_picture(MpegEncContext *s, int picture_number)
 {
     int i;
     int bits;
+    int context_count = USE_AVCODEC_EXECUTE(s->avctx) ? s->avctx->thread_count : 1;
 
     s->picture_number = picture_number;
 
     /* Reset the average MB variance */
     s->me.mb_var_sum_temp    =
     s->me.mc_mb_var_sum_temp = 0;
 
     /* we need to initialize some time vars before we can encode b-frames */
     // RAL: Condition added for MPEG1VIDEO
     if (s->codec_id == CODEC_ID_MPEG1VIDEO || s->codec_id == CODEC_ID_MPEG2VIDEO || (s->h263_pred && !s->h263_msmpeg4))
         set_frame_distances(s);
@@ -2736,51 +2738,51 @@
             return -1;
         ff_get_2pass_fcode(s);
     }else if(!(s->flags & CODEC_FLAG_QSCALE)){
         if(s->pict_type==FF_B_TYPE)
             s->lambda= s->last_lambda_for[s->pict_type];
         else
             s->lambda= s->last_lambda_for[s->last_non_b_pict_type];
         update_qscale(s);
     }
 
     s->mb_intra=0; //for the rate distortion & bit compare functions
-    for(i=1; i<s->avctx->thread_count; i++){
+    for(i=1; i<context_count; i++){
         ff_update_duplicate_context(s->thread_context[i], s);
     }
 
     if(ff_init_me(s)<0)
         return -1;
 
     /* Estimate motion for every MB */
     if(s->pict_type != FF_I_TYPE){
         s->lambda = (s->lambda * s->avctx->me_penalty_compensation + 128)>>8;
         s->lambda2= (s->lambda2* (int64_t)s->avctx->me_penalty_compensation + 128)>>8;
         if(s->pict_type != FF_B_TYPE && s->avctx->me_threshold==0){
             if((s->avctx->pre_me && s->last_non_b_pict_type==FF_I_TYPE) || s->avctx->pre_me==2){
-                s->avctx->execute(s->avctx, pre_estimate_motion_thread, (void**)&(s->thread_context[0]), NULL, s->avctx->thread_count, sizeof(void*));
+                s->avctx->execute(s->avctx, pre_estimate_motion_thread, (void**)&(s->thread_context[0]), NULL, context_count, sizeof(void*));
             }
         }
 
-        s->avctx->execute(s->avctx, estimate_motion_thread, (void**)&(s->thread_context[0]), NULL, s->avctx->thread_count, sizeof(void*));
+        s->avctx->execute(s->avctx, estimate_motion_thread, (void**)&(s->thread_context[0]), NULL, context_count, sizeof(void*));
     }else /* if(s->pict_type == FF_I_TYPE) */{
         /* I-Frame */
         for(i=0; i<s->mb_stride*s->mb_height; i++)
             s->mb_type[i]= CANDIDATE_MB_TYPE_INTRA;
 
         if(!s->fixed_qscale){
             /* finding spatial complexity for I-frame rate control */
-            s->avctx->execute(s->avctx, mb_var_thread, (void**)&(s->thread_context[0]), NULL, s->avctx->thread_count, sizeof(void*));
+            s->avctx->execute(s->avctx, mb_var_thread, (void**)&(s->thread_context[0]), NULL, context_count, sizeof(void*));
         }
     }
-    for(i=1; i<s->avctx->thread_count; i++){
+    for(i=1; i<context_count; i++){
         merge_context_after_me(s, s->thread_context[i]);
     }
     s->current_picture.mc_mb_var_sum= s->current_picture_ptr->mc_mb_var_sum= s->me.mc_mb_var_sum_temp;
     s->current_picture.   mb_var_sum= s->current_picture_ptr->   mb_var_sum= s->me.   mb_var_sum_temp;
     emms_c();
 
     if(s->me.scene_change_score > s->avctx->scenechange_threshold && s->pict_type == FF_P_TYPE){
         s->pict_type= FF_I_TYPE;
         for(i=0; i<s->mb_stride*s->mb_height; i++)
             s->mb_type[i]= CANDIDATE_MB_TYPE_INTRA;
 //printf("Scene change detected, encoding as I Frame %d %d\n", s->current_picture.mb_var_sum, s->current_picture.mc_mb_var_sum);
@@ -2898,27 +2900,27 @@
         if (CONFIG_MPEG1VIDEO_ENCODER || CONFIG_MPEG2VIDEO_ENCODER)
             mpeg1_encode_picture_header(s, picture_number);
         break;
     case FMT_H264:
         break;
     default:
         assert(0);
     }
     bits= put_bits_count(&s->pb);
     s->header_bits= bits - s->last_bits;
 
-    for(i=1; i<s->avctx->thread_count; i++){
+    for(i=1; i<context_count; i++){
         update_duplicate_context_after_me(s->thread_context[i], s);
     }
-    s->avctx->execute(s->avctx, encode_thread, (void**)&(s->thread_context[0]), NULL, s->avctx->thread_count, sizeof(void*));
-    for(i=1; i<s->avctx->thread_count; i++){
+    s->avctx->execute(s->avctx, encode_thread, (void**)&(s->thread_context[0]), NULL, context_count, sizeof(void*));
+    for(i=1; i<context_count; i++){
         merge_context_after_encode(s, s->thread_context[i]);
     }
     emms_c();
     return 0;
 }
 
 void  denoise_dct_c(MpegEncContext *s, DCTELEM *block){
     const int intra= s->mb_intra;
     int i;
 
     s->dct_count[intra]++;
--- options.c	Sun Jan 25 13:19:00 2009
+++ options.c	Sun Jan 25 13:07:22 2009
@@ -385,22 +385,25 @@
 {"timecode_frame_start", "GOP timecode frame start number, in non drop frame format", OFFSET(timecode_frame_start), FF_OPT_TYPE_INT, 0, 0, INT_MAX, V|E},
 {"drop_frame_timecode", NULL, 0, FF_OPT_TYPE_CONST, CODEC_FLAG2_DROP_FRAME_TIMECODE, INT_MIN, INT_MAX, V|E, "flags2"},
 {"non_linear_q", "use non linear quantizer", 0, FF_OPT_TYPE_CONST, CODEC_FLAG2_NON_LINEAR_QUANT, INT_MIN, INT_MAX, V|E, "flags2"},
 {"request_channels", "set desired number of audio channels", OFFSET(request_channels), FF_OPT_TYPE_INT, DEFAULT, 0, INT_MAX, A|D},
 {"drc_scale", "percentage of dynamic range compression to apply", OFFSET(drc_scale), FF_OPT_TYPE_FLOAT, 1.0, 0.0, 1.0, A|D},
 {"reservoir", "use bit reservoir", 0, FF_OPT_TYPE_CONST, CODEC_FLAG2_BIT_RESERVOIR, INT_MIN, INT_MAX, A|E, "flags2"},
 {"bits_per_raw_sample", NULL, OFFSET(bits_per_raw_sample), FF_OPT_TYPE_INT, DEFAULT, INT_MIN, INT_MAX},
 {"channel_layout", NULL, OFFSET(channel_layout), FF_OPT_TYPE_INT64, DEFAULT, 0, INT64_MAX, A|E|D, "channel_layout"},
 {"request_channel_layout", NULL, OFFSET(request_channel_layout), FF_OPT_TYPE_INT64, DEFAULT, 0, INT64_MAX, A|D, "request_channel_layout"},
 {"rc_max_vbv_use", NULL, OFFSET(rc_max_available_vbv_use), FF_OPT_TYPE_FLOAT, 1.0/3, 0.0, FLT_MAX, V|E},
 {"rc_min_vbv_use", NULL, OFFSET(rc_min_vbv_overflow_use),  FF_OPT_TYPE_FLOAT, 3,     0.0, FLT_MAX, V|E},
+{"thread_type", "select multithreading type", OFFSET(thread_type), FF_OPT_TYPE_INT, FF_THREAD_DEFAULT, 0, INT_MAX, V|E|D, "thread_type"},
+{"slice", NULL, 0, FF_OPT_TYPE_CONST, FF_THREAD_SLICE, INT_MIN, INT_MAX, V|E|D, "thread_type"},
+{"frame", NULL, 0, FF_OPT_TYPE_CONST, FF_THREAD_FRAME, INT_MIN, INT_MAX, V|E|D, "thread_type"},
 {NULL},
 };
 
 #undef A
 #undef V
 #undef S
 #undef E
 #undef D
 #undef DEFAULT
 
 static const AVClass av_codec_context_class = { "AVCodecContext", context_to_name, options };
--- os2thread.c	Sun Jan 25 13:19:00 2009
+++ os2thread.c	Sun Jan 25 13:07:22 2009
@@ -106,23 +106,29 @@
         c[i].func= NULL;
         if(ret) ret[i]= c[i].ret;
     }
     return 0;
 }
 
 int avcodec_thread_init(AVCodecContext *s, int thread_count){
     int i;
     ThreadContext *c;
     uint32_t threadid;
 
+    if(!(s->thread_type & FF_THREAD_SLICE)){
+        av_log(s, AV_LOG_WARNING, "The requested thread algorithm is not supported with this thread library.\n");
+        return 0;
+    }
+
     s->thread_count= thread_count;
+    s->active_thread_type= FF_THREAD_SLICE;
 
     assert(!s->thread_opaque);
     c= av_mallocz(sizeof(ThreadContext)*thread_count);
     s->thread_opaque= c;
 
     for(i=0; i<thread_count; i++){
 //printf("init semaphors %d\n", i); fflush(stdout);
         c[i].avctx= s;
 
         if (DosCreateEventSem(NULL,&c[i].work_sem,DC_SEM_SHARED,0))
             goto fail;
--- pthread.c	Sun Jan 25 13:19:00 2009
+++ pthread.c	Sun Jan 25 13:07:23 2009
@@ -1,13 +1,14 @@
 /*
  * Copyright (c) 2004 Roman Shaposhnik
+ * Copyright (c) 2008 Alexander Strange (astrange@ithinksw.com)
  *
  * Many thanks to Steven M. Schultz for providing clever ideas and
  * to Michael Niedermayer <michaelni@gmx.at> for writing initial
  * implementation.
  *
  * This file is part of FFmpeg.
  *
  * FFmpeg is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
  * License as published by the Free Software Foundation; either
  * version 2.1 of the License, or (at your option) any later version.
@@ -16,41 +17,99 @@
  * but WITHOUT ANY WARRANTY; without even the implied warranty of
  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
  * Lesser General Public License for more details.
  *
  * You should have received a copy of the GNU Lesser General Public
  * License along with FFmpeg; if not, write to the Free Software
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 #include <pthread.h>
 
 #include "avcodec.h"
+#include "thread.h"
+
+#define MAX_DELAYED_RELEASED_BUFFERS 32
 
 typedef int (action_func)(AVCodecContext *c, void *arg);
 
 typedef struct ThreadContext {
     pthread_t *workers;
     action_func *func;
     void *args;
     int *rets;
     int rets_count;
     int job_count;
     int job_size;
 
     pthread_cond_t last_job_cond;
     pthread_cond_t current_job_cond;
     pthread_mutex_t current_job_lock;
     int current_job;
     int done;
 } ThreadContext;
 
+typedef struct PerThreadContext {
+    pthread_t thread;
+    pthread_cond_t input_cond;      ///< Used to wait for a new frame from the main thread.
+    pthread_cond_t progress_cond;   ///< Used by child threads to wait for decoding/encoding progress.
+    pthread_cond_t output_cond;     ///< Used by the main thread to wait for frames to finish.
+
+    pthread_mutex_t mutex;          ///< Mutex used to protect the contents of the PerThreadContext.
+    pthread_mutex_t progress_mutex; ///< Mutex used to protect frame progress values and progress_cond.
+
+    AVCodecContext *avctx;          ///< Context used to decode frames passed to this thread.
+
+    uint8_t *buf;                   ///< Input frame (for decoding) or output (for encoding).
+    int buf_size;
+    int allocated_buf_size;
+
+    AVFrame picture;                ///< Output frame (for decoding) or input (for encoding).
+    int got_picture;                ///< The output of got_picture_ptr from the last avcodec_decode_video() call (for decoding).
+    int result;                     ///< The result of the last codec decode/encode() call.
+
+    struct FrameThreadContext *parent;
+
+    enum {
+        STATE_INPUT_READY,          ///< Set when the thread is sleeping.
+        STATE_SETTING_UP,           ///< Set before the codec has called ff_report_frame_setup_done().
+        STATE_SETUP_FINISHED        /**<
+                                     * Set after the codec has called ff_report_frame_setup_done().
+                                     * At this point it is safe to start the next thread.
+                                     */
+    } state;
+
+    /**
+     * Array of frames passed to ff_release_buffer(),
+     * to be released later.
+     */
+    AVFrame released_buffers[MAX_DELAYED_RELEASED_BUFFERS];
+    int num_released_buffers;
+} PerThreadContext;
+
+typedef struct FrameThreadContext {
+    PerThreadContext *threads;     ///< The contexts for frame decoding threads.
+    PerThreadContext *prev_thread; ///< The last thread submit_frame() was called on.
+
+    int next_decoding;             ///< The next context to submit frames to.
+    int next_finished;             ///< The next context to return output from.
+
+    int delaying;                  /**
+                                    * Set for the first N frames, where N is the number of threads.
+                                    * While it is set, ff_en/decode_frame_threaded won't return any results.
+                                    */
+
+    pthread_mutex_t buffer_mutex;  ///< Mutex used to protect get/release_buffer().
+
+    int die;                       ///< Set to cause threads to exit.
+} FrameThreadContext;
+
 static void* attribute_align_arg worker(void *v)
 {
     AVCodecContext *avctx = v;
     ThreadContext *c = avctx->thread_opaque;
     int our_job = c->job_count;
     int thread_count = avctx->thread_count;
     int self_id;
 
     pthread_mutex_lock(&c->current_job_lock);
     self_id = c->current_job++;
     for (;;){
@@ -73,23 +132,23 @@
         pthread_mutex_lock(&c->current_job_lock);
         our_job = c->current_job++;
     }
 }
 
 static av_always_inline void avcodec_thread_park_workers(ThreadContext *c, int thread_count)
 {
     pthread_cond_wait(&c->last_job_cond, &c->current_job_lock);
     pthread_mutex_unlock(&c->current_job_lock);
 }
 
-void avcodec_thread_free(AVCodecContext *avctx)
+static void thread_free(AVCodecContext *avctx)
 {
     ThreadContext *c = avctx->thread_opaque;
     int i;
 
     pthread_mutex_lock(&c->current_job_lock);
     c->done = 1;
     pthread_cond_broadcast(&c->current_job_cond);
     pthread_mutex_unlock(&c->current_job_lock);
 
     for (i=0; i<avctx->thread_count; i++)
          pthread_join(c->workers[i], NULL);
@@ -98,22 +157,25 @@
     pthread_cond_destroy(&c->current_job_cond);
     pthread_cond_destroy(&c->last_job_cond);
     av_free(c->workers);
     av_freep(&avctx->thread_opaque);
 }
 
 int avcodec_thread_execute(AVCodecContext *avctx, action_func* func, void *arg, int *ret, int job_count, int job_size)
 {
     ThreadContext *c= avctx->thread_opaque;
     int dummy_ret;
 
+    if (!USE_AVCODEC_EXECUTE(avctx) || avctx->thread_count <= 1)
+        return avcodec_default_execute(avctx, func, arg, ret, job_count, job_size);
+
     if (job_count <= 0)
         return 0;
 
     pthread_mutex_lock(&c->current_job_lock);
 
     c->current_job = avctx->thread_count;
     c->job_count = job_count;
     c->job_size = job_size;
     c->args = arg;
     c->func = func;
     if (ret) {
@@ -122,23 +184,23 @@
     } else {
         c->rets = &dummy_ret;
         c->rets_count = 1;
     }
     pthread_cond_broadcast(&c->current_job_cond);
 
     avcodec_thread_park_workers(c, avctx->thread_count);
 
     return 0;
 }
 
-int avcodec_thread_init(AVCodecContext *avctx, int thread_count)
+static int thread_init(AVCodecContext *avctx, int thread_count)
 {
     int i;
     ThreadContext *c;
 
     c = av_mallocz(sizeof(ThreadContext));
     if (!c)
         return -1;
 
     c->workers = av_mallocz(sizeof(pthread_t)*thread_count);
     if (!c->workers) {
         av_free(c);
@@ -160,12 +222,500 @@
            avctx->thread_count = i;
            pthread_mutex_unlock(&c->current_job_lock);
            avcodec_thread_free(avctx);
            return -1;
         }
     }
 
     avcodec_thread_park_workers(c, thread_count);
 
     avctx->execute = avcodec_thread_execute;
     return 0;
+}
+
+/**
+ * Read and decode frames from the main thread until fctx->die is set.
+ * ff_report_frame_setup_done() is called before decoding if the codec
+ * doesn't define update_context(). To simplify codecs and avoid deadlock
+ * bugs, progress is set to INT_MAX on all returned frames.
+ */
+static attribute_align_arg void *frame_worker_thread(void *arg)
+{
+    PerThreadContext * volatile p = arg;
+    AVCodecContext *avctx = p->avctx;
+    FrameThreadContext * volatile fctx = p->parent;
+    AVCodec *codec = avctx->codec;
+
+    while (1) {
+        pthread_mutex_lock(&p->mutex);
+        while (p->state == STATE_INPUT_READY && !fctx->die)
+            pthread_cond_wait(&p->input_cond, &p->mutex);
+        pthread_mutex_unlock(&p->mutex);
+
+        if (fctx->die) break;
+
+        if (!codec->update_context) ff_report_frame_setup_done(avctx);
+
+        pthread_mutex_lock(&p->mutex);
+        p->result = codec->decode(avctx, &p->picture, &p->got_picture, p->buf, p->buf_size);
+
+        if (p->state == STATE_SETTING_UP) ff_report_frame_setup_done(avctx);
+        if (p->got_picture) {
+            ff_report_field_progress(&p->picture, INT_MAX, 0);
+            ff_report_field_progress(&p->picture, INT_MAX, 1);
+        }
+
+        p->buf_size = 0;
+        p->state = STATE_INPUT_READY;
+
+        pthread_mutex_lock(&p->progress_mutex);
+        pthread_cond_signal(&p->output_cond);
+        pthread_mutex_unlock(&p->progress_mutex);
+        pthread_mutex_unlock(&p->mutex);
+    };
+
+    return NULL;
+}
+
+static int frame_thread_init(AVCodecContext *avctx)
+{
+    FrameThreadContext *fctx;
+    AVCodecContext *src = avctx;
+    AVCodec *codec = avctx->codec;
+    int i, thread_count = avctx->thread_count, err = 0;
+
+    avctx->thread_opaque = fctx = av_mallocz(sizeof(FrameThreadContext));
+    fctx->delaying = 1;
+    pthread_mutex_init(&fctx->buffer_mutex, NULL);
+
+    fctx->threads = av_mallocz(sizeof(PerThreadContext) * thread_count);
+
+    for (i = 0; i < thread_count; i++) {
+        AVCodecContext *copy = av_malloc(sizeof(AVCodecContext));
+        PerThreadContext *p  = &fctx->threads[i];
+
+        pthread_mutex_init(&p->mutex, NULL);
+        pthread_mutex_init(&p->progress_mutex, NULL);
+        pthread_cond_init(&p->input_cond, NULL);
+        pthread_cond_init(&p->progress_cond, NULL);
+        pthread_cond_init(&p->output_cond, NULL);
+
+        p->parent = fctx;
+        p->avctx  = copy;
+
+        *copy = *src;
+        copy->thread_opaque = p;
+
+        if (!i) {
+            src = copy;
+
+            if (codec->init)
+                err = codec->init(copy);
+        } else {
+            copy->is_copy   = 1;
+            copy->priv_data = av_malloc(codec->priv_data_size);
+            memcpy(copy->priv_data, src->priv_data, codec->priv_data_size);
+
+            if (codec->init_copy)
+                err = codec->init_copy(copy);
+        }
+
+        if (err) goto error;
+
+        pthread_create(&p->thread, NULL, frame_worker_thread, p);
+    }
+
+    return 0;
+
+error:
+    avctx->thread_count = i;
+    avcodec_thread_free(avctx);
+
+    return err;
+}
+
+/**
+ * Update a thread's context from the last thread. This is used for returning
+ * frames and for starting new decoding jobs after the previous one finishes
+ * predecoding.
+ *
+ * @param dst The destination context.
+ * @param src The source context.
+ * @param for_user Whether or not dst is the user-visible context. update_context won't be called and some pointers will be copied.
+ */
+static int update_context_from_copy(AVCodecContext *dst, AVCodecContext *src, int for_user)
+{
+    int err = 0;
+#define COPY(f) dst->f = src->f;
+#define COPY_FIELDS(s, e) memcpy(&dst->s, &src->s, (char*)&dst->e - (char*)&dst->s);
+
+    //coded_width/height are not copied here, so that codecs' update_context can see when they change
+    //many encoding parameters could be theoretically changed during encode, but aren't copied ATM
+
+    COPY(sub_id);
+    COPY(width);
+    COPY(height);
+    COPY(pix_fmt);
+    COPY(real_pict_num); //necessary?
+    COPY(delay);
+    COPY(max_b_frames);
+
+    COPY_FIELDS(mv_bits, opaque);
+
+    COPY(has_b_frames);
+    COPY(bits_per_coded_sample);
+    COPY(sample_aspect_ratio);
+    COPY(idct_algo);
+    if (for_user) COPY(coded_frame);
+    memcpy(dst->error, src->error, sizeof(src->error));
+    COPY(last_predictor_count); //necessary?
+    COPY(dtg_active_format);
+    COPY(color_table_id);
+    COPY(profile);
+    COPY(level);
+    COPY(bits_per_raw_sample);
+
+    if (!for_user) {
+        if (dst->codec->update_context)
+            err = dst->codec->update_context(dst, src);
+    }
+
+    return err;
+}
+
+///Update the next decoding thread with values set by the user
+static void update_context_from_user(AVCodecContext *dst, AVCodecContext *src)
+{
+    COPY(hurry_up);
+    COPY_FIELDS(skip_loop_filter, bidir_refine);
+    COPY(frame_number);
+    COPY(reordered_opaque);
+}
+
+/// Release all frames passed to ff_release_buffer()
+static void handle_delayed_releases(PerThreadContext *p)
+{
+    FrameThreadContext *fctx = p->parent;
+
+    while (p->num_released_buffers > 0) {
+        AVFrame *f = &p->released_buffers[--p->num_released_buffers];
+
+        av_freep(&f->thread_opaque);
+
+        pthread_mutex_lock(&fctx->buffer_mutex);
+        f->owner->release_buffer(f->owner, f);
+        pthread_mutex_unlock(&fctx->buffer_mutex);
+    }
+}
+
+/// Submit a frame to the next decoding thread
+static int submit_frame(PerThreadContext * volatile p, const uint8_t *buf, int buf_size)
+{
+    FrameThreadContext *fctx = p->parent;
+    PerThreadContext *prev_thread = fctx->prev_thread;
+    AVCodec *codec = p->avctx->codec;
+    int err = 0;
+
+    if (!buf_size && !(codec->capabilities & CODEC_CAP_DELAY)) return 0;
+
+    pthread_mutex_lock(&p->mutex);
+    if (prev_thread) {
+        pthread_mutex_lock(&prev_thread->progress_mutex);
+        while (prev_thread->state == STATE_SETTING_UP)
+            pthread_cond_wait(&prev_thread->progress_cond, &prev_thread->progress_mutex);
+        pthread_mutex_unlock(&prev_thread->progress_mutex);
+
+        err = update_context_from_copy(p->avctx, prev_thread->avctx, 0);
+        if (err) return err;
+    }
+
+    //FIXME: the client API should allow copy-on-write
+    p->buf = av_fast_realloc(p->buf, &p->allocated_buf_size, buf_size + FF_INPUT_BUFFER_PADDING_SIZE);
+    memcpy(p->buf, buf, buf_size);
+    memset(p->buf + buf_size, 0, FF_INPUT_BUFFER_PADDING_SIZE);
+    p->buf_size = buf_size;
+
+    handle_delayed_releases(p);
+
+    p->state = STATE_SETTING_UP;
+    pthread_cond_signal(&p->input_cond);
+    pthread_mutex_unlock(&p->mutex);
+
+    fctx->prev_thread = p;
+
+    return err;
+}
+
+int ff_decode_frame_threaded(AVCodecContext *avctx,
+                             void *data, int *data_size,
+                             const uint8_t *buf, int buf_size)
+{
+    FrameThreadContext *fctx;
+    PerThreadContext * volatile p;
+    int thread_count = avctx->thread_count, err = 0;
+    int returning_thread;
+
+    if (!avctx->thread_opaque) frame_thread_init(avctx);
+    fctx = avctx->thread_opaque;
+    returning_thread = fctx->next_finished;
+
+    p = &fctx->threads[fctx->next_decoding];
+    update_context_from_user(p->avctx, avctx);
+    err = submit_frame(p, buf, buf_size);
+    if (err) return err;
+
+    fctx->next_decoding++;
+
+    if (fctx->delaying) {
+        if (fctx->next_decoding >= (thread_count-1)) fctx->delaying = 0;
+
+        *data_size=0;
+        return 0;
+    }
+
+    //If it's draining frames at EOF, ignore null frames from the codec.
+    //Only return one when we've run out of codec frames to return.
+    do {
+        p = &fctx->threads[returning_thread++];
+
+        pthread_mutex_lock(&p->progress_mutex);
+        while (p->state != STATE_INPUT_READY)
+            pthread_cond_wait(&p->output_cond, &p->progress_mutex);
+        pthread_mutex_unlock(&p->progress_mutex);
+
+        *(AVFrame*)data = p->picture;
+        *data_size = p->got_picture;
+
+        avcodec_get_frame_defaults(&p->picture);
+        p->got_picture = 0;
+
+        if (returning_thread >= thread_count) returning_thread = 0;
+    } while (!buf_size && !*data_size && returning_thread != fctx->next_finished);
+
+    update_context_from_copy(avctx, p->avctx, 1);
+
+    if (fctx->next_decoding >= thread_count) fctx->next_decoding = 0;
+    fctx->next_finished = returning_thread;
+
+    return p->result;
+}
+
+void ff_report_field_progress(AVFrame *f, int n, int field)
+{
+    PerThreadContext *p = f->owner->thread_opaque;
+    int *progress = f->thread_opaque;
+
+    if (progress[field] >= n) return;
+
+    pthread_mutex_lock(&p->progress_mutex);
+    progress[field] = n;
+    pthread_cond_broadcast(&p->progress_cond);
+    pthread_mutex_unlock(&p->progress_mutex);
+}
+
+void ff_await_field_progress(AVFrame *f, int n, int field)
+{
+    PerThreadContext *p = f->owner->thread_opaque;
+    int * volatile progress = f->thread_opaque;
+
+    if (progress[field] >= n) return;
+
+    pthread_mutex_lock(&p->progress_mutex);
+    while (progress[field] < n)
+        pthread_cond_wait(&p->progress_cond, &p->progress_mutex);
+    pthread_mutex_unlock(&p->progress_mutex);
+}
+
+void ff_report_frame_progress(AVFrame *f, int n)
+{
+    ff_report_field_progress(f, n, 0);
+}
+
+void ff_await_frame_progress(AVFrame *f, int n)
+{
+    ff_await_field_progress(f, n, 0);
+}
+
+void ff_report_frame_setup_done(AVCodecContext *avctx) {
+    PerThreadContext *p = avctx->thread_opaque;
+
+    if (!USE_FRAME_THREADING(avctx)) return;
+
+    pthread_mutex_lock(&p->progress_mutex);
+    p->state = STATE_SETUP_FINISHED;
+    pthread_cond_broadcast(&p->progress_cond);
+    pthread_mutex_unlock(&p->progress_mutex);
+}
+
+/// Wait for all threads to finish decoding
+static void park_frame_worker_threads(FrameThreadContext *fctx, int thread_count)
+{
+    int i;
+
+    for (i = 0; i < thread_count; i++) {
+        PerThreadContext *p = &fctx->threads[i];
+
+        pthread_mutex_lock(&p->progress_mutex);
+        while (p->state != STATE_INPUT_READY)
+            pthread_cond_wait(&p->output_cond, &p->progress_mutex);
+        pthread_mutex_unlock(&p->progress_mutex);
+    }
+}
+
+static void frame_thread_free(AVCodecContext *avctx)
+{
+    FrameThreadContext *fctx = avctx->thread_opaque;
+    AVCodec *codec = avctx->codec;
+    int i;
+
+    park_frame_worker_threads(fctx, avctx->thread_count);
+
+    if (fctx->prev_thread && fctx->prev_thread != fctx->threads)
+        update_context_from_copy(fctx->threads->avctx, fctx->prev_thread->avctx, 0);
+
+    fctx->die = 1;
+
+    for (i = 0; i < avctx->thread_count; i++) {
+        PerThreadContext *p = &fctx->threads[i];
+
+        pthread_mutex_lock(&p->mutex);
+        pthread_cond_signal(&p->input_cond);
+        pthread_mutex_unlock(&p->mutex);
+
+        pthread_join(p->thread, NULL);
+
+        if (codec->close)
+            codec->close(p->avctx);
+
+        handle_delayed_releases(p);
+    }
+
+    for (i = 0; i < avctx->thread_count; i++) {
+        PerThreadContext *p = &fctx->threads[i];
+
+        avcodec_default_free_buffers(p->avctx);
+
+        pthread_mutex_destroy(&p->mutex);
+        pthread_mutex_destroy(&p->progress_mutex);
+        pthread_cond_destroy(&p->input_cond);
+        pthread_cond_destroy(&p->progress_cond);
+        pthread_cond_destroy(&p->output_cond);
+        av_freep(&p->buf);
+
+        if (i)
+            av_freep(&p->avctx->priv_data);
+
+        av_freep(&p->avctx);
+    }
+
+    av_freep(&fctx->threads);
+    pthread_mutex_destroy(&fctx->buffer_mutex);
+    av_freep(&avctx->thread_opaque);
+}
+
+void ff_frame_thread_flush(AVCodecContext *avctx)
+{
+    FrameThreadContext *fctx = avctx->thread_opaque;
+
+    if (!avctx->thread_opaque) return;
+
+    park_frame_worker_threads(fctx, avctx->thread_count);
+
+    if (fctx->prev_thread && fctx->prev_thread != fctx->threads)
+        update_context_from_copy(fctx->threads->avctx, fctx->prev_thread->avctx, 0);
+
+    fctx->next_decoding = fctx->next_finished = 0;
+    fctx->delaying = 1;
+    fctx->prev_thread = NULL;
+}
+
+int ff_get_buffer(AVCodecContext *avctx, AVFrame *f)
+{
+    int ret, *progress;
+    PerThreadContext *p = avctx->thread_opaque;
+
+    f->owner = avctx;
+    f->thread_opaque = progress = av_malloc(sizeof(int)*2);
+
+    if (!USE_FRAME_THREADING(avctx)) {
+        progress[0] =
+        progress[1] = INT_MAX;
+        return avctx->get_buffer(avctx, f);
+    }
+
+    progress[0] =
+    progress[1] = -1;
+
+    pthread_mutex_lock(&p->parent->buffer_mutex);
+    ret = avctx->get_buffer(avctx, f);
+    pthread_mutex_unlock(&p->parent->buffer_mutex);
+
+    /*
+     * The buffer list isn't shared between threads,
+     * so age doesn't mean what codecs expect it to mean.
+     * Disable it for now.
+     */
+    f->age = INT_MAX;
+
+    return ret;
+}
+
+void ff_release_buffer(AVCodecContext *avctx, AVFrame *f)
+{
+    PerThreadContext *p = avctx->thread_opaque;
+
+    if (!USE_FRAME_THREADING(avctx)) {
+        av_freep(&f->thread_opaque);
+        avctx->release_buffer(avctx, f);
+        return;
+    }
+
+    if (p->num_released_buffers >= MAX_DELAYED_RELEASED_BUFFERS) {
+        av_log(p->avctx, AV_LOG_ERROR, "too many delayed release_buffer calls!\n");
+        return;
+    }
+
+    if(avctx->debug & FF_DEBUG_BUFFERS)
+        av_log(avctx, AV_LOG_DEBUG, "delayed_release_buffer called on pic %p, %d buffers used\n",
+                                    f, f->owner->internal_buffer_count);
+
+    p->released_buffers[p->num_released_buffers++] = *f;
+    memset(f->data, 0, sizeof(f->data));
+}
+
+/// Set the threading algorithm used, or none if an algorithm was set but no thread count.
+static void validate_thread_parameters(AVCodecContext *avctx)
+{
+    int frame_threading_supported = (avctx->codec->capabilities & CODEC_CAP_FRAME_THREADS)
+                                && !(avctx->flags & CODEC_FLAG_TRUNCATED)
+                                && !(avctx->flags & CODEC_FLAG_LOW_DELAY)
+                                && !(avctx->flags2 & CODEC_FLAG2_CHUNKS);
+    if (avctx->thread_count <= 1)
+        avctx->active_thread_type = 0;
+    else if (frame_threading_supported && (avctx->thread_type & FF_THREAD_FRAME))
+        avctx->active_thread_type = FF_THREAD_FRAME;
+    else
+        avctx->active_thread_type = FF_THREAD_SLICE;
+}
+
+int avcodec_thread_init(AVCodecContext *avctx, int thread_count)
+{
+    avctx->thread_count = thread_count;
+
+    if (avctx->codec) {
+        validate_thread_parameters(avctx);
+
+        // frame_thread_init must be called after codec init
+        if (USE_AVCODEC_EXECUTE(avctx))
+            return thread_init(avctx, thread_count);
+    }
+
+    return 0;
+}
+
+void avcodec_thread_free(AVCodecContext *avctx)
+{
+    if (USE_FRAME_THREADING(avctx))
+        frame_thread_free(avctx);
+    else
+        thread_free(avctx);
 }
--- rectangle.h	Sun Jan 25 13:19:00 2009
+++ rectangle.h	Sun Jan 25 13:07:23 2009
@@ -20,26 +20,23 @@
  */
 
 /**
  * @file rectangle.h
  * useful rectangle filling function
  * @author Michael Niedermayer <michaelni@gmx.at>
  */
 
 #ifndef AVCODEC_RECTANGLE_H
 #define AVCODEC_RECTANGLE_H
 
-#include <assert.h>
-#include "config.h"
 #include "libavutil/common.h"
-#include "dsputil.h"
 
 /**
  * fill a rectangle.
  * @param h height of the rectangle, should be a constant
  * @param w width of the rectangle, should be a constant
  * @param size the size of val (1 or 4), should be a constant
  */
 static av_always_inline void fill_rectangle(void *vp, int w, int h, int stride, uint32_t val, int size){
     uint8_t *p= (uint8_t*)vp;
     assert(size==1 || size==4);
     assert(w<=4);
--- roqvideo.h	Sun Jan 25 13:19:00 2009
+++ roqvideo.h	Sun Jan 25 13:07:23 2009
@@ -31,24 +31,22 @@
     unsigned char u, v;
 } roq_cell;
 
 typedef struct {
     int idx[4];
 } roq_qcell;
 
 typedef struct {
     int d[2];
 } motion_vect;
 
-struct RoqTempData;
-
 typedef struct RoqContext {
 
     AVCodecContext *avctx;
     DSPContext dsp;
     AVFrame frames[2];
     AVFrame *last_frame;
     AVFrame *current_frame;
     int first_frame;
 
     roq_cell cb2x2[256];
     roq_qcell cb4x4[256];
@@ -63,23 +61,22 @@
 
     motion_vect *this_motion4;
     motion_vect *last_motion4;
 
     motion_vect *this_motion8;
     motion_vect *last_motion8;
 
     unsigned int framesSinceKeyframe;
 
     AVFrame *frame_to_enc;
     uint8_t *out_buf;
-    struct RoqTempData *tmpData;
 } RoqContext;
 
 #define RoQ_INFO              0x1001
 #define RoQ_QUAD_CODEBOOK     0x1002
 #define RoQ_QUAD_VQ           0x1011
 #define RoQ_SOUND_MONO        0x1020
 #define RoQ_SOUND_STEREO      0x1021
 
 #define RoQ_ID_MOT              0x00
 #define RoQ_ID_FCC              0x01
 #define RoQ_ID_SLD              0x02
--- roqvideoenc.c	Sun Jan 25 13:19:00 2009
+++ roqvideoenc.c	Sun Jan 25 13:07:24 2009
@@ -212,23 +212,23 @@
     int numCB2;
     int usedCB2[MAX_CBS_2x2];
     int usedCB4[MAX_CBS_4x4];
     uint8_t unpacked_cb2[MAX_CBS_2x2*2*2*3];
     uint8_t unpacked_cb4[MAX_CBS_4x4*4*4*3];
     uint8_t unpacked_cb4_enlarged[MAX_CBS_4x4*8*8*3];
 } RoqCodebooks;
 
 /**
  * Temporary vars
  */
-typedef struct RoqTempData
+typedef struct
 {
     CelEvaluation *cel_evals;
 
     int f2i4[MAX_CBS_4x4];
     int i2f4[MAX_CBS_4x4];
     int f2i2[MAX_CBS_2x2];
     int i2f2[MAX_CBS_2x2];
 
     int mainChunkSize;
 
     int numCB4;
@@ -873,62 +873,62 @@
         enlarge_roq_mb4(codebooks->unpacked_cb4 + i*4*4*3,
                         codebooks->unpacked_cb4_enlarged + i*8*8*3);
     }
 
     av_free(yuvClusters);
     av_free(points);
     av_free(results4);
 }
 
 static void roq_encode_video(RoqContext *enc)
 {
-    RoqTempdata *tempData = enc->tmpData;
+    RoqTempdata tempData;
     int i;
 
-    memset(tempData, 0, sizeof(*tempData));
+    memset(&tempData, 0, sizeof(tempData));
 
-    create_cel_evals(enc, tempData);
+    create_cel_evals(enc, &tempData);
 
-    generate_new_codebooks(enc, tempData);
+    generate_new_codebooks(enc, &tempData);
 
     if (enc->framesSinceKeyframe >= 1) {
         motion_search(enc, 8);
         motion_search(enc, 4);
     }
 
  retry_encode:
     for (i=0; i<enc->width*enc->height/64; i++)
-        gather_data_for_cel(tempData->cel_evals + i, enc, tempData);
+        gather_data_for_cel(tempData.cel_evals + i, enc, &tempData);
 
     /* Quake 3 can't handle chunks bigger than 65536 bytes */
-    if (tempData->mainChunkSize/8 > 65536) {
+    if (tempData.mainChunkSize/8 > 65536) {
         enc->lambda *= .8;
         goto retry_encode;
     }
 
-    remap_codebooks(enc, tempData);
+    remap_codebooks(enc, &tempData);
 
-    write_codebooks(enc, tempData);
+    write_codebooks(enc, &tempData);
 
-    reconstruct_and_encode_image(enc, tempData, enc->width, enc->height,
+    reconstruct_and_encode_image(enc, &tempData, enc->width, enc->height,
                                  enc->width*enc->height/64);
 
     enc->avctx->coded_frame = enc->current_frame;
 
     /* Rotate frame history */
     FFSWAP(AVFrame *, enc->current_frame, enc->last_frame);
     FFSWAP(motion_vect *, enc->last_motion4, enc->this_motion4);
     FFSWAP(motion_vect *, enc->last_motion8, enc->this_motion8);
 
-    av_free(tempData->cel_evals);
-    av_free(tempData->closest_cb2);
+    av_free(tempData.cel_evals);
+    av_free(tempData.closest_cb2);
 
     enc->framesSinceKeyframe++;
 }
 
 static int roq_encode_init(AVCodecContext *avctx)
 {
     RoqContext *enc = avctx->priv_data;
 
     av_random_init(&enc->randctx, 1);
 
     enc->framesSinceKeyframe = 0;
@@ -947,24 +947,22 @@
     }
 
     enc->width = avctx->width;
     enc->height = avctx->height;
 
     enc->framesSinceKeyframe = 0;
     enc->first_frame = 1;
 
     enc->last_frame    = &enc->frames[0];
     enc->current_frame = &enc->frames[1];
 
-    enc->tmpData      = av_malloc(sizeof(RoqTempdata));
-
     enc->this_motion4 =
         av_mallocz((enc->width*enc->height/16)*sizeof(motion_vect));
 
     enc->last_motion4 =
         av_malloc ((enc->width*enc->height/16)*sizeof(motion_vect));
 
     enc->this_motion8 =
         av_mallocz((enc->width*enc->height/64)*sizeof(motion_vect));
 
     enc->last_motion8 =
         av_malloc ((enc->width*enc->height/64)*sizeof(motion_vect));
@@ -1044,23 +1042,22 @@
 
     return enc->out_buf - buf_start;
 }
 
 static int roq_encode_end(AVCodecContext *avctx)
 {
     RoqContext *enc = avctx->priv_data;
 
     avctx->release_buffer(avctx, enc->last_frame);
     avctx->release_buffer(avctx, enc->current_frame);
 
-    av_free(enc->tmpData);
     av_free(enc->this_motion4);
     av_free(enc->last_motion4);
     av_free(enc->this_motion8);
     av_free(enc->last_motion8);
 
     return 0;
 }
 
 AVCodec roq_encoder =
 {
     "roqvideo",
--- snow.c	Sun Jan 25 13:19:00 2009
+++ snow.c	Sun Jan 25 13:07:24 2009
@@ -4109,25 +4109,25 @@
 
 //FIXME border!
     }
 }
 
 static int frame_start(SnowContext *s){
    AVFrame tmp;
    int w= s->avctx->width; //FIXME round up to x16 ?
    int h= s->avctx->height;
 
     if(s->current_picture.data[0]){
-        s->dsp.draw_edges(s->current_picture.data[0], s->current_picture.linesize[0], w   , h   , EDGE_WIDTH  );
-        s->dsp.draw_edges(s->current_picture.data[1], s->current_picture.linesize[1], w>>1, h>>1, EDGE_WIDTH/2);
-        s->dsp.draw_edges(s->current_picture.data[2], s->current_picture.linesize[2], w>>1, h>>1, EDGE_WIDTH/2);
+        s->dsp.draw_edges(s->current_picture.data[0], s->current_picture.linesize[0], w   , h   , EDGE_WIDTH  , EDGE_TOP|EDGE_BOTTOM);
+        s->dsp.draw_edges(s->current_picture.data[1], s->current_picture.linesize[1], w>>1, h>>1, EDGE_WIDTH/2, EDGE_TOP|EDGE_BOTTOM);
+        s->dsp.draw_edges(s->current_picture.data[2], s->current_picture.linesize[2], w>>1, h>>1, EDGE_WIDTH/2, EDGE_TOP|EDGE_BOTTOM);
     }
 
     tmp= s->last_picture[s->max_ref_frames-1];
     memmove(s->last_picture+1, s->last_picture, (s->max_ref_frames-1)*sizeof(AVFrame));
     memmove(s->halfpel_plane+1, s->halfpel_plane, (s->max_ref_frames-1)*sizeof(void*)*4*4);
     if(USE_HALFPEL_PLANE && s->current_picture.data[0])
         halfpel_interpol(s, s->halfpel_plane[0], &s->current_picture);
     s->last_picture[0]= s->current_picture;
     s->current_picture= tmp;
 
     if(s->keyframe){
--- utils.c	Sun Jan 25 13:19:00 2009
+++ utils.c	Sun Jan 25 13:07:24 2009
@@ -27,22 +27,23 @@
 
 /* needed for mkstemp() */
 #define _XOPEN_SOURCE 600
 
 #include "libavutil/avstring.h"
 #include "libavutil/integer.h"
 #include "libavutil/crc.h"
 #include "avcodec.h"
 #include "dsputil.h"
 #include "opt.h"
 #include "imgconvert.h"
+#include "thread.h"
 #include "audioconvert.h"
 #include "internal.h"
 #include <stdlib.h>
 #include <stdarg.h>
 #include <limits.h>
 #include <float.h>
 #if !HAVE_MKSTEMP
 #include <fcntl.h>
 #endif
 
 const uint8_t ff_reverse[256]={
@@ -308,33 +309,35 @@
 
     return 0;
 }
 
 void avcodec_default_release_buffer(AVCodecContext *s, AVFrame *pic){
     int i;
     InternalBuffer *buf, *last;
 
     assert(pic->type==FF_BUFFER_TYPE_INTERNAL);
     assert(s->internal_buffer_count);
 
+    if(s->internal_buffer){
     buf = NULL; /* avoids warning */
     for(i=0; i<s->internal_buffer_count; i++){ //just 3-5 checks so is not worth to optimize
         buf= &((InternalBuffer*)s->internal_buffer)[i];
         if(buf->data[0] == pic->data[0])
             break;
     }
     assert(i < s->internal_buffer_count);
     s->internal_buffer_count--;
     last = &((InternalBuffer*)s->internal_buffer)[s->internal_buffer_count];
 
     FFSWAP(InternalBuffer, *buf, *last);
+    }
 
     for(i=0; i<4; i++){
         pic->data[i]=NULL;
 //        pic->base[i]=NULL;
     }
 //printf("R%X\n", pic->opaque);
 
     if(s->debug&FF_DEBUG_BUFFERS)
         av_log(s, AV_LOG_DEBUG, "default_release_buffer called on pic %p, %d buffers used\n", pic, s->internal_buffer_count);
 }
 
@@ -430,23 +433,33 @@
         avcodec_set_dimensions(avctx, avctx->width, avctx->height);
 
     if((avctx->coded_width||avctx->coded_height) && avcodec_check_dimensions(avctx,avctx->coded_width,avctx->coded_height)){
         av_freep(&avctx->priv_data);
         ret = AVERROR(EINVAL);
         goto end;
     }
 
     avctx->codec = codec;
     avctx->codec_id = codec->id;
     avctx->frame_number = 0;
-    if(avctx->codec->init){
+
+    if (HAVE_THREADS && avctx->thread_count>1 && !avctx->thread_opaque) {
+        ret = avcodec_thread_init(avctx, avctx->thread_count);
+        if (ret < 0) {
+            av_freep(&avctx->priv_data);
+            avctx->codec= NULL;
+            goto end;
+        }
+    }
+
+    if(avctx->codec->init && !USE_FRAME_THREADING(avctx)){
         ret = avctx->codec->init(avctx);
         if (ret < 0) {
             av_freep(&avctx->priv_data);
             avctx->codec= NULL;
             goto end;
         }
     }
     ret=0;
 end:
     entangled_thread_counter--;
     return ret;
@@ -492,28 +505,31 @@
     int ret;
     ret = avctx->codec->encode(avctx, buf, buf_size, (void *)sub);
     avctx->frame_number++;
     return ret;
 }
 
 int attribute_align_arg avcodec_decode_video(AVCodecContext *avctx, AVFrame *picture,
                          int *got_picture_ptr,
                          const uint8_t *buf, int buf_size)
 {
     int ret;
+    int threaded = USE_FRAME_THREADING(avctx);
 
     *got_picture_ptr= 0;
     if((avctx->coded_width||avctx->coded_height) && avcodec_check_dimensions(avctx,avctx->coded_width,avctx->coded_height))
         return -1;
-    if((avctx->codec->capabilities & CODEC_CAP_DELAY) || buf_size){
-        ret = avctx->codec->decode(avctx, picture, got_picture_ptr,
+    if((avctx->codec->capabilities & CODEC_CAP_DELAY) || buf_size || threaded){
+        if (threaded) ret = ff_decode_frame_threaded(avctx, picture,
+                                got_picture_ptr, buf, buf_size);
+        else ret = avctx->codec->decode(avctx, picture, got_picture_ptr,
                                 buf, buf_size);
 
         emms_c(); //needed to avoid an emms_c() call before every return;
 
         if (*got_picture_ptr)
             avctx->frame_number++;
     }else
         ret= 0;
 
     return ret;
 }
@@ -563,27 +579,28 @@
 int avcodec_close(AVCodecContext *avctx)
 {
     entangled_thread_counter++;
     if(entangled_thread_counter != 1){
         av_log(avctx, AV_LOG_ERROR, "insufficient thread locking around avcodec_open/close()\n");
         entangled_thread_counter--;
         return -1;
     }
 
     if (HAVE_THREADS && avctx->thread_opaque)
         avcodec_thread_free(avctx);
-    if (avctx->codec->close)
+    if (avctx->codec->close && !USE_FRAME_THREADING(avctx))
         avctx->codec->close(avctx);
     avcodec_default_free_buffers(avctx);
     av_freep(&avctx->priv_data);
     avctx->codec = NULL;
+    avctx->active_thread_type = 0;
     entangled_thread_counter--;
     return 0;
 }
 
 AVCodec *avcodec_find_encoder(enum CodecID id)
 {
     AVCodec *p;
     p = first_avcodec;
     while (p) {
         if (p->encode != NULL && p->id == id)
             return p;
@@ -801,22 +818,24 @@
     static int initialized = 0;
 
     if (initialized != 0)
         return;
     initialized = 1;
 
     dsputil_static_init();
 }
 
 void avcodec_flush_buffers(AVCodecContext *avctx)
 {
+    if(USE_FRAME_THREADING(avctx))
+        ff_frame_thread_flush(avctx);
     if(avctx->codec->flush)
         avctx->codec->flush(avctx);
 }
 
 void avcodec_default_free_buffers(AVCodecContext *s){
     int i, j;
 
     if(s->internal_buffer==NULL) return;
 
     for(i=0; i<INTERNAL_BUFFER_SIZE; i++){
         InternalBuffer *buf= &((InternalBuffer*)s->internal_buffer)[i];
--- vc1.c	Sun Jan 25 13:19:00 2009
+++ vc1.c	Sun Jan 25 13:07:24 2009
@@ -1166,23 +1166,28 @@
     v->pqindex = pqindex;
     if (pqindex < 9) v->halfpq = get_bits1(gb);
     else v->halfpq = 0;
     if (v->quantizer_mode == QUANT_FRAME_EXPLICIT)
         v->pquantizer = get_bits1(gb);
     v->dquantfrm = 0;
     if (v->extended_mv == 1) v->mvrange = get_unary(gb, 0, 3);
     v->k_x = v->mvrange + 9 + (v->mvrange >> 1); //k_x can be 9 10 12 13
     v->k_y = v->mvrange + 8; //k_y can be 8 9 10 11
     v->range_x = 1 << (v->k_x - 1);
     v->range_y = 1 << (v->k_y - 1);
-    if (v->multires && v->s.pict_type != FF_B_TYPE) v->respic = get_bits(gb, 2);
+    if (v->profile == PROFILE_ADVANCED)
+    {
+        if (v->postprocflag) v->postproc = get_bits1(gb);
+    }
+    else
+        if (v->multires && v->s.pict_type != FF_B_TYPE) v->respic = get_bits(gb, 2);
 
     if(v->res_x8 && (v->s.pict_type == FF_I_TYPE || v->s.pict_type == FF_BI_TYPE)){
         v->x8_type = get_bits1(gb);
     }else v->x8_type = 0;
 //av_log(v->s.avctx, AV_LOG_INFO, "%c Frame: QP=[%i]%i (+%i/2) %i\n",
 //        (v->s.pict_type == FF_P_TYPE) ? 'P' : ((v->s.pict_type == FF_I_TYPE) ? 'I' : 'B'), pqindex, v->pq, v->halfpq, v->rangeredfrm);
 
     if(v->s.pict_type == FF_I_TYPE || v->s.pict_type == FF_P_TYPE) v->use_ic = 0;
 
     switch(v->s.pict_type) {
     case FF_P_TYPE:
@@ -1394,23 +1399,23 @@
     v->pquantizer = 1;
     if (v->quantizer_mode == QUANT_FRAME_IMPLICIT)
         v->pquantizer = pqindex < 9;
     if (v->quantizer_mode == QUANT_NON_UNIFORM)
         v->pquantizer = 0;
     v->pqindex = pqindex;
     if (pqindex < 9) v->halfpq = get_bits1(gb);
     else v->halfpq = 0;
     if (v->quantizer_mode == QUANT_FRAME_EXPLICIT)
         v->pquantizer = get_bits1(gb);
     if(v->postprocflag)
-        v->postproc = get_bits(gb, 2);
+        v->postproc = get_bits1(gb);
 
     if(v->s.pict_type == FF_I_TYPE || v->s.pict_type == FF_P_TYPE) v->use_ic = 0;
 
     switch(v->s.pict_type) {
     case FF_I_TYPE:
     case FF_BI_TYPE:
         status = bitplane_decoding(v->acpred_plane, &v->acpred_is_raw, v);
         if (status < 0) return -1;
         av_log(v->s.avctx, AV_LOG_DEBUG, "ACPRED plane encoding: "
                 "Imode: %i, Invert: %i\n", status>>1, status&1);
         v->condover = CONDOVER_NONE;
@@ -3095,37 +3100,37 @@
                 if(apply_filter && cbp_top & (2 << j))
                     vc1_loop_filter(dst + j*4, 1, linesize, 4, mquant);
                 if(apply_filter && j ? pat & 0x5 : (cbp_left & 0xA))
                     vc1_loop_filter(dst + j*4, linesize, 1, 8, mquant);
             }
         }
         break;
     }
     return pat;
 }
 
-static const int size_table  [6] = { 0, 2, 3, 4,  5,  8 };
-static const int offset_table[6] = { 0, 1, 3, 7, 15, 31 };
 
 /** Decode one P-frame MB (in Simple/Main profile)
  */
 static int vc1_decode_p_mb(VC1Context *v)
 {
     MpegEncContext *s = &v->s;
     GetBitContext *gb = &s->gb;
     int i, j;
     int mb_pos = s->mb_x + s->mb_y * s->mb_stride;
     int cbp; /* cbp decoding stuff */
     int mqdiff, mquant; /* MB quantization */
     int ttmb = v->ttfrm; /* MB Transform type */
 
+    static const int size_table[6] = { 0, 2, 3, 4, 5, 8 },
+      offset_table[6] = { 0, 1, 3, 7, 15, 31 };
     int mb_has_coeffs = 1; /* last_flag */
     int dmv_x, dmv_y; /* Differential MV components */
     int index, index1; /* LUT indexes */
     int val, sign; /* temp values */
     int first_block = 1;
     int dst_idx, off;
     int skipped, fourmv;
     int block_cbp = 0, pat;
     int apply_loop_filter;
 
     mquant = v->pq; /* Loosy initialization */
@@ -3403,22 +3408,25 @@
 /** Decode one B-frame MB (in Main profile)
  */
 static void vc1_decode_b_mb(VC1Context *v)
 {
     MpegEncContext *s = &v->s;
     GetBitContext *gb = &s->gb;
     int i, j;
     int mb_pos = s->mb_x + s->mb_y * s->mb_stride;
     int cbp = 0; /* cbp decoding stuff */
     int mqdiff, mquant; /* MB quantization */
     int ttmb = v->ttfrm; /* MB Transform type */
+
+    static const int size_table[6] = { 0, 2, 3, 4, 5, 8 },
+      offset_table[6] = { 0, 1, 3, 7, 15, 31 };
     int mb_has_coeffs = 0; /* last_flag */
     int index, index1; /* LUT indexes */
     int val, sign; /* temp values */
     int first_block = 1;
     int dst_idx, off;
     int skipped, direct;
     int dmv_x[2], dmv_y[2];
     int bmvtype = BMV_TYPE_BACKWARD;
 
     mquant = v->pq; /* Loosy initialization */
     s->mb_intra = 0;
--- w32thread.c	Sun Jan 25 13:19:01 2009
+++ w32thread.c	Sun Jan 25 13:07:24 2009
@@ -96,23 +96,29 @@
         c[i].func= NULL;
         if(ret) ret[i]= c[i].ret;
     }
     return 0;
 }
 
 int avcodec_thread_init(AVCodecContext *s, int thread_count){
     int i;
     ThreadContext *c;
     uint32_t threadid;
 
+    if(!(s->thread_type & FF_THREAD_SLICE)){
+        av_log(s, AV_LOG_WARNING, "The requested thread algorithm is not supported with this thread library.\n");
+        return 0;
+    }
+
     s->thread_count= thread_count;
+    s->active_thread_type= FF_THREAD_SLICE;
 
     assert(!s->thread_opaque);
     c= av_mallocz(sizeof(ThreadContext)*thread_count);
     s->thread_opaque= c;
 
     for(i=0; i<thread_count; i++){
 //printf("init semaphors %d\n", i); fflush(stdout);
         c[i].avctx= s;
 
         if(!(c[i].work_sem = CreateSemaphore(NULL, 0, s->thread_count, NULL)))
             goto fail;
